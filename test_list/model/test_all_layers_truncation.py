#!/usr/bin/env python3
"""
é€å±‚éªŒè¯ä¿®å¤åçš„æ¨¡å‹ï¼šç¡®ä¿æ¯ä¸€å±‚çš„è¾“å‡ºåœ¨ä¸åŒé…ç½®ä¸‹éƒ½ç›¸åŒ

æµ‹è¯•é…ç½®ï¼š
1. [P][Mâ‚]
2. [P][Mâ‚][Râ‚]
3. [P][Mâ‚][Râ‚][Mâ‚‚]
4. å®Œæ•´åºåˆ—

éªŒè¯ï¼šæ¯ä¸€å±‚çš„ Mâ‚ ä½ç½®è¾“å‡ºåœ¨æ‰€æœ‰é…ç½®ä¸‹éƒ½åº”è¯¥å®Œå…¨ç›¸åŒ
"""

import os
os.environ["TORCH_COMPILE_DISABLE"] = "1"

import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer

project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from dllm_reasoning.trainer.interleaved_sft_dataset import InterleavedSFTDataset
from dllm_reasoning.model.DLLM.modeling_dllm import DLLMForCausalLM
from dllm_reasoning.model.DLLM.configuration_dllm import DLLMConfig


def capture_all_layers(model, input_ids, position_ids, block_info, prompt_len, seq_len, device):
    """
    æ•è·æ‰€æœ‰å±‚çš„ Mâ‚ ä½ç½®è¾“å‡º

    Returns:
        Dict[int, torch.Tensor]: {layer_idx: hidden_state_at_m1}
    """
    m1_pos = prompt_len
    layer_outputs = {}

    # Hookæ¯ä¸€å±‚çš„è¾“å‡º
    def make_hook(layer_idx):
        def hook(module, input, output):
            # output is tuple: (hidden_states, ...)
            # hidden_states shape: [batch, seq_len, hidden_dim]
            hidden_states = output[0]
            layer_outputs[layer_idx] = hidden_states[0, m1_pos].detach().clone()
        return hook

    # æ³¨å†Œæ‰€æœ‰å±‚çš„hooks
    hooks = []
    num_layers = len(model.model.layers)
    for i in range(num_layers):
        hook = model.model.layers[i].register_forward_hook(make_hook(i))
        hooks.append(hook)

    # Forward pass
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids.unsqueeze(0),
            position_ids=position_ids.unsqueeze(0),
            block_info=[block_info],
            prompt_len=[prompt_len],
            seq_lens=[seq_len],
            use_cache=False
        )

    # Remove hooks
    for hook in hooks:
        hook.remove()

    return layer_outputs


def test_config(model, sample, num_blocks_to_keep, device):
    """
    æµ‹è¯•ç‰¹å®šé…ç½®ä¸‹çš„æ‰€æœ‰å±‚è¾“å‡º

    Args:
        model: æ¨¡å‹
        sample: æ•°æ®æ ·æœ¬
        num_blocks_to_keep: ä¿ç•™å¤šå°‘ä¸ªblock
        device: è®¾å¤‡

    Returns:
        Dict[int, torch.Tensor]: {layer_idx: hidden_state_at_m1}
    """
    input_ids = sample['input_ids'].to(device)
    position_ids = sample['position_ids'].to(device)
    block_info = sample['block_info']
    prompt_len = sample['prompt_len']

    # æˆªæ–­åºåˆ—
    current_pos = prompt_len
    blocks_seen = 0
    truncate_pos = None

    for seg_type, seg_idx, seg_len in block_info:
        blocks_seen += 1
        current_pos += seg_len
        if blocks_seen >= num_blocks_to_keep:
            truncate_pos = current_pos
            break

    if truncate_pos is None:
        truncate_pos = input_ids.size(0)

    # æˆªæ–­åºåˆ—å’Œblock_info
    input_ids_truncated = input_ids[:truncate_pos]
    position_ids_truncated = position_ids[:truncate_pos]

    block_info_truncated = []
    blocks_added = 0
    for seg_type, seg_idx, seg_len in block_info:
        if blocks_added >= num_blocks_to_keep:
            break
        block_info_truncated.append((seg_type, seg_idx, seg_len))
        blocks_added += 1

    # æ•è·æ‰€æœ‰å±‚è¾“å‡º
    layer_outputs = capture_all_layers(
        model=model,
        input_ids=input_ids_truncated,
        position_ids=position_ids_truncated,
        block_info=block_info_truncated,
        prompt_len=prompt_len,
        seq_len=truncate_pos,
        device=device,
    )

    return layer_outputs


def main():
    MODEL_PATH = "dllm_reasoning/dllm_reasoning/models/DLLM-1.5B"
    DATA_PATH = "data/openr1.parquet"

    print("="*80)
    print("é€å±‚éªŒè¯ä¿®å¤åçš„æ¨¡å‹")
    print("="*80)
    print()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print("åŠ è½½ä¿®å¤åçš„æœ¬åœ°æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

    # ä»æœ¬åœ°ä»£ç åŠ è½½æ¨¡å‹
    config = DLLMConfig.from_pretrained(MODEL_PATH)
    model = DLLMForCausalLM(config)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    from transformers import AutoModelForCausalLM
    pretrained = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    model.load_state_dict(pretrained.state_dict())
    model = model.to(device).to(torch.bfloat16)

    # ä½¿ç”¨è®­ç»ƒæ¨¡å¼
    model.train()

    num_layers = len(model.model.layers)
    print(f"æ¨¡å‹å±‚æ•°: {num_layers}")

    print("åŠ è½½æ•°æ®é›†...")
    dataset = InterleavedSFTDataset(
        parquet_files=DATA_PATH,
        tokenizer=tokenizer,
        prompt_key="prompt",
        response_key="target",
        block_size=4,
        max_length=6000,
        truncation="right",
    )

    sample = dataset[0]

    print(f"\næ ·æœ¬ä¿¡æ¯:")
    print(f"  æ€»åºåˆ—é•¿åº¦: {sample['input_ids'].shape[0]}")
    print(f"  Prompté•¿åº¦: {sample['prompt_len']}")
    print(f"  æ€»Blockæ•°: {len(sample['block_info'])}")
    print()

    # æµ‹è¯•å››ä¸ªé…ç½®
    test_configs = [
        (1, "[P][Mâ‚]"),
        (2, "[P][Mâ‚][Râ‚]"),
        (3, "[P][Mâ‚][Râ‚][Mâ‚‚]"),
        (len(sample['block_info']), f"å®Œæ•´åºåˆ—({len(sample['block_info'])}å—)"),
    ]

    print("="*80)
    print("å¼€å§‹é€å±‚æ•è·...")
    print("="*80)
    print()

    # æ”¶é›†æ‰€æœ‰é…ç½®çš„layer outputs
    all_results = {}

    for num_blocks, config_name in test_configs:
        if num_blocks > len(sample['block_info']):
            continue

        print(f"é…ç½®: {config_name} (ä¿ç•™å‰ {num_blocks} ä¸ªblocks)")

        layer_outputs = test_config(
            model=model,
            sample=sample,
            num_blocks_to_keep=num_blocks,
            device=device,
        )

        all_results[config_name] = layer_outputs
        print(f"  âœ“ å·²æ•è· {len(layer_outputs)} å±‚çš„è¾“å‡º")
        print()

    # é€å±‚å¯¹æ¯”
    print("="*80)
    print("é€å±‚å¯¹æ¯”ç»“æœ")
    print("="*80)
    print()

    config_names = list(all_results.keys())
    base_config = config_names[0]  # [P][Mâ‚]

    print(f"åŸºå‡†é…ç½®: {base_config}")
    print(f"å¯¹æ¯”é…ç½®: {', '.join(config_names[1:])}")
    print()

    # ç»Ÿè®¡ä¿¡æ¯
    total_layers = num_layers
    all_same_count = 0
    diff_layers = []

    print(f"{'Layer':^8} | {'çŠ¶æ€':^10} | {'æœ€å¤§L2å·®å¼‚':^20}")
    print("-" * 50)

    for layer_idx in range(num_layers):
        base_output = all_results[base_config][layer_idx]

        # è®¡ç®—è¿™ä¸€å±‚ä¸å…¶ä»–é…ç½®çš„æœ€å¤§å·®å¼‚
        max_diff = 0.0
        diff_config = None

        for config_name in config_names[1:]:
            other_output = all_results[config_name][layer_idx]
            diff = torch.norm(base_output - other_output).item()

            if diff > max_diff:
                max_diff = diff
                diff_config = config_name

        # åˆ¤æ–­æ˜¯å¦ç›¸åŒï¼ˆé˜ˆå€¼ 1e-5ï¼‰
        is_same = max_diff < 1e-5

        if is_same:
            all_same_count += 1
            status = "âœ… ç›¸åŒ"
        else:
            status = "âŒ ä¸åŒ"
            diff_layers.append((layer_idx, max_diff, diff_config))

        print(f"{layer_idx:^8} | {status:^10} | {max_diff:>12.6e} ({diff_config if diff_config else 'N/A'})")

    # æ€»ç»“
    print()
    print("="*80)
    print("æ€»ç»“")
    print("="*80)
    print()

    print(f"æ€»å±‚æ•°: {total_layers}")
    print(f"å®Œå…¨ç›¸åŒçš„å±‚: {all_same_count}/{total_layers} ({all_same_count/total_layers*100:.1f}%)")
    print(f"å­˜åœ¨å·®å¼‚çš„å±‚: {len(diff_layers)}/{total_layers}")

    if len(diff_layers) == 0:
        print()
        print("ğŸ‰ æ‰€æœ‰å±‚çš„ Mâ‚ è¾“å‡ºåœ¨ä¸åŒé…ç½®ä¸‹éƒ½å®Œå…¨ç›¸åŒï¼")
        print()
        print("è¿™è¯´æ˜ï¼š")
        print("  âœ… BlockMask åœ¨æ¯ä¸€å±‚éƒ½æ­£ç¡®å·¥ä½œ")
        print("  âœ… Mâ‚ åœ¨æ‰€æœ‰å±‚éƒ½çœ‹ä¸åˆ°åç»­ token")
        print("  âœ… æ¨¡å‹ä¿®å¤å®Œå…¨æˆåŠŸï¼")
    else:
        print()
        print("âš ï¸ å‘ç°å·®å¼‚çš„å±‚ï¼š")
        print()
        for layer_idx, max_diff, diff_config in diff_layers:
            print(f"  Layer {layer_idx}: æœ€å¤§å·®å¼‚ {max_diff:.6e} (ä¸ {diff_config} å¯¹æ¯”)")

        print()
        print("å¯èƒ½çš„åŸå› ï¼š")
        print("  - æ•°å€¼ç²¾åº¦é—®é¢˜ï¼ˆå¦‚æœå·®å¼‚å¾ˆå° <1e-4ï¼‰")
        print("  - BlockMask åœ¨æŸäº›å±‚æ²¡æœ‰æ­£ç¡®åº”ç”¨")
        print("  - éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

    print()
    print("="*80)


if __name__ == "__main__":
    main()
