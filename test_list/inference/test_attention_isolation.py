#!/usr/bin/env python3
"""
æµ‹è¯•FlexAttentionæ˜¯å¦çœŸæ­£éš”ç¦»äº†Maskå’ŒReal tokens
é€šè¿‡å¯¹æ¯”ä¸¤ä¸ªforward passçš„hidden statesæ¥éªŒè¯
"""

import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dllm_reasoning.trainer.interleaved_sft_dataset import create_interleaved_training_data
from dllm_reasoning.trainer.flex_attention_utils import create_block_mask_from_batch

MODEL_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
DATA_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/data/openr1.parquet"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("="*100)
print("æµ‹è¯•FlexAttentionéš”ç¦»æ€§")
print("="*100)

# ========== åŠ è½½æ¨¡å‹ ==========
print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    local_files_only=True,
).to(device).train()  # Training mode for FlexAttention

print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ========== å‡†å¤‡æ•°æ® ==========
print("\nğŸ“‚ å‡†å¤‡æµ‹è¯•æ•°æ®...")
df = pd.read_parquet(DATA_PATH)
sample = df.iloc[0]

prompt = sample['prompt']
target = sample['target']

# å¤„ç†
messages = list(prompt) if isinstance(prompt, (list, tuple)) else prompt
messages = messages.tolist() if hasattr(messages, 'tolist') else messages

target_list = target.tolist() if hasattr(target, 'tolist') else target
if isinstance(target_list, list) and len(target_list) > 0:
    target_content = target_list[0].get('content', '') if isinstance(target_list[0], dict) else str(target_list[0])
else:
    target_content = str(target_list)

# å»æ‰<think>
if target_content.strip().startswith('<think>'):
    think_start = target_content.find('<think>')
    target_content = target_content[think_start + 7:].lstrip()

# å®Œæ•´å¯¹è¯
full_messages = messages + [{"role": "assistant", "content": target_content}]

# Tokenize
input_ids = tokenizer.apply_chat_template(
    full_messages,
    tokenize=True,
    add_generation_prompt=False,
    return_tensors="pt"
)[0].to(device)

prompt_ids = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt"
)[0].to(device)

prompt_len = prompt_ids.size(0)
loss_mask = torch.ones(input_ids.size(0), dtype=torch.long, device=device)

# åˆ›å»ºinterleavedæ ¼å¼
interleaved_ids, position_ids, interleaved_loss_mask, block_info = create_interleaved_training_data(
    input_ids=input_ids,
    loss_mask=loss_mask,
    block_size=4,
    mask_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)

print(f"   Prompté•¿åº¦: {prompt_len}")
print(f"   Interleavedåºåˆ—é•¿åº¦: {interleaved_ids.size(0)}")
print(f"   Block info (å‰3ä¸ª): {block_info[:3]}")

# è½¬æ¢block_infoæ ¼å¼: (seg_type, list_idx, seg_len) -> (seg_type, seg_len)
block_info_converted = [(seg_type, seg_len) for seg_type, _, seg_len in block_info]
print(f"   Block info converted (å‰3ä¸ª): {block_info_converted[:3]}")

# ä¸ºäº†é¿å…åºåˆ—è¿‡é•¿,åªæµ‹è¯•ç¬¬ä¸€ä¸ªblock: Prompt + MaskÃ—3 + RealÃ—4
first_block_end = prompt_len + 3 + 4  # 277 + 3 + 4 = 284
interleaved_ids_short = interleaved_ids[:first_block_end]
position_ids_short = position_ids[:first_block_end]
block_info_short = block_info_converted[:2]  # åªæœ‰ç¬¬ä¸€ä¸ªmaskå’Œç¬¬ä¸€ä¸ªreal

print(f"\nä»…æµ‹è¯•ç¬¬ä¸€ä¸ªblockä»¥é¿å…å†…å­˜é—®é¢˜:")
print(f"   æˆªæ–­é•¿åº¦: {first_block_end}")
print(f"   Block info: {block_info_short}")

# ========== æµ‹è¯•1: å®Œæ•´åºåˆ— (è®­ç»ƒæ—¶çš„æƒ…å†µ) ==========
print("\n" + "="*100)
print("æµ‹è¯•1: å®Œæ•´åºåˆ— [Prompt + MaskÃ—3 + RealÃ—4]")
print("="*100)

# æ„é€ batchæ ¼å¼
batch = {
    "input_ids": interleaved_ids_short.unsqueeze(0),
    "position_ids": position_ids_short.unsqueeze(0),
    "block_info": [block_info_short],
    "prompt_len": [prompt_len],
    "seq_lens": [interleaved_ids_short.size(0)],
}

# åˆ›å»ºFlexAttention mask
block_mask_full = create_block_mask_from_batch(batch, device)

# Forward pass
with torch.no_grad():
    outputs_full = model(
        interleaved_ids_short.unsqueeze(0),
        position_ids=position_ids_short.unsqueeze(0),
        attention_mask=block_mask_full,
        use_cache=False,
        output_hidden_states=True
    )

logits_full = outputs_full.logits[0]  # [seq_len, vocab]
hidden_states_full = outputs_full.hidden_states[-1][0]  # æœ€åä¸€å±‚çš„hidden states [seq_len, hidden_dim]

print(f"   Logits shape: {logits_full.shape}")
print(f"   Hidden states shape: {hidden_states_full.shape}")

# æå–Mask positionsçš„hidden stateså’Œlogits
mask_start = prompt_len
mask_end = mask_start + 3

mask_hidden_full = hidden_states_full[mask_start:mask_end]  # [3, hidden_dim]
mask_logits_full = logits_full[mask_start:mask_end]  # [3, vocab]

print(f"\n   Mask positions: [{mask_start}:{mask_end}]")
print(f"   Mask hidden states shape: {mask_hidden_full.shape}")
print(f"   Mask logits shape: {mask_logits_full.shape}")

# Maskçš„é¢„æµ‹
mask_predictions_full = mask_logits_full.argmax(dim=-1)
print(f"   Mask predictions: {mask_predictions_full.tolist()}")
print(f"   Mask predictions (text): {[tokenizer.decode([t]) for t in mask_predictions_full]}")

# ========== æµ‹è¯•2: æˆªæ–­åºåˆ— (åªæœ‰Prompt + MaskÃ—3,æ¨ç†æ—¶çš„æƒ…å†µ) ==========
print("\n" + "="*100)
print("æµ‹è¯•2: æˆªæ–­åºåˆ— [Prompt + MaskÃ—3] (æ¨¡æ‹Ÿæ¨ç†)")
print("="*100)

# åªä¿ç•™Promptå’Œç¬¬ä¸€ç»„Mask
truncated_ids = interleaved_ids[:mask_end].clone()
truncated_position_ids = position_ids[:mask_end].clone()

print(f"   æˆªæ–­åºåˆ—é•¿åº¦: {truncated_ids.size(0)}")
print(f"   = Prompt({prompt_len}) + Mask(3)")

# ä¸ºæˆªæ–­åºåˆ—åˆ›å»ºFlexAttention mask
# æ³¨æ„: è¿™é‡Œçš„block_infoåªåŒ…å«promptå’Œç¬¬ä¸€ä¸ªmask group
truncated_block_info = [('mask', 0, 3)]  # åªæœ‰ä¸€ä¸ªmask segment

batch_truncated = {
    "input_ids": truncated_ids.unsqueeze(0),
    "position_ids": truncated_position_ids.unsqueeze(0),
    "block_info": [truncated_block_info],
    "prompt_len": [prompt_len],
    "seq_lens": [truncated_ids.size(0)],
}

block_mask_truncated = create_block_mask_from_batch(batch_truncated, device)

# Forward pass
with torch.no_grad():
    outputs_truncated = model(
        truncated_ids.unsqueeze(0),
        position_ids=truncated_position_ids.unsqueeze(0),
        attention_mask=block_mask_truncated,
        use_cache=False,
        output_hidden_states=True
    )

logits_truncated = outputs_truncated.logits[0]  # [mask_end, vocab]
hidden_states_truncated = outputs_truncated.hidden_states[-1][0]  # [mask_end, hidden_dim]

# Mask positionsçš„hidden stateså’Œlogits
mask_hidden_truncated = hidden_states_truncated[mask_start:mask_end]
mask_logits_truncated = logits_truncated[mask_start:mask_end]

mask_predictions_truncated = mask_logits_truncated.argmax(dim=-1)
print(f"   Mask predictions: {mask_predictions_truncated.tolist()}")
print(f"   Mask predictions (text): {[tokenizer.decode([t]) for t in mask_predictions_truncated]}")

# ========== å¯¹æ¯”ç»“æœ ==========
print("\n" + "="*100)
print("å¯¹æ¯”ç»“æœ")
print("="*100)

# è®¡ç®—hidden statesçš„å·®å¼‚
hidden_diff = torch.abs(mask_hidden_full - mask_hidden_truncated)
hidden_max_diff = hidden_diff.max().item()
hidden_mean_diff = hidden_diff.mean().item()

print(f"\nMask positionsçš„Hidden Stateså·®å¼‚:")
print(f"   æœ€å¤§å·®å¼‚: {hidden_max_diff:.6f}")
print(f"   å¹³å‡å·®å¼‚: {hidden_mean_diff:.6f}")

# è®¡ç®—logitsçš„å·®å¼‚
logits_diff = torch.abs(mask_logits_full - mask_logits_truncated)
logits_max_diff = logits_diff.max().item()
logits_mean_diff = logits_diff.mean().item()

print(f"\nMask positionsçš„Logitså·®å¼‚:")
print(f"   æœ€å¤§å·®å¼‚: {logits_max_diff:.6f}")
print(f"   å¹³å‡å·®å¼‚: {logits_mean_diff:.6f}")

# é¢„æµ‹æ˜¯å¦ç›¸åŒ
predictions_match = torch.equal(mask_predictions_full, mask_predictions_truncated)
print(f"\né¢„æµ‹ç»“æœæ˜¯å¦å®Œå…¨ç›¸åŒ: {predictions_match}")
if not predictions_match:
    print(f"   å®Œæ•´åºåˆ—é¢„æµ‹: {mask_predictions_full.tolist()}")
    print(f"   æˆªæ–­åºåˆ—é¢„æµ‹: {mask_predictions_truncated.tolist()}")

# ========== ç»“è®º ==========
print("\n" + "="*100)
print("ç»“è®º")
print("="*100)

if hidden_max_diff < 1e-4 and predictions_match:
    print("âœ… FlexAttentionå®Œç¾éš”ç¦»äº†Maskå’ŒReal tokens!")
    print("   è®­ç»ƒæ—¶æœ‰Real tokenså’Œæ¨ç†æ—¶æ²¡æœ‰Real tokensçš„ç»“æœå®Œå…¨ä¸€è‡´")
    print("   é—®é¢˜ä¸åœ¨éš”ç¦»æ€§ä¸Š,éœ€è¦å¯»æ‰¾å…¶ä»–åŸå› ")
elif hidden_max_diff < 1e-2:
    print("âš ï¸  æœ‰è½»å¾®å·®å¼‚,ä½†åŸºæœ¬éš”ç¦»")
    print(f"   Hidden statesæœ€å¤§å·®å¼‚: {hidden_max_diff:.6f}")
    print("   è¿™å¯èƒ½æ˜¯æ•°å€¼ç²¾åº¦é—®é¢˜")
else:
    print("âŒ éš”ç¦»å¤±è´¥! FlexAttention maskæ²¡æœ‰æ­£ç¡®éš”ç¦»Maskå’ŒReal tokens")
    print(f"   Hidden statesæœ€å¤§å·®å¼‚: {hidden_max_diff:.6f}")
    print("   è¿™è¯´æ˜è®­ç»ƒæ—¶çš„maskå®ç°æœ‰é—®é¢˜!")
