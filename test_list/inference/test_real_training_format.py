#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®è®­ç»ƒæ•°æ®æ ¼å¼æµ‹è¯•æ¨¡å‹
"""

import sys
from pathlib import Path
import torch

# ç¦ç”¨æ‰€æœ‰torch.compile
import torch._dynamo
torch._dynamo.config.disable = True

from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dllm_reasoning.trainer.interleaved_sft_dataset import create_interleaved_training_data, create_interleaved_labels
from dllm_reasoning.trainer.flex_attention_utils import create_block_mask_from_batch

MODEL_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
DATA_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/data/openr1.parquet"

print("="*100)
print("ä½¿ç”¨çœŸå®è®­ç»ƒæ•°æ®æ ¼å¼æµ‹è¯•æ¨¡å‹")
print("="*100)

# ========== åŠ è½½æ¨¡å‹ ==========
print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    local_files_only=True,
).to(device).train()  # ä½¿ç”¨ train() æ¨¡å¼ä»¥å¯ç”¨ FlexAttention

print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ (training mode for FlexAttention)")

# ========== åŠ è½½è®­ç»ƒæ•°æ® ==========
print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {DATA_PATH}")
df = pd.read_parquet(DATA_PATH)
print(f"   æ€»æ ·æœ¬æ•°: {df.shape[0]}")

# å–ç¬¬ä¸€ä¸ªæ ·æœ¬
sample = df.iloc[0]

# å¤„ç†æ•°æ®
prompt = sample['prompt']
target = sample['target']

messages = list(prompt) if isinstance(prompt, (list, tuple)) else prompt
messages = messages.tolist() if hasattr(messages, 'tolist') else messages

target_list = target.tolist() if hasattr(target, 'tolist') else target
if isinstance(target_list, list) and len(target_list) > 0:
    target_content = target_list[0].get('content', '') if isinstance(target_list[0], dict) else str(target_list[0])
else:
    target_content = str(target_list)

# å»æ‰<think>æ ‡ç­¾ (å’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
if target_content.strip().startswith('<think>'):
    think_start = target_content.find('<think>')
    target_content = target_content[think_start + 7:].lstrip()

# å®Œæ•´å¯¹è¯
full_messages = messages + [{"role": "assistant", "content": target_content}]

# Tokenize
input_ids = tokenizer.apply_chat_template(
    full_messages,
    tokenize=True,
    add_generation_prompt=False,
    return_tensors="pt"
)[0].to(device)

prompt_ids = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt"
)[0].to(device)

prompt_len = prompt_ids.size(0)

print(f"\nğŸ“ æ•°æ®ä¿¡æ¯:")
print(f"   Prompté•¿åº¦: {prompt_len}")
print(f"   å®Œæ•´åºåˆ—é•¿åº¦: {input_ids.size(0)}")
print(f"   Responseé•¿åº¦: {input_ids.size(0) - prompt_len}")

# åˆ›å»ºloss_mask (è®­ç»ƒæ—¶ç”¨çš„)
loss_mask = torch.ones(input_ids.size(0), dtype=torch.long, device=device)
loss_mask[:prompt_len] = 0  # Promptéƒ¨åˆ†mask=0

# ========== ä½¿ç”¨è®­ç»ƒæ—¶çš„å‡½æ•°åˆ›å»ºinterleavedæ ¼å¼ ==========
print(f"\nğŸ”§ åˆ›å»ºInterleavedè®­ç»ƒæ ¼å¼...")
interleaved_ids, position_ids, interleaved_loss_mask, block_info = create_interleaved_training_data(
    input_ids=input_ids,
    loss_mask=loss_mask,
    block_size=4,
    mask_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)

# åˆ›å»ºlabels
labels = create_interleaved_labels(
    original_input_ids=input_ids,
    interleaved_input_ids=interleaved_ids,
    interleaved_loss_mask=interleaved_loss_mask,
    block_size=4,
    prompt_len=prompt_len,
    pad_token_id=tokenizer.pad_token_id,
)

print(f"   Interleavedåºåˆ—é•¿åº¦: {interleaved_ids.size(0)}")
print(f"   Block infoæ•°é‡: {len(block_info)}")
print(f"   å‰3ä¸ªblocks: {block_info[:3]}")

# è½¬æ¢block_infoæ ¼å¼
block_info_converted = [(seg_type, seg_len) for seg_type, _, seg_len in block_info]

# ========== åªæµ‹è¯•ç¬¬ä¸€ä¸ªblock ==========
# æ‰¾åˆ°ç¬¬ä¸€ä¸ªmaskå’Œç¬¬ä¸€ä¸ªreal
first_mask_idx = None
first_real_idx = None

for idx, (seg_type, _, seg_len) in enumerate(block_info):
    if seg_type == 'mask' and first_mask_idx is None:
        first_mask_idx = idx
    if seg_type == 'real' and first_real_idx is None:
        first_real_idx = idx
        break

print(f"\nç¬¬ä¸€ä¸ªmask block index: {first_mask_idx}")
print(f"ç¬¬ä¸€ä¸ªreal block index: {first_real_idx}")

# è®¡ç®—å®é™…åºåˆ—èŒƒå›´
# ä»promptåˆ°ç¬¬ä¸€ä¸ªreal blockç»“æŸ
# block_infoæ ¼å¼: (seg_type, list_idx, seg_len)
# interleaved_idsæ˜¯æŒ‰listé¡ºåºæ‹¼æ¥çš„

# è®¡ç®—ç¬¬ä¸€ä¸ªreal blockåœ¨interleaved_idsä¸­çš„ç»“æŸä½ç½®
seq_pos = prompt_len
for idx, (seg_type, _, seg_len) in enumerate(block_info):
    if idx <= first_real_idx:
        seq_pos += seg_len
    else:
        break

test_seq_len = seq_pos
print(f"æµ‹è¯•åºåˆ—é•¿åº¦: {test_seq_len} (Prompt + ç¬¬ä¸€ä¸ªMask + ç¬¬ä¸€ä¸ªReal)")

# æˆªå–
test_ids = interleaved_ids[:test_seq_len]
test_position_ids = position_ids[:test_seq_len]
test_labels = labels[:test_seq_len]
test_block_info = block_info_converted[:first_real_idx+1]

print(f"\næµ‹è¯•åºåˆ—ä¿¡æ¯:")
print(f"  IDs shape: {test_ids.shape}")
print(f"  Position IDs shape: {test_position_ids.shape}")
print(f"  Labels shape: {test_labels.shape}")
print(f"  Block info: {test_block_info}")

# æ‰“å°å…³é”®éƒ¨åˆ†å†…å®¹
print(f"\nå…³é”®éƒ¨åˆ†å†…å®¹:")
# ç¬¬ä¸€ä¸ªmaskçš„ä½ç½®
mask_start = prompt_len
mask_end = prompt_len + 3
real_start = mask_end
real_end = real_start + 4

print(f"  Maskéƒ¨åˆ† [{mask_start}:{mask_end}]:")
print(f"    IDs: {test_ids[mask_start:mask_end].tolist()}")
print(f"    Text: {tokenizer.decode(test_ids[mask_start:mask_end])}")

print(f"  Realéƒ¨åˆ† [{real_start}:{real_end}]:")
print(f"    IDs: {test_ids[real_start:real_end].tolist()}")
print(f"    Text: '{tokenizer.decode(test_ids[real_start:real_end])}'")

print(f"  Labels (Maskä½ç½®) [{mask_start}:{mask_end}]:")
print(f"    {test_labels[mask_start:mask_end].tolist()}")
for i in range(3):
    label_id = test_labels[mask_start + i].item()
    if label_id != -100:
        print(f"      Mask[{i}]åº”è¯¥é¢„æµ‹: ID={label_id}, '{tokenizer.decode([label_id])}'")

# ========== Forward pass ==========
print(f"\nğŸš€ Forward pass...")

# æ„é€ batchæ ¼å¼
batch = {
    "input_ids": test_ids.unsqueeze(0),
    "position_ids": test_position_ids.unsqueeze(0),
    "block_info": [test_block_info],
    "prompt_len": [prompt_len],
    "seq_lens": [test_ids.size(0)],
}

# åˆ›å»ºFlexAttention mask
block_mask = create_block_mask_from_batch(batch, device)

with torch.no_grad():
    outputs = model(
        test_ids.unsqueeze(0),
        position_ids=test_position_ids.unsqueeze(0),
        attention_mask=block_mask,
        use_cache=False
    )
    logits = outputs.logits[0]  # [seq_len, vocab]

print(f"  Logits shape: {logits.shape}")

# ========== éªŒè¯é¢„æµ‹ ==========
print(f"\nğŸ“Š éªŒè¯Maskä½ç½®çš„é¢„æµ‹:")

for i in range(3):
    pos = mask_start + i
    pred_logit = logits[pos]  # è¿™ä¸ªlogité¢„æµ‹labels[pos]
    true_label = test_labels[pos].item()

    if true_label == -100:
        print(f"  Mask[{i}] (ä½ç½®{pos}): æ²¡æœ‰label")
        continue

    # Top 5é¢„æµ‹
    top5_logits, top5_indices = pred_logit.topk(5)
    print(f"\n  Mask[{i}] (ä½ç½®{pos}) åº”è¯¥é¢„æµ‹ ID={true_label} '{tokenizer.decode([true_label])}':")
    print(f"    Top 5 predictions:")
    for j, (logit_val, token_id) in enumerate(zip(top5_logits, top5_indices)):
        is_correct = "âœ…" if token_id.item() == true_label else "  "
        print(f"      {is_correct} #{j+1}: ID={token_id.item():6d}, logit={logit_val.item():8.4f}, text={repr(tokenizer.decode([token_id.item()]))}")

    # çœŸå®labelçš„logitå’Œæ’å
    true_logit = pred_logit[true_label].item()
    all_logits_sorted = pred_logit.sort(descending=True)
    true_rank = (all_logits_sorted.values > true_logit).sum().item() + 1
    print(f"    çœŸå®tokenæ’å: {true_rank}/{pred_logit.size(0)}")

    # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
    predicted_id = pred_logit.argmax().item()
    if predicted_id == true_label:
        print(f"    âœ… é¢„æµ‹æ­£ç¡®!")
    else:
        print(f"    âŒ é¢„æµ‹é”™è¯¯! é¢„æµ‹äº† ID={predicted_id} '{tokenizer.decode([predicted_id])}'")
