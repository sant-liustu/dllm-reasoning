#!/usr/bin/env python3
"""
æµ‹è¯•å¤šä¸ªæ ·æœ¬çš„å‡†ç¡®ç‡åˆ†å¸ƒ
"""

import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd

# ç¦ç”¨torch.compile
import torch._dynamo
torch._dynamo.config.disable = True

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dllm_reasoning.trainer.interleaved_sft_dataset import create_interleaved_training_data, create_interleaved_labels
from dllm_reasoning.trainer.flex_attention_utils import create_block_mask_from_batch

MODEL_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
DATA_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/data/openr1.parquet"

print("="*100)
print("æµ‹è¯•å¤šä¸ªæ ·æœ¬çš„å‡†ç¡®ç‡")
print("="*100)

# ========== åŠ è½½æ¨¡å‹ ==========
print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    local_files_only=True,
).to(device).train()  # ä½¿ç”¨trainæ¨¡å¼ï¼Œå’Œè®­ç»ƒæ—¶ä¸€è‡´

print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ (train mode)")

# ========== åŠ è½½æ•°æ® ==========
print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {DATA_PATH}")
df = pd.read_parquet(DATA_PATH)
print(f"   æ€»æ ·æœ¬æ•°: {len(df)}")

# ========== æµ‹è¯•å‰20ä¸ªæ ·æœ¬ ==========
NUM_SAMPLES = 20
print(f"\nğŸ” æµ‹è¯•å‰{NUM_SAMPLES}ä¸ªæ ·æœ¬:")

results = []

for sample_idx in range(NUM_SAMPLES):
    sample = df.iloc[sample_idx]

    # å¤„ç†æ•°æ® - å¯¹é½è®­ç»ƒé€»è¾‘
    prompt = sample['prompt']
    target = sample['target']

    messages = list(prompt) if isinstance(prompt, (list, tuple)) else prompt
    messages = messages.tolist() if hasattr(messages, 'tolist') else messages

    target_list = target.tolist() if hasattr(target, 'tolist') else target
    if isinstance(target_list, list) and len(target_list) > 0:
        target_content = target_list[0].get('content', '') if isinstance(target_list[0], dict) else str(target_list[0])
    else:
        target_content = str(target_list)

    # å»æ‰<think>
    if target_content.strip().startswith('<think>'):
        think_start = target_content.find('<think>')
        target_content = target_content[think_start + 7:].lstrip()

    # Tokenize - å¯¹é½è®­ç»ƒé€»è¾‘
    prompt_only_str = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    full_conversation_str = prompt_only_str + target_content + tokenizer.eos_token

    prompt_ids = tokenizer(prompt_only_str, return_tensors="pt", add_special_tokens=False)["input_ids"][0].to(device)
    input_ids = tokenizer(full_conversation_str, return_tensors="pt", add_special_tokens=False)["input_ids"][0].to(device)

    prompt_len = prompt_ids.size(0)

    # ========== é‡è¦ï¼šå¯¹é½è®­ç»ƒæ—¶çš„max_length truncation ==========
    MAX_LENGTH = 2048
    if input_ids.size(0) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]  # right truncation
        # é‡æ–°è°ƒæ•´prompt_lenå¦‚æœè¢«æˆªæ–­
        if prompt_len > MAX_LENGTH:
            prompt_len = MAX_LENGTH

    loss_mask = torch.ones(input_ids.size(0), dtype=torch.long, device=device)
    loss_mask[:prompt_len] = 0

    # åˆ›å»ºinterleavedæ ¼å¼
    interleaved_ids, position_ids, interleaved_loss_mask, block_info = create_interleaved_training_data(
        input_ids=input_ids,
        loss_mask=loss_mask,
        block_size=4,
        mask_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
    )

    # åˆ›å»ºlabels
    labels = create_interleaved_labels(
        original_input_ids=input_ids,
        interleaved_input_ids=interleaved_ids,
        interleaved_loss_mask=interleaved_loss_mask,
        block_size=4,
        prompt_len=prompt_len,
        pad_token_id=tokenizer.pad_token_id,
    )

    # è½¬æ¢block_infoæ ¼å¼
    block_info_converted = [(seg_type, seg_len) for seg_type, _, seg_len in block_info]

    # Forward pass
    batch = {
        "input_ids": interleaved_ids.unsqueeze(0),
        "position_ids": position_ids.unsqueeze(0),
        "block_info": [block_info_converted],
        "prompt_len": [prompt_len],
        "seq_lens": [interleaved_ids.size(0)],
    }

    block_mask = create_block_mask_from_batch(batch, device)

    with torch.no_grad():
        outputs = model(
            interleaved_ids.unsqueeze(0),
            position_ids=position_ids.unsqueeze(0),
            attention_mask=block_mask,
            use_cache=False
        )
        logits = outputs.logits[0]

    # è®¡ç®—å‡†ç¡®ç‡
    predictions = logits.argmax(dim=-1)
    valid_mask = (labels != -100)

    # åˆ†ç±»ç»Ÿè®¡
    mask_positions = []
    real_positions = []

    current_pos = prompt_len
    for seg_type, _, seg_len in block_info:
        if seg_type == 'mask':
            mask_positions.extend(range(current_pos, current_pos + seg_len))
        elif seg_type == 'real':
            real_positions.extend(range(current_pos, current_pos + seg_len))
        current_pos += seg_len

    # Prompt[-1]
    prompt_last_correct = 0
    prompt_last_total = 0
    if labels[prompt_len - 1].item() != -100:
        prompt_last_total = 1
        if predictions[prompt_len - 1].item() == labels[prompt_len - 1].item():
            prompt_last_correct = 1

    # Mask positions
    mask_positions_tensor = torch.tensor(mask_positions, device=device)
    mask_labels = labels[mask_positions_tensor]
    mask_valid = mask_labels != -100
    mask_total = mask_valid.sum().item()
    mask_correct = ((predictions[mask_positions_tensor] == mask_labels) & mask_valid).sum().item()

    # Real positions
    real_positions_tensor = torch.tensor(real_positions, device=device)
    real_labels = labels[real_positions_tensor]
    real_valid = real_labels != -100
    real_total = real_valid.sum().item()
    real_correct = ((predictions[real_positions_tensor] == real_labels) & real_valid).sum().item()

    # æ€»ä½“
    total_valid = valid_mask.sum().item()
    total_correct = ((predictions == labels) & valid_mask).sum().item()

    overall_acc = total_correct / total_valid if total_valid > 0 else 0
    mask_acc = mask_correct / mask_total if mask_total > 0 else 0
    real_acc = real_correct / real_total if real_total > 0 else 0
    prompt_acc = prompt_last_correct / prompt_last_total if prompt_last_total > 0 else 0

    results.append({
        'sample_idx': sample_idx,
        'overall_acc': overall_acc,
        'mask_acc': mask_acc,
        'real_acc': real_acc,
        'prompt_acc': prompt_acc,
        'mask_total': mask_total,
        'real_total': real_total,
        'total_valid': total_valid,
    })

    print(f"\næ ·æœ¬ {sample_idx:2d}:")
    print(f"  æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.4f} ({total_correct}/{total_valid})")
    print(f"  Prompt[-1]: {prompt_acc:.4f} ({prompt_last_correct}/{prompt_last_total})")
    print(f"  Mask:       {mask_acc:.4f} ({mask_correct}/{mask_total})")
    print(f"  Real:       {real_acc:.4f} ({real_correct}/{real_total})")

# ========== ç»Ÿè®¡æ±‡æ€» ==========
print(f"\n" + "="*100)
print("æ±‡æ€»ç»Ÿè®¡")
print("="*100)

import numpy as np

overall_accs = [r['overall_acc'] for r in results]
mask_accs = [r['mask_acc'] for r in results]
real_accs = [r['real_acc'] for r in results]
prompt_accs = [r['prompt_acc'] for r in results]

print(f"\næ€»ä½“å‡†ç¡®ç‡:")
print(f"  å¹³å‡: {np.mean(overall_accs):.4f}")
print(f"  ä¸­ä½æ•°: {np.median(overall_accs):.4f}")
print(f"  æœ€å°: {np.min(overall_accs):.4f}")
print(f"  æœ€å¤§: {np.max(overall_accs):.4f}")

print(f"\nMaskå‡†ç¡®ç‡:")
print(f"  å¹³å‡: {np.mean(mask_accs):.4f}")
print(f"  ä¸­ä½æ•°: {np.median(mask_accs):.4f}")
print(f"  æœ€å°: {np.min(mask_accs):.4f}")
print(f"  æœ€å¤§: {np.max(mask_accs):.4f}")

print(f"\nRealå‡†ç¡®ç‡:")
print(f"  å¹³å‡: {np.mean(real_accs):.4f}")
print(f"  ä¸­ä½æ•°: {np.median(real_accs):.4f}")
print(f"  æœ€å°: {np.min(real_accs):.4f}")
print(f"  æœ€å¤§: {np.max(real_accs):.4f}")

print(f"\nPrompt[-1]å‡†ç¡®ç‡:")
print(f"  å¹³å‡: {np.mean(prompt_accs):.4f}")

# åŠ æƒå¹³å‡ï¼ˆæŒ‰tokenæ•°é‡ï¼‰
total_mask_tokens = sum(r['mask_total'] for r in results)
total_real_tokens = sum(r['real_total'] for r in results)
total_tokens = sum(r['total_valid'] for r in results)

weighted_mask_acc = sum(r['mask_acc'] * r['mask_total'] for r in results) / total_mask_tokens if total_mask_tokens > 0 else 0
weighted_real_acc = sum(r['real_acc'] * r['real_total'] for r in results) / total_real_tokens if total_real_tokens > 0 else 0
weighted_overall_acc = sum(r['overall_acc'] * r['total_valid'] for r in results) / total_tokens if total_tokens > 0 else 0

print(f"\nåŠ æƒå¹³å‡å‡†ç¡®ç‡ï¼ˆæŒ‰tokenæ•°é‡ï¼‰:")
print(f"  æ€»ä½“: {weighted_overall_acc:.4f}")
print(f"  Mask: {weighted_mask_acc:.4f}")
print(f"  Real: {weighted_real_acc:.4f}")
