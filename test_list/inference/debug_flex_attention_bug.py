#!/usr/bin/env python3
"""
éªŒè¯FlexAttentionçš„bug

å·²ç»è¯æ˜:
1. æ‰‹åŠ¨è®¡ç®—çš„attention outputå®Œå…¨ç›¸åŒ âœ…
2. ä½†FlexAttentionçš„å®é™…outputä¸åŒ âŒ

ç°åœ¨ç›´æ¥å¯¹æ¯”:
- æ‰‹åŠ¨è®¡ç®—çš„attention output
- FlexAttentionçš„å®é™…output
"""

import os
os.environ["TORCH_COMPILE_DISABLE"] = "1"

import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from dllm_reasoning.trainer.interleaved_sft_dataset import InterleavedSFTDataset


def capture_both_outputs(model, input_ids, position_ids, block_info, prompt_len, device):
    """
    åŒæ—¶æ•è·:
    1. FlexAttentionçš„å®é™…è¾“å‡º
    2. æ‰‹åŠ¨è®¡ç®—çš„attention output (åŸºäºå¯è§ä½ç½®çš„Q/K/V)
    """
    m1_pos = prompt_len
    captured = {}

    def hook_forward(module, args, kwargs, output):
        """Hook self_attn.forward"""
        if len(args) >= 3:
            hidden_states = args[0]
            position_embeddings = args[1]
            attention_mask = args[2]
        else:
            hidden_states = kwargs.get('hidden_states', args[0] if len(args) > 0 else None)
            position_embeddings = kwargs.get('position_embeddings', args[1] if len(args) > 1 else None)
            attention_mask = kwargs.get('attention_mask', args[2] if len(args) > 2 else None)

        bsz, q_len = hidden_states.shape[:-1]
        hidden_shape = (bsz, q_len, -1, module.head_dim)

        # Q/K/V projection
        query_states = module.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states = module.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = module.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        # Apply RoPE
        import sys
        modeling_module = sys.modules.get(module.__class__.__module__)
        if modeling_module and hasattr(modeling_module, 'apply_rotary_pos_emb'):
            apply_rotary_pos_emb = modeling_module.apply_rotary_pos_emb
        else:
            import importlib
            modeling_module = importlib.import_module('transformers_modules.huggingface.modeling_dllm')
            apply_rotary_pos_emb = modeling_module.apply_rotary_pos_emb

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, cos, sin
        )

        # æå–Mâ‚ä½ç½®çš„Q
        q_m1 = query_states[0, :, m1_pos, :]  # [num_q_heads, head_dim]

        # æå–å¯è§ä½ç½®çš„K/V (0 to m1_pos)
        visible_len = m1_pos + 1
        k_visible = key_states[0, :, :visible_len, :]  # [num_kv_heads, visible_len, head_dim]
        v_visible = value_states[0, :, :visible_len, :]

        # GQA: é‡å¤K/V
        num_q_heads = q_m1.shape[0]
        num_kv_heads = k_visible.shape[0]
        num_groups = num_q_heads // num_kv_heads

        k_visible_repeated = k_visible.repeat_interleave(num_groups, dim=0)
        v_visible_repeated = v_visible.repeat_interleave(num_groups, dim=0)

        # æ‰‹åŠ¨è®¡ç®—attention
        scaling = 1.0 / (q_m1.shape[-1] ** 0.5)
        scores = torch.bmm(
            q_m1.unsqueeze(1),  # [num_q_heads, 1, head_dim]
            k_visible_repeated.transpose(1, 2)  # [num_q_heads, head_dim, visible_len]
        ).squeeze(1) * scaling  # [num_q_heads, visible_len]

        weights = torch.softmax(scores, dim=-1)  # [num_q_heads, visible_len]

        manual_output = torch.bmm(
            weights.unsqueeze(1),  # [num_q_heads, 1, visible_len]
            v_visible_repeated  # [num_q_heads, visible_len, head_dim]
        ).squeeze(1)  # [num_q_heads, head_dim]

        # å±•å¹³æˆ [num_q_heads * head_dim]
        manual_output_flat = manual_output.flatten()
        captured['manual_output'] = manual_output_flat.detach().clone()

        # FlexAttentionçš„å®é™…è¾“å‡ºä¼šåœ¨outputä¸­
        # output[0] æ˜¯ attn_output (after o_proj)
        # æˆ‘ä»¬éœ€è¦æ•è·before o_projçš„è¾“å‡º
        # æ‰€ä»¥éœ€è¦ä¿å­˜o_projçš„è¾“å…¥

        return output

    # åŒæ—¶hook o_projçš„è¾“å…¥æ¥æ•è·FlexAttentionçš„å®é™…è¾“å‡º(before o_proj)
    def hook_o_proj_input(module, input, output):
        # input[0] is attention output before o_proj
        # Shape: [batch, seq_len, num_q_heads * head_dim]
        attn_out_before_o = input[0][0, m1_pos, :].detach().clone()  # [num_q_heads * head_dim]
        captured['flex_output'] = attn_out_before_o

    layer0_attn = model.model.layers[0].self_attn
    hook_handle1 = layer0_attn.register_forward_hook(hook_forward, with_kwargs=True)
    hook_handle2 = layer0_attn.o_proj.register_forward_hook(hook_o_proj_input)

    # Forward pass
    with torch.no_grad():
        outputs = model(
            input_ids.unsqueeze(0),
            position_ids=position_ids.unsqueeze(0),
            block_info=[block_info],
            prompt_len=[prompt_len],
            seq_lens=[input_ids.size(0)],
            use_cache=False,
        )

    # Remove hooks
    hook_handle1.remove()
    hook_handle2.remove()

    return captured


def main():
    MODEL_PATH = "dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
    DATA_PATH = "data/openr1.parquet"

    print("="*80)
    print("ğŸ” FlexAttention BugéªŒè¯")
    print("="*80)
    print("\nç›®æ ‡: å¯¹æ¯”æ‰‹åŠ¨è®¡ç®—å’ŒFlexAttentionçš„å®é™…è¾“å‡º")
    print()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    ).to(device)

    model.train()

    dataset = InterleavedSFTDataset(
        parquet_files=DATA_PATH,
        tokenizer=tokenizer,
        prompt_key="prompt",
        response_key="target",
        block_size=4,
        max_length=6000,
        truncation="right",
    )

    sample = dataset[0]

    prompt_len = sample['prompt_len']
    input_ids_full = sample['input_ids'].to(device)
    position_ids_full = sample['position_ids'].to(device)

    print(f"æ ·æœ¬ä¿¡æ¯:")
    print(f"  Prompté•¿åº¦: {prompt_len}")
    print(f"  Mâ‚ä½ç½®: {prompt_len}")

    # é…ç½®1: [P][Mâ‚]
    print("\n" + "#"*80)
    print("é…ç½®1: [P][Mâ‚]")
    print("#"*80)

    truncate_pos_1 = prompt_len + 3
    input_ids_1 = input_ids_full[:truncate_pos_1]
    position_ids_1 = position_ids_full[:truncate_pos_1]
    block_info_1 = [('mask', 3)]

    captured_1 = capture_both_outputs(
        model, input_ids_1, position_ids_1, block_info_1, prompt_len, device
    )

    # é…ç½®2: [P][Mâ‚][Râ‚]
    print("\n" + "#"*80)
    print("é…ç½®2: [P][Mâ‚][Râ‚]")
    print("#"*80)

    truncate_pos_2 = prompt_len + 7
    input_ids_2 = input_ids_full[:truncate_pos_2]
    position_ids_2 = position_ids_full[:truncate_pos_2]
    block_info_2 = [('mask', 3), ('real', 4)]

    captured_2 = capture_both_outputs(
        model, input_ids_2, position_ids_2, block_info_2, prompt_len, device
    )

    # å¯¹æ¯”
    print("\n" + "="*80)
    print("ğŸ“Š å¯¹æ¯”ç»“æœ (Mâ‚ä½ç½®)")
    print("="*80)

    # æ‰‹åŠ¨è®¡ç®—çš„è¾“å‡º
    manual_1 = captured_1['manual_output']
    manual_2 = captured_2['manual_output']
    manual_diff = torch.norm(manual_1 - manual_2).item()

    print(f"\næ‰‹åŠ¨è®¡ç®—çš„Attention Output (åŸºäºå¯è§Q/K/V):")
    print(f"  L2è·ç¦»: {manual_diff:.10f}")
    print(f"  {'âœ… å®Œå…¨ç›¸åŒ' if manual_diff < 1e-5 else 'âŒ ä¸åŒ'}")

    # FlexAttentionçš„å®é™…è¾“å‡º
    flex_1 = captured_1['flex_output']
    flex_2 = captured_2['flex_output']
    flex_diff = torch.norm(flex_1 - flex_2).item()

    print(f"\nFlexAttentionçš„å®é™…è¾“å‡º:")
    print(f"  L2è·ç¦»: {flex_diff:.10f}")
    print(f"  {'âœ… å®Œå…¨ç›¸åŒ' if flex_diff < 1e-5 else 'âŒ ä¸åŒ'}")

    # å¯¹æ¯”é…ç½®1: æ‰‹åŠ¨ vs FlexAttention
    diff_1 = torch.norm(manual_1 - flex_1).item()
    print(f"\né…ç½®1 [P][Mâ‚]: æ‰‹åŠ¨ vs FlexAttention:")
    print(f"  L2è·ç¦»: {diff_1:.10f}")
    print(f"  {'âœ… ç›¸åŒ' if diff_1 < 1e-5 else 'âŒ ä¸åŒ'}")

    # å¯¹æ¯”é…ç½®2: æ‰‹åŠ¨ vs FlexAttention
    diff_2 = torch.norm(manual_2 - flex_2).item()
    print(f"\né…ç½®2 [P][Mâ‚][Râ‚]: æ‰‹åŠ¨ vs FlexAttention:")
    print(f"  L2è·ç¦»: {diff_2:.10f}")
    print(f"  {'âœ… ç›¸åŒ' if diff_2 < 1e-5 else 'âŒ ä¸åŒ'}")

    print("\n" + "="*80)
    print("ğŸ” æœ€ç»ˆç»“è®º")
    print("="*80)

    if manual_diff < 1e-5 and flex_diff >= 1e-5:
        print("\nâœ… ç¡®è®¤FlexAttentionæœ‰bugï¼")
        print("\nè¯æ®:")
        print(f"  1. æ‰‹åŠ¨è®¡ç®—(åŸºäºå¯è§Q/K/V): ä¸¤ç§é…ç½®å®Œå…¨ç›¸åŒ (L2={manual_diff:.2e})")
        print(f"  2. FlexAttentionå®é™…è¾“å‡º: ä¸¤ç§é…ç½®ä¸åŒ (L2={flex_diff:.2e})")
        print("\nè¿™è¯´æ˜:")
        print("  - FlexAttentionçš„maskå¹¶æ²¡æœ‰å®Œå…¨éš”ç¦»ä¸å¯è§ä½ç½®")
        print("  - æˆ–è€…FlexAttentionåœ¨ä¸åŒåºåˆ—é•¿åº¦ä¸‹æœ‰æ•°å€¼ç¨³å®šæ€§é—®é¢˜")
        print("  - Râ‚çš„å­˜åœ¨å½±å“äº†FlexAttentionçš„å†…éƒ¨è®¡ç®—,å³ä½¿Mâ‚ç†è®ºä¸Šçœ‹ä¸åˆ°Râ‚")

        if diff_1 < 1e-5 and diff_2 >= 1e-5:
            print("\nè¿›ä¸€æ­¥åˆ†æ:")
            print(f"  - é…ç½®1 [P][Mâ‚]: æ‰‹åŠ¨è®¡ç®— = FlexAttention âœ… (L2={diff_1:.2e})")
            print(f"  - é…ç½®2 [P][Mâ‚][Râ‚]: æ‰‹åŠ¨è®¡ç®— â‰  FlexAttention âŒ (L2={diff_2:.2e})")
            print("\n  è¿™è¯´æ˜åªæœ‰å½“åºåˆ—é•¿åº¦å¢åŠ æ—¶,FlexAttentionçš„è¡Œä¸ºæ‰å¼‚å¸¸!")
        elif diff_1 >= 1e-5 and diff_2 < 1e-5:
            print("\nè¿›ä¸€æ­¥åˆ†æ:")
            print(f"  - é…ç½®1 [P][Mâ‚]: æ‰‹åŠ¨è®¡ç®— â‰  FlexAttention âŒ (L2={diff_1:.2e})")
            print(f"  - é…ç½®2 [P][Mâ‚][Râ‚]: æ‰‹åŠ¨è®¡ç®— = FlexAttention âœ… (L2={diff_2:.2e})")
            print("\n  å¥‡æ€ªï¼è¿™ä¸ç¬¦åˆé¢„æœŸ...")
        elif diff_1 >= 1e-5 and diff_2 >= 1e-5:
            print("\nè¿›ä¸€æ­¥åˆ†æ:")
            print(f"  - é…ç½®1 [P][Mâ‚]: æ‰‹åŠ¨è®¡ç®— â‰  FlexAttention âŒ (L2={diff_1:.2e})")
            print(f"  - é…ç½®2 [P][Mâ‚][Râ‚]: æ‰‹åŠ¨è®¡ç®— â‰  FlexAttention âŒ (L2={diff_2:.2e})")
            print("\n  FlexAttentionåœ¨ä¸¤ç§é…ç½®ä¸‹éƒ½ä¸æ‰‹åŠ¨è®¡ç®—ä¸åŒ")
            print("  ä½†æ‰‹åŠ¨è®¡ç®—æœ¬èº«æ˜¯ä¸€è‡´çš„")
            print("\n  å¯èƒ½çš„åŸå› :")
            print("    - FlexAttentionçš„maskå®ç°ä¸æˆ‘ä»¬çš„å‡è®¾ä¸åŒ")
            print("    - éœ€è¦æ£€æŸ¥BlockMaskçš„å®é™…æ„é€ é€»è¾‘")
    elif manual_diff >= 1e-5:
        print("\nâŒ æ„å¤–ï¼šæ‰‹åŠ¨è®¡ç®—ä¸åŒï¼")
        print(f"  è¿™ä¸åº”è¯¥å‘ç”Ÿ (L2={manual_diff:.2e})")
    else:
        print("\nâ“ æ„å¤–ï¼šFlexAttentionä¹Ÿç›¸åŒï¼Ÿ")
        print(f"  FlexAttention L2={flex_diff:.2e}")
        print("  è¿™ä¸ä¹‹å‰çš„ç»“æœçŸ›ç›¾...")


if __name__ == "__main__":
    main()
