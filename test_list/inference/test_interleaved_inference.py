#!/usr/bin/env python3
"""
æµ‹è¯• Interleaved ç”Ÿæˆå™¨ (Next Block Prediction)

ç›®æ ‡:
1. éªŒè¯æ¨ç†ä»£ç çš„æ­£ç¡®æ€§
2. æµ‹è¯•æ¨¡å‹çš„å¤šæ­¥è§£ç èƒ½åŠ› (0 mask, 1 mask, 2 mask, ...)
3. å¯¹æ¯”ä¸åŒ mask æ•°é‡ä¸‹çš„ç”Ÿæˆç»“æœ

æµ‹è¯•åœºæ™¯:
- num_masks=0: æ ‡å‡†è‡ªå›å½’ (æ¯æ¬¡é¢„æµ‹1ä¸ªtoken)
- num_masks=1: è·³1æ­¥é¢„æµ‹ (æ¯æ¬¡é¢„æµ‹2ä¸ªtoken)
- num_masks=2: è·³2æ­¥é¢„æµ‹ (æ¯æ¬¡é¢„æµ‹3ä¸ªtoken)
- num_masks=3: è·³3æ­¥é¢„æµ‹ (æ¯æ¬¡é¢„æµ‹4ä¸ªtoken)
"""

import sys
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dllm_reasoning.inference.interleaved_generator import interleaved_generate

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_interleaved_generation(
    model_path: str,
    test_prompts: list,
    num_masks_list: list = [0, 1, 2, 3],
    max_new_tokens: int = 50,
):
    """
    æµ‹è¯•ä¸åŒ mask æ•°é‡ä¸‹çš„ç”Ÿæˆæ•ˆæœ

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        test_prompts: æµ‹è¯• prompt åˆ—è¡¨
        num_masks_list: è¦æµ‹è¯•çš„ mask æ•°é‡åˆ—è¡¨
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
    """
    print("="*100)
    print("ğŸš€ æµ‹è¯• Interleaved ç”Ÿæˆå™¨ (Next Block Prediction)")
    print("="*100)

    # ========== åŠ è½½æ¨¡å‹ ==========
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   è®¾å¤‡: {device}")

    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œé¿å…è·¯å¾„éªŒè¯é—®é¢˜
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        local_files_only=True,
    ).to(device).eval()

    print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # è·å–ç‰¹æ®Š token ID
    eos_token_id = tokenizer.eos_token_id
    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id else eos_token_id

    # è·å– mask token ID
    # æ£€æŸ¥æ˜¯å¦æœ‰ mask_token
    if hasattr(tokenizer, 'mask_token') and tokenizer.mask_token is not None:
        mask_token_id = tokenizer.mask_token_id
        print(f"   Mask token: {tokenizer.mask_token} (ID: {mask_token_id})")
    else:
        # å¦‚æœæ²¡æœ‰ mask_tokenï¼Œä½¿ç”¨ unk_token æˆ– eos_token
        if hasattr(tokenizer, 'unk_token') and tokenizer.unk_token is not None:
            mask_token_id = tokenizer.unk_token_id
            print(f"   âš ï¸  æ²¡æœ‰ mask_tokenï¼Œä½¿ç”¨ unk_token: {tokenizer.unk_token} (ID: {mask_token_id})")
        else:
            mask_token_id = eos_token_id
            print(f"   âš ï¸  æ²¡æœ‰ mask_token å’Œ unk_tokenï¼Œä½¿ç”¨ eos_token (ID: {mask_token_id})")

    print(f"   EOS token ID: {eos_token_id}")
    print(f"   PAD token ID: {pad_token_id}")

    # ========== å¯¹æ¯ä¸ª prompt è¿›è¡Œæµ‹è¯• ==========
    for prompt_idx, prompt in enumerate(test_prompts):
        print(f"\n" + "="*100)
        print(f"ğŸ“ Prompt {prompt_idx + 1}/{len(test_prompts)}")
        print(f"="*100)
        print(f"å†…å®¹: {prompt}")
        print()

        # å‡†å¤‡è¾“å…¥
        messages = [{"role": "user", "content": prompt}]
        input_ids = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(device)

        print(f"è¾“å…¥é•¿åº¦: {input_ids.size(1)} tokens")

        # å¯¹æ¯ä¸ª num_masks è¿›è¡Œæµ‹è¯•
        results = {}

        for num_masks in num_masks_list:
            print(f"\n{'-'*100}")
            print(f"ğŸ”¬ æµ‹è¯• num_masks={num_masks} (æ¯æ¬¡é¢„æµ‹ {num_masks+1} ä¸ª token)")
            print(f"{'-'*100}")

            try:
                # ç”Ÿæˆ
                with torch.no_grad():
                    output_ids = interleaved_generate(
                        model=model,
                        input_ids=input_ids,
                        eos_token_id=eos_token_id,
                        mask_token_id=mask_token_id,
                        pad_token_id=pad_token_id,
                        max_new_tokens=max_new_tokens,
                        num_masks=num_masks,
                        max_length=8192,
                        verbose=False,  # è®¾ä¸º True å¯ä»¥çœ‹åˆ°è¯¦ç»†è¿‡ç¨‹
                        tokenizer=tokenizer,
                    )

                # è§£ç 
                full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                generated_text = tokenizer.decode(
                    output_ids[0, input_ids.size(1):],
                    skip_special_tokens=True
                )

                # ç»Ÿè®¡
                total_len = output_ids.size(1)
                generated_len = total_len - input_ids.size(1)
                tokens_per_block = num_masks + 1
                num_blocks = (generated_len + tokens_per_block - 1) // tokens_per_block

                results[num_masks] = {
                    'generated_text': generated_text,
                    'generated_len': generated_len,
                    'num_blocks': num_blocks,
                    'success': True
                }

                print(f"âœ… ç”ŸæˆæˆåŠŸ")
                print(f"   ç”Ÿæˆé•¿åº¦: {generated_len} tokens")
                print(f"   å—æ•°: {num_blocks} blocks (æ¯å— {tokens_per_block} tokens)")
                print(f"   ç”Ÿæˆå†…å®¹: {generated_text[:200]}{'...' if len(generated_text) > 200 else ''}")

            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
                results[num_masks] = {
                    'generated_text': None,
                    'generated_len': 0,
                    'num_blocks': 0,
                    'success': False,
                    'error': str(e)
                }

        # ========== ç»“æœå¯¹æ¯” ==========
        print(f"\n{'='*100}")
        print(f"ğŸ“Š ç»“æœå¯¹æ¯” (Prompt {prompt_idx + 1})")
        print(f"{'='*100}")

        print(f"\n{'num_masks':<12} {'æ¯å—tokens':<12} {'å—æ•°':<10} {'æ€»tokens':<12} {'çŠ¶æ€':<10}")
        print(f"{'-'*60}")
        for num_masks in num_masks_list:
            res = results[num_masks]
            tokens_per_block = num_masks + 1
            status = "âœ… æˆåŠŸ" if res['success'] else "âŒ å¤±è´¥"
            print(f"{num_masks:<12} {tokens_per_block:<12} {res['num_blocks']:<10} {res['generated_len']:<12} {status:<10}")

        # æ£€æŸ¥ç”Ÿæˆçš„å¤šæ ·æ€§
        print(f"\nç”Ÿæˆå†…å®¹æ˜¯å¦ç›¸åŒ:")
        unique_texts = set()
        for num_masks in num_masks_list:
            if results[num_masks]['success']:
                text = results[num_masks]['generated_text']
                unique_texts.add(text)

        if len(unique_texts) == 1:
            print(f"   âœ… æ‰€æœ‰ num_masks ç”Ÿæˆçš„å†…å®¹å®Œå…¨ç›¸åŒ")
            print(f"   è¯´æ˜: æ¨¡å‹å¯¹ä¸åŒçš„ mask æ•°é‡ç»™å‡ºäº†ä¸€è‡´çš„é¢„æµ‹")
        else:
            print(f"   âš ï¸  ä¸åŒ num_masks ç”Ÿæˆäº†ä¸åŒçš„å†…å®¹ ({len(unique_texts)} ç§)")
            print(f"   è¯´æ˜: ä¸åŒçš„ mask æ•°é‡å¯èƒ½å½±å“äº†é¢„æµ‹ç»“æœ")

    print(f"\n{'='*100}")
    print(f"âœ… æµ‹è¯•å®Œæˆ")
    print(f"{'='*100}")


def main():
    """ä¸»å‡½æ•°"""
    # ========== é…ç½® ==========
    # ä½¿ç”¨ Interleaved SFT è®­ç»ƒå®Œæˆçš„æ¨¡å‹
    MODEL_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"

    TEST_PROMPTS = [
        "What is 2+2?",
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
        "Solve: If x + 5 = 10, what is x?",
    ]

    NUM_MASKS_LIST = [0, 1, 2, 3]  # æµ‹è¯•ä¸åŒçš„ mask æ•°é‡
    MAX_NEW_TOKENS = 50  # é™åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œä¾¿äºå¿«é€Ÿæµ‹è¯•

    # ========== è¿è¡Œæµ‹è¯• ==========
    test_interleaved_generation(
        model_path=MODEL_PATH,
        test_prompts=TEST_PROMPTS,
        num_masks_list=NUM_MASKS_LIST,
        max_new_tokens=MAX_NEW_TOKENS,
    )


if __name__ == "__main__":
    main()
