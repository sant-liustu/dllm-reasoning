#!/usr/bin/env python3
"""
æµ‹è¯•çº¯causalè§£ç ï¼ˆä¸ä½¿ç”¨Mask tokensï¼‰
éªŒè¯æ¨¡å‹åŸºç¡€ç”Ÿæˆèƒ½åŠ›æ˜¯å¦æ­£å¸¸
"""

import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd

# ç¦ç”¨torch.compile
import torch._dynamo
torch._dynamo.config.disable = True

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

MODEL_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
DATA_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/data/openr1.parquet"

print("="*100)
print("æµ‹è¯•çº¯Causalè§£ç  (ä¸ä½¿ç”¨Mask)")
print("="*100)

# ========== åŠ è½½æ¨¡å‹ ==========
print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    local_files_only=True,
).to(device).eval()  # ä½¿ç”¨evalæ¨¡å¼

print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ (eval mode)")

# ========== åŠ è½½è®­ç»ƒæ•°æ® ==========
print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {DATA_PATH}")
df = pd.read_parquet(DATA_PATH)
sample = df.iloc[0]

# å¤„ç†æ•°æ®
prompt = sample['prompt']
target = sample['target']

messages = list(prompt) if isinstance(prompt, (list, tuple)) else prompt
messages = messages.tolist() if hasattr(messages, 'tolist') else messages

target_list = target.tolist() if hasattr(target, 'tolist') else target
if isinstance(target_list, list) and len(target_list) > 0:
    target_content = target_list[0].get('content', '') if isinstance(target_list[0], dict) else str(target_list[0])
else:
    target_content = str(target_list)

# å»æ‰<think> - å¯¹é½è®­ç»ƒæ•°æ®å¤„ç†é€»è¾‘
if target_content.strip().startswith('<think>'):
    think_start = target_content.find('<think>')
    target_content = target_content[think_start + 7:].lstrip()

print(f"\nğŸ“ Promptæœ€åéƒ¨åˆ†: ...{messages[-1]['content'][-100:]}")
print(f"\nğŸ“ Targetå¼€å¤´: {target_content[:200]}")

# ========== é‡è¦ï¼šå¯¹é½è®­ç»ƒæ•°æ®å¤„ç†é€»è¾‘ ==========
# è®­ç»ƒæ—¶æ˜¯è¿™æ ·å¤„ç†çš„ï¼ˆè§ interleaved_sft_dataset.py:502-514ï¼‰ï¼š
# 1. å…ˆå°†promptè½¬æˆå­—ç¬¦ä¸²ï¼ˆadd_generation_prompt=Trueï¼‰
# 2. ç„¶åç›´æ¥å­—ç¬¦ä¸²æ‹¼æ¥response
# 3. æœ€åä¸€èµ·tokenize

# æ­¥éª¤1: Promptè½¬å­—ç¬¦ä¸²
prompt_only_str = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=False
)

# æ­¥éª¤2: å­—ç¬¦ä¸²æ‹¼æ¥ï¼ˆresponse + eosï¼‰
full_conversation_str = prompt_only_str + target_content + tokenizer.eos_token

# æ­¥éª¤3: åˆ†åˆ«tokenize
prompt_ids = tokenizer(prompt_only_str, return_tensors="pt", add_special_tokens=False)["input_ids"][0].to(device)
full_input_ids = tokenizer(full_conversation_str, return_tensors="pt", add_special_tokens=False)["input_ids"][0].to(device)

prompt_len = prompt_ids.size(0)
response_ids = full_input_ids[prompt_len:]

print(f"\nğŸ“ åºåˆ—é•¿åº¦:")
print(f"   Prompt: {prompt_len}")
print(f"   Response: {response_ids.size(0)}")
print(f"   Total: {full_input_ids.size(0)}")

# ========== æµ‹è¯•1: Teacher Forcing - ä½¿ç”¨çœŸå®tokensè®¡ç®—logits ==========
print(f"\n" + "="*100)
print("æµ‹è¯•1: Teacher Forcing (ä½¿ç”¨çœŸå®response tokens)")
print("="*100)

with torch.no_grad():
    outputs = model(
        full_input_ids.unsqueeze(0),
        use_cache=False
    )
    logits = outputs.logits[0]  # [seq_len, vocab]

# åˆ†æå‰20ä¸ªresponse positionsçš„é¢„æµ‹å‡†ç¡®æ€§
print(f"\nğŸ“Š Responseå‰20ä¸ªä½ç½®çš„é¢„æµ‹:")
predictions = logits.argmax(dim=-1)

num_correct = 0
num_total = 0

for i in range(min(20, response_ids.size(0))):
    # Position prompt_len-1+i é¢„æµ‹ position prompt_len+i
    pred_pos = prompt_len - 1 + i
    true_pos = prompt_len + i

    pred_id = predictions[pred_pos].item()
    true_id = full_input_ids[true_pos].item()

    is_correct = pred_id == true_id
    if is_correct:
        num_correct += 1
    num_total += 1

    status = "âœ…" if is_correct else "âŒ"
    pred_text = tokenizer.decode([pred_id])
    true_text = tokenizer.decode([true_id])

    print(f"   [{i:2d}] pos {pred_pos:4d}â†’{true_pos:4d}: {status} pred={pred_id:6d} '{pred_text}', true={true_id:6d} '{true_text}'")

accuracy_tf = num_correct / num_total if num_total > 0 else 0
print(f"\n   Teacher Forcing å‡†ç¡®ç‡: {accuracy_tf:.4f} ({num_correct}/{num_total})")

# ========== æµ‹è¯•2: Auto-regressive Generation - é€ä¸ªç”Ÿæˆ ==========
print(f"\n" + "="*100)
print("æµ‹è¯•2: Auto-regressive Generation (é€ä¸ªç”Ÿæˆ)")
print("="*100)

# ä»promptå¼€å§‹ï¼Œé€ä¸ªç”Ÿæˆ20ä¸ªtokens
generated_ids = prompt_ids.clone()
num_generate = 20

print(f"\nğŸš€ ç”Ÿæˆ{num_generate}ä¸ªtokens:")

for step in range(num_generate):
    with torch.no_grad():
        outputs = model(
            generated_ids.unsqueeze(0),
            use_cache=False
        )
        next_token_logits = outputs.logits[0, -1, :]  # Last position
        next_token_id = next_token_logits.argmax().item()

    # æ·»åŠ ç”Ÿæˆçš„token
    generated_ids = torch.cat([generated_ids, torch.tensor([next_token_id], device=device)])

    # å¯¹æ¯”çœŸå®token
    true_token_id = response_ids[step].item() if step < response_ids.size(0) else -1
    is_correct = next_token_id == true_token_id

    status = "âœ…" if is_correct else "âŒ"
    gen_text = tokenizer.decode([next_token_id])
    true_text = tokenizer.decode([true_token_id]) if true_token_id != -1 else "N/A"

    print(f"   Step {step:2d}: {status} gen={next_token_id:6d} '{gen_text}', true={true_token_id:6d} '{true_text}'")

# è®¡ç®—ARç”Ÿæˆå‡†ç¡®ç‡
generated_response = generated_ids[prompt_len:]
num_correct_ar = 0
for i in range(min(num_generate, response_ids.size(0))):
    if generated_response[i].item() == response_ids[i].item():
        num_correct_ar += 1

accuracy_ar = num_correct_ar / num_generate
print(f"\n   Auto-regressive å‡†ç¡®ç‡: {accuracy_ar:.4f} ({num_correct_ar}/{num_generate})")

# ========== æ€»ç»“ ==========
print(f"\n" + "="*100)
print("æ€»ç»“")
print("="*100)
print(f"Teacher Forcingå‡†ç¡®ç‡: {accuracy_tf:.4f} - è¯´æ˜æ¨¡å‹åœ¨çœ‹åˆ°çœŸå®tokensæ—¶çš„é¢„æµ‹èƒ½åŠ›")
print(f"Auto-regressiveå‡†ç¡®ç‡: {accuracy_ar:.4f} - è¯´æ˜æ¨¡å‹é€æ­¥ç”Ÿæˆæ—¶çš„å‡†ç¡®æ€§")
print(f"\nå¦‚æœä¸¤ä¸ªå‡†ç¡®ç‡éƒ½å¾ˆé«˜(>0.9)ï¼Œè¯´æ˜ï¼š")
print(f"  âœ… æ¨¡å‹åŸºç¡€è®­ç»ƒæ­£å¸¸")
print(f"  âœ… Labelsè®¾ç½®æ­£ç¡®")
print(f"  âœ… çº¯causalè§£ç åŠŸèƒ½æ­£å¸¸")
print(f"  âš ï¸  é—®é¢˜å¯èƒ½å‡ºåœ¨Maskæœºåˆ¶çš„å®ç°ä¸Š")
print(f"\nå¦‚æœå‡†ç¡®ç‡éƒ½å¾ˆä½(<0.5)ï¼Œè¯´æ˜ï¼š")
print(f"  âŒ æ¨¡å‹è®­ç»ƒå¯èƒ½ä¸å……åˆ†")
print(f"  âŒ æˆ–è€…ä½¿ç”¨äº†é”™è¯¯çš„checkpoint")
