#!/usr/bin/env python3
"""
ä½¿ç”¨hooksè¯¦ç»†æ‹†è§£Layer 0çš„è®¡ç®—è¿‡ç¨‹ï¼Œæ‰¾å‡º[P][Mâ‚]å’Œ[P][Mâ‚][Râ‚]å·®å¼‚çš„æ ¹æº

é€šè¿‡åœ¨æ¨¡å‹å…³é”®ä½ç½®æ³¨å†Œhooksæ¥æ•è·ï¼š
1. Embeddingè¾“å‡º
2. Position IDs
3. Layer 0çš„å„ä¸ªä¸­é—´æ­¥éª¤
"""

import os
os.environ["TORCH_COMPILE_DISABLE"] = "1"

import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from dllm_reasoning.trainer.interleaved_sft_dataset import InterleavedSFTDataset


def capture_layer0_details(model, input_ids, position_ids, block_info, prompt_len, device):
    """
    ä½¿ç”¨hooksæ•è·Layer 0çš„è¯¦ç»†è®¡ç®—è¿‡ç¨‹

    Returns:
        åŒ…å«å„æ­¥éª¤Mâ‚ä½ç½®tensorçš„å­—å…¸
    """
    m1_pos = prompt_len
    captured = {}

    # Hook 1: Capture embedding output
    def hook_embedding(module, input, output):
        # output shape: [batch, seq_len, hidden_dim]
        captured['embedding'] = output[0, m1_pos].detach().clone()

    # Hook 2: Capture Layer 0 input (after input_layernorm)
    def hook_layer0_input_norm(module, input, output):
        captured['layer0_input_normed'] = output[0, m1_pos].detach().clone()

    # Hook 3: Capture Layer 0 Q/K/V projections
    def hook_q_proj(module, input, output):
        # output shape: [batch, seq_len, num_heads * head_dim]
        captured['layer0_q_proj'] = output[0, m1_pos].detach().clone()

    def hook_k_proj(module, input, output):
        captured['layer0_k_proj'] = output[0, m1_pos].detach().clone()

    def hook_v_proj(module, input, output):
        captured['layer0_v_proj'] = output[0, m1_pos].detach().clone()

    # Hook 4: Capture attention output (before o_proj)
    def hook_attn_output(module, input, output):
        # output is tuple: (attn_output, attn_weights)
        # attn_output shape: [batch, seq_len, hidden_dim]
        captured['layer0_attn_output'] = output[0][0, m1_pos].detach().clone()

    # Hook 5: Capture Layer 0 output
    def hook_layer0_output(module, input, output):
        # output is tuple: (hidden_states, ...)
        captured['layer0_output'] = output[0][0, m1_pos].detach().clone()

    # Register hooks
    layer0 = model.model.layers[0]
    hooks = []

    hooks.append(model.model.embed_tokens.register_forward_hook(hook_embedding))
    hooks.append(layer0.input_layernorm.register_forward_hook(hook_layer0_input_norm))
    hooks.append(layer0.self_attn.q_proj.register_forward_hook(hook_q_proj))
    hooks.append(layer0.self_attn.k_proj.register_forward_hook(hook_k_proj))
    hooks.append(layer0.self_attn.v_proj.register_forward_hook(hook_v_proj))
    hooks.append(layer0.self_attn.register_forward_hook(hook_attn_output))
    hooks.append(layer0.register_forward_hook(hook_layer0_output))

    # Capture position IDs
    captured['position_id'] = position_ids[m1_pos].item()

    # Forward pass
    with torch.no_grad():
        outputs = model(
            input_ids.unsqueeze(0),
            position_ids=position_ids.unsqueeze(0),
            block_info=[block_info],
            prompt_len=[prompt_len],
            seq_lens=[input_ids.size(0)],
            use_cache=False,
        )

    # Remove hooks
    for hook in hooks:
        hook.remove()

    return captured


def compare_two_configs(model, tokenizer, sample, device):
    """
    å¯¹æ¯”[P][Mâ‚]å’Œ[P][Mâ‚][Râ‚]ä¸¤ç§é…ç½®çš„è¯¦ç»†è®¡ç®—è¿‡ç¨‹
    """
    print("="*80)
    print("è¯¦ç»†å¯¹æ¯”ï¼š[P][Mâ‚] vs [P][Mâ‚][Râ‚]")
    print("="*80)

    prompt_len = sample['prompt_len']
    input_ids_full = sample['input_ids'].to(device)
    position_ids_full = sample['position_ids'].to(device)

    # é…ç½®1: [P][Mâ‚]
    print("\n" + "#"*80)
    print("é…ç½®1: [P][Mâ‚]")
    print("#"*80)

    truncate_pos_1 = prompt_len + 3
    input_ids_1 = input_ids_full[:truncate_pos_1]
    position_ids_1 = position_ids_full[:truncate_pos_1]
    block_info_1 = [('mask', 3)]

    print(f"\nåºåˆ—é•¿åº¦: {input_ids_1.size(0)}")
    print(f"Block info: {block_info_1}")
    print(f"Mâ‚ position: {prompt_len}")
    print(f"Mâ‚ position_id: {position_ids_1[prompt_len].item()}")

    results_1 = capture_layer0_details(
        model, input_ids_1, position_ids_1, block_info_1, prompt_len, device
    )

    # é…ç½®2: [P][Mâ‚][Râ‚]
    print("\n" + "#"*80)
    print("é…ç½®2: [P][Mâ‚][Râ‚]")
    print("#"*80)

    truncate_pos_2 = prompt_len + 7
    input_ids_2 = input_ids_full[:truncate_pos_2]
    position_ids_2 = position_ids_full[:truncate_pos_2]
    block_info_2 = [('mask', 3), ('real', 4)]

    print(f"\nåºåˆ—é•¿åº¦: {input_ids_2.size(0)}")
    print(f"Block info: {block_info_2}")
    print(f"Mâ‚ position: {prompt_len}")
    print(f"Mâ‚ position_id: {position_ids_2[prompt_len].item()}")

    results_2 = capture_layer0_details(
        model, input_ids_2, position_ids_2, block_info_2, prompt_len, device
    )

    # ========== å¯¹æ¯”åˆ†æ ==========
    print("\n" + "="*80)
    print("ğŸ“Š å¯¹æ¯”åˆ†æ")
    print("="*80)

    # 1. Position IDså¯¹æ¯”
    print("\n1ï¸âƒ£  Position IDs")
    print(f"  é…ç½®1 Mâ‚ position_id: {results_1['position_id']}")
    print(f"  é…ç½®2 Mâ‚ position_id: {results_2['position_id']}")
    if results_1['position_id'] == results_2['position_id']:
        print("  âœ… Position IDsç›¸åŒ")
    else:
        print(f"  âŒ Position IDsä¸åŒï¼å·®å¼‚: {abs(results_1['position_id'] - results_2['position_id'])}")
        print("  âš ï¸ è¿™æ˜¯RoPEç¼–ç ä¸åŒçš„æ ¹æºï¼")

    # 2. å…¶ä»–ä¸­é—´æ­¥éª¤å¯¹æ¯”
    comparisons = [
        ('embedding', 'Embedding'),
        ('layer0_input_normed', 'Input LayerNorm'),
        ('layer0_q_proj', 'Q projection'),
        ('layer0_k_proj', 'K projection'),
        ('layer0_v_proj', 'V projection'),
        ('layer0_attn_output', 'Attention output'),
        ('layer0_output', 'Layer 0 output'),
    ]

    print(f"\n{'æ­¥éª¤':^30} | {'L2è·ç¦»':^15} | {'ä½™å¼¦ç›¸ä¼¼åº¦':^15} | {'æ˜¯å¦ç›¸åŒ':^10}")
    print("-"*80)

    for key, name in comparisons:
        if key not in results_1 or key not in results_2:
            continue

        t1 = results_1[key]
        t2 = results_2[key]

        # Flatten if multi-dimensional
        if t1.dim() > 1:
            t1 = t1.flatten()
            t2 = t2.flatten()

        l2_dist = torch.norm(t1 - t2).item()
        cos_sim = torch.nn.functional.cosine_similarity(
            t1.unsqueeze(0), t2.unsqueeze(0)
        ).item()

        is_same = "âœ…" if l2_dist < 1e-5 else "âŒ"

        print(f"{name:^30} | {l2_dist:>12.6f}  | {cos_sim:>12.6f}  | {is_same:^10}")

    # ========== å…³é”®å‘ç° ==========
    print("\n" + "="*80)
    print("ğŸ” å…³é”®å‘ç°")
    print("="*80)

    # æ£€æŸ¥embeddingæ˜¯å¦ç›¸åŒ
    emb_diff = torch.norm(results_1['embedding'] - results_2['embedding']).item()
    if emb_diff < 1e-5:
        print("âœ… Embeddingå±‚ï¼šMâ‚çš„embeddingå®Œå…¨ç›¸åŒ")
    else:
        print(f"âŒ Embeddingå±‚ï¼šMâ‚çš„embeddingä¸åŒï¼å·®å¼‚: {emb_diff:.6f}")
        print("   è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºMâ‚çš„input_idåº”è¯¥ç›¸åŒ")
        return

    # æ£€æŸ¥position IDs
    if results_1['position_id'] != results_2['position_id']:
        print(f"\nâŒ Position IDsä¸åŒï¼")
        print(f"   é…ç½®1 [P][Mâ‚]: position_id = {results_1['position_id']}")
        print(f"   é…ç½®2 [P][Mâ‚][Râ‚]: position_id = {results_2['position_id']}")
        print(f"\n   âš ï¸ å…³é”®å‘ç°ï¼šè™½ç„¶Mâ‚çœ‹ä¸åˆ°Râ‚ï¼ˆé€šè¿‡FlexAttention maské˜»æ­¢ï¼‰ï¼Œ")
        print(f"   ä½†position_idæœ¬èº«çš„å·®å¼‚å¯¼è‡´RoPEç¼–ç ä¸åŒï¼")
        print(f"   RoPEåœ¨Q/KæŠ•å½±ååº”ç”¨ï¼Œposition_idä¸åŒä¼šå¯¼è‡´Q/Kçš„ä½ç½®ç¼–ç ä¸åŒï¼Œ")
        print(f"   è¿›è€Œå½±å“attentionè®¡ç®—ï¼Œæœ€ç»ˆå¯¼è‡´Mâ‚çš„è¾“å‡ºä¸åŒã€‚")
    else:
        print("âœ… Position IDsç›¸åŒ")

        # è¿›ä¸€æ­¥åˆ†æattention outputå·®å¼‚
        attn_diff = torch.norm(results_1['layer0_attn_output'] - results_2['layer0_attn_output']).item()
        if attn_diff > 1e-5:
            print(f"\nâŒ Attention Outputä¸åŒï¼å·®å¼‚: {attn_diff:.6f}")
            print(f"\n   âš ï¸ å…³é”®å‘ç°ï¼šè™½ç„¶Position IDsç›¸åŒï¼ŒQ/K/V projectionä¹Ÿç›¸åŒï¼Œ")
            print(f"   ä½†Attentionè¾“å‡ºå´ä¸åŒï¼")
            print(f"\n   åŸå› åˆ†æï¼š")
            print(f"   1. é…ç½®1 [P][Mâ‚]: åºåˆ—é•¿åº¦280ï¼ŒMâ‚çš„Qå¯ä»¥attendåˆ°çš„K/Væ¥è‡ª280ä¸ªä½ç½®")
            print(f"   2. é…ç½®2 [P][Mâ‚][Râ‚]: åºåˆ—é•¿åº¦284ï¼ŒMâ‚çš„Qå¯ä»¥attendåˆ°çš„K/Væ¥è‡ª284ä¸ªä½ç½®")
            print(f"\n   è™½ç„¶FlexAttentionçš„BlockMaské˜»æ­¢Mâ‚ç›´æ¥attendåˆ°Râ‚çš„tokenï¼Œ")
            print(f"   ä½†æ˜¯K/VçŸ©é˜µçš„**å½¢çŠ¶ä¸åŒ**ï¼ˆ280 vs 284ï¼‰ï¼Œè¿™ä¼šå½±å“attentionè®¡ç®—ï¼")
            print(f"\n   æ›´æ·±å±‚çš„åŸå› ï¼š")
            print(f"   - FlexAttention maskæ˜¯åŠ¨æ€çš„ï¼Œå–å†³äºblock_infoå’Œåºåˆ—é•¿åº¦")
            print(f"   - [P][Mâ‚]çš„mask: åªæœ‰prompt+maskçš„280ä¸ªä½ç½®")
            print(f"   - [P][Mâ‚][Râ‚]çš„mask: æœ‰prompt+mask+realçš„284ä¸ªä½ç½®")
            print(f"   - å³ä½¿Mâ‚ä¸èƒ½ç›´æ¥attendåˆ°Râ‚ï¼Œä½†maskçš„æ•´ä½“ç»“æ„ä¸åŒï¼Œ")
            print(f"     å¯¼è‡´attention patternå‘ç”Ÿå˜åŒ–ï¼ˆä¾‹å¦‚attention weightsçš„å½’ä¸€åŒ–ï¼‰")
        else:
            print("   ä½†attention outputç›¸åŒï¼Œå·®å¼‚å¯èƒ½åœ¨MLPæˆ–residualä¸­...")


def main():
    MODEL_PATH = "dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
    DATA_PATH = "data/openr1.parquet"

    print("åŠ è½½æ¨¡å‹...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    ).to(device)

    model.train()

    print("åŠ è½½æ•°æ®é›†...")
    dataset = InterleavedSFTDataset(
        parquet_files=DATA_PATH,
        tokenizer=tokenizer,
        prompt_key="prompt",
        response_key="target",
        block_size=4,
        max_length=6000,
        truncation="right",
    )

    sample = dataset[0]

    print(f"\næ ·æœ¬ä¿¡æ¯:")
    print(f"  Prompté•¿åº¦: {sample['prompt_len']}")
    print(f"  æ€»åºåˆ—é•¿åº¦: {sample['input_ids'].shape[0]}")

    # è¿è¡Œè¯¦ç»†å¯¹æ¯”
    compare_two_configs(model, tokenizer, sample, device)


if __name__ == "__main__":
    main()
