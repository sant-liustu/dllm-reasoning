#!/usr/bin/env python3
"""
æ‰¹é‡æ¨ç†è„šæœ¬ï¼šæµ‹è¯•æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æ‹Ÿåˆèƒ½åŠ›

âœ… å·²å¯¹é½è®­ç»ƒæ—¶çš„æ•°æ®å¤„ç†æ–¹å¼ï¼ˆapply_chat_templateï¼‰

ç”¨é€”ï¼š
1. è¯»å–è®­ç»ƒæ•°æ®ï¼ˆparquet æ ¼å¼ï¼Œchat messagesï¼‰
2. ä½¿ç”¨ apply_chat_template å¤„ç† promptï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰
3. å¯¹æ¯ä¸ª prompt è¿›è¡Œæ¨ç†
4. ä¿å­˜ prompt + inference ç»“æœ
5. å¯ä¸åŸå§‹ target å¯¹æ¯”ï¼ŒéªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®å­¦ä¹ 

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/batch_inference.py \
        --model_path checkpoints/my_exp/global_step_1000/huggingface \
        --data_file data/train.parquet \
        --output_file results/train_inference.jsonl \
        --num_samples 100 \
        --batch_size 4

æ³¨æ„ï¼š
- prompt_key å’Œ target_key åº”ä¸º chat messages æ ¼å¼çš„åˆ—
- tokenization æ–¹å¼ä¸è®­ç»ƒè„šæœ¬ (sft_dataset.py) å®Œå…¨å¯¹é½
"""

import sys
import json
import argparse
from pathlib import Path
from tqdm import tqdm

import torch
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dllm_reasoning.inference.generator import iterative_generate


def load_data(data_file, prompt_key, target_key, num_samples=None):
    """
    åŠ è½½æ•°æ®æ–‡ä»¶ï¼ˆå¯¹é½è®­ç»ƒæ—¶çš„æ•°æ®å¤„ç†æ–¹å¼ï¼‰

    Args:
        data_file: parquet æ–‡ä»¶è·¯å¾„
        prompt_key: prompt åˆ—åï¼ˆchat messages æ ¼å¼ï¼‰
        target_key: target åˆ—åï¼ˆchat messages æ ¼å¼ï¼‰
        num_samples: é‡‡æ ·æ•°é‡ï¼ˆNone=å…¨éƒ¨ï¼‰

    Returns:
        list of dict: [{"prompt_messages": ..., "target_content": ...}, ...]
        - prompt_messages: åŸå§‹ chat messagesï¼ˆå¾… apply_chat_templateï¼‰
        - target_content: assistant å›å¤çš„æ–‡æœ¬å†…å®¹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    """
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_file}")

    # è¯»å– parquet æ–‡ä»¶
    df = pd.read_parquet(data_file)

    print(f"   æ€»æ ·æœ¬æ•°: {len(df)}")

    # æ£€æŸ¥åˆ—åæ˜¯å¦å­˜åœ¨
    if prompt_key not in df.columns:
        raise ValueError(f"æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ° '{prompt_key}' åˆ—ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}")
    if target_key not in df.columns:
        raise ValueError(f"æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ° '{target_key}' åˆ—ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}")

    # é‡‡æ ·
    if num_samples is not None and num_samples < len(df):
        df = df.sample(n=num_samples, random_state=42)
        print(f"   é‡‡æ ·æ•°é‡: {num_samples}")

    # è½¬æ¢ä¸º list of dict
    data = []
    for _, row in df.iterrows():
        prompt_val = row[prompt_key]
        target_val = row[target_key]

        # å¤„ç† prompt: ä¿ç•™åŸå§‹ chat messages æ ¼å¼
        if hasattr(prompt_val, 'tolist'):  # numpy array
            prompt_messages = prompt_val.tolist()
        elif isinstance(prompt_val, (list, tuple)):
            prompt_messages = list(prompt_val)
        else:
            # å¦‚æœä¸æ˜¯æ¶ˆæ¯æ ¼å¼,åˆ›å»ºä¸€ä¸ªç®€å•çš„ user æ¶ˆæ¯
            prompt_messages = [{"role": "user", "content": str(prompt_val)}]

        # å¤„ç† target: æå– assistant å›å¤çš„æ–‡æœ¬å†…å®¹
        if hasattr(target_val, 'tolist'):  # numpy array
            target_list = target_val.tolist()
            if isinstance(target_list, list) and len(target_list) > 0 and isinstance(target_list[0], dict):
                # æå– assistant æ¶ˆæ¯çš„ content
                target_content = '\n'.join([msg.get('content', '') for msg in target_list if msg.get('role') == 'assistant'])
            else:
                target_content = str(target_list)
        elif isinstance(target_val, (list, tuple)):
            target_content = '\n'.join([msg.get('content', '') for msg in target_val if msg.get('role') == 'assistant'])
        else:
            target_content = str(target_val)

        data.append({
            "prompt_messages": prompt_messages,  # åŸå§‹æ¶ˆæ¯æ ¼å¼
            "target_content": target_content      # æ–‡æœ¬å†…å®¹
        })

    return data


def batch_inference(model, tokenizer, prompt_messages_list, batch_size,
                    add_eos_length=127, refine_iter=2, max_new_tokens=1024):
    """
    æ‰¹é‡æ¨ç†ï¼ˆå¯¹é½è®­ç»ƒæ—¶çš„ tokenization æ–¹å¼ï¼‰

    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        prompt_messages_list: list of chat messagesï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨ï¼‰
        batch_size: æ‰¹å¤§å°
        add_eos_length: æ¯å—æ·»åŠ çš„ EOS æ•°é‡
        refine_iter: refine è½®æ•°
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°

    Returns:
        list of str: ç”Ÿæˆçš„ç»“æœ
    """
    responses = []

    # åˆ†æ‰¹å¤„ç†
    import sys
    for i in tqdm(range(0, len(prompt_messages_list), batch_size), desc="æ¨ç†è¿›åº¦"):
        batch_prompt_messages = prompt_messages_list[i:i+batch_size]

        print(f"\n[Batch {i//batch_size + 1}/{(len(prompt_messages_list) + batch_size - 1)//batch_size}] å¤„ç†æ ·æœ¬ {i} ~ {min(i+batch_size, len(prompt_messages_list))-1}")
        sys.stdout.flush()

        # å‡†å¤‡è¾“å…¥ - ä½¿ç”¨ apply_chat_templateï¼ˆå¯¹é½è®­ç»ƒï¼‰
        batch_inputs = []
        for messages in batch_prompt_messages:
            # ä½¿ç”¨ apply_chat_templateï¼Œæ·»åŠ  generation promptï¼ˆå¯¹é½è®­ç»ƒ sft_dataset.py:169ï¼‰
            input_ids = tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            )
            # squeeze å»æ‰ batch ç»´åº¦ (1, seq_len) -> (seq_len,)
            batch_inputs.append(input_ids.squeeze(0))

        # Padding
        from torch.nn.utils.rnn import pad_sequence
        input_ids = pad_sequence(
            batch_inputs,
            batch_first=True,
            padding_value=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id
        ).to(model.device)

        print(f"  è¾“å…¥åºåˆ—é•¿åº¦: {[len(inp) for inp in batch_inputs]}")

        # ğŸ” DEBUG: æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å…¥è§£ç ç»“æœ
        if i == 0:
            print(f"\n  ğŸ” DEBUG - æ ·æœ¬ {i} çš„è¾“å…¥ token è§£ç :")
            print(f"  " + "="*76)
            input_decoded = tokenizer.decode(batch_inputs[0], skip_special_tokens=False)
            print(f"  {input_decoded}")
            print(f"  " + "="*76)
            print(f"  è¾“å…¥æœ€å100ä¸ªå­—ç¬¦: ...{input_decoded[-100:]}")
            print()

        print(f"  å¼€å§‹ç”Ÿæˆ (max_new_tokens={max_new_tokens})...")
        sys.stdout.flush()

        # ç”Ÿæˆ
        with torch.no_grad():
            output_ids = iterative_generate(
                model=model,
                input_ids=input_ids,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,
                add_eos_length=add_eos_length,
                refine_iter=refine_iter,
                max_new_tokens=max_new_tokens,
                max_length=8192,
                verbose_trace=False,
            )

        print(f"  âœ… ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {output_ids.shape[1]}")
        sys.stdout.flush()

        # è§£ç 
        for j in range(len(batch_prompt_messages)):
            # åªè§£ç ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰ promptï¼‰
            response = tokenizer.decode(
                output_ids[j, input_ids[j].size(0):],
                skip_special_tokens=True
            )
            responses.append(response)
            print(f"  æ ·æœ¬ {i+j}: ç”Ÿæˆäº† {len(response)} ä¸ªå­—ç¬¦")

            # ğŸ” DEBUG: æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å®Œæ•´è¾“å‡ºè§£ç 
            if i == 0 and j == 0:
                print(f"\n  ğŸ” DEBUG - æ ·æœ¬ {i+j} çš„å®Œæ•´è¾“å‡º token è§£ç :")
                print(f"  " + "="*76)
                full_output = tokenizer.decode(output_ids[j], skip_special_tokens=False)
                print(f"  {full_output[:500]}...")
                print(f"  " + "="*76)
                print(f"\n  ğŸ” DEBUG - ä»…ç”Ÿæˆéƒ¨åˆ† (ä¸å«è¾“å…¥):")
                print(f"  " + "="*76)
                print(f"  {response[:500]}...")
                print(f"  " + "="*76)
                print()

            sys.stdout.flush()

    return responses


def save_results(results, output_file):
    """
    ä¿å­˜ç»“æœåˆ° JSONL æ–‡ä»¶

    Args:
        results: list of dict
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file}")

    with open(output_file, 'w', encoding='utf-8') as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"   âœ… å·²ä¿å­˜ {len(results)} æ¡ç»“æœ")


def compute_metrics(results):
    """
    è®¡ç®—ç®€å•çš„è¯„ä¼°æŒ‡æ ‡

    Args:
        results: list of dict with keys: prompt, target, prediction

    Returns:
        dict: è¯„ä¼°æŒ‡æ ‡
    """
    print("\n" + "="*80)
    print("ğŸ“Š è¯„ä¼°æŒ‡æ ‡")
    print("="*80)

    total = len(results)

    # å®Œå…¨åŒ¹é…ç‡ï¼ˆä¸¥æ ¼ï¼‰
    exact_match = sum(1 for r in results if r['prediction'].strip() == r['target'].strip())
    exact_match_rate = exact_match / total * 100

    # åŒ…å«åŒ¹é…ç‡ï¼ˆå®½æ¾ï¼‰
    contain_match = sum(1 for r in results if r['target'].strip() in r['prediction'])
    contain_match_rate = contain_match / total * 100

    # å¹³å‡é•¿åº¦
    avg_target_len = sum(len(r['target']) for r in results) / total
    avg_pred_len = sum(len(r['prediction']) for r in results) / total

    metrics = {
        "total_samples": total,
        "exact_match": exact_match,
        "exact_match_rate": f"{exact_match_rate:.2f}%",
        "contain_match": contain_match,
        "contain_match_rate": f"{contain_match_rate:.2f}%",
        "avg_target_length": f"{avg_target_len:.1f}",
        "avg_prediction_length": f"{avg_pred_len:.1f}",
    }

    for key, value in metrics.items():
        print(f"  {key}: {value}")

    print("="*80)

    return metrics


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡æ¨ç†è„šæœ¬")

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_path", type=str, required=True,
                       help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")

    # æ•°æ®å‚æ•°
    parser.add_argument("--data_file", type=str, required=True,
                       help="æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆparquet æ ¼å¼ï¼‰")
    parser.add_argument("--prompt_key", type=str, default="prompt",
                       help="prompt åˆ—åï¼ˆé»˜è®¤: promptï¼‰")
    parser.add_argument("--target_key", type=str, default="target",
                       help="target åˆ—åï¼ˆé»˜è®¤: targetï¼‰")
    parser.add_argument("--num_samples", type=int, default=None,
                       help="é‡‡æ ·æ•°é‡ï¼ˆé»˜è®¤: å…¨éƒ¨ï¼‰")

    # æ¨ç†å‚æ•°
    parser.add_argument("--batch_size", type=int, default=4,
                       help="æ‰¹å¤§å°ï¼ˆé»˜è®¤: 4ï¼‰")
    parser.add_argument("--add_eos_length", type=int, default=127,
                       help="æ¯å—æ·»åŠ çš„ EOS æ•°é‡ï¼ˆé»˜è®¤: 127ï¼‰")
    parser.add_argument("--refine_iter", type=int, default=2,
                       help="refine è½®æ•°ï¼ˆé»˜è®¤: 2ï¼‰")
    parser.add_argument("--max_new_tokens", type=int, default=1024,
                       help="æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤: 1024ï¼‰")

    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_file", type=str,
                       default="results/batch_inference.jsonl",
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: results/batch_inference.jsonlï¼‰")
    parser.add_argument("--save_metrics", action="store_true",
                       help="æ˜¯å¦ä¿å­˜è¯„ä¼°æŒ‡æ ‡åˆ°å•ç‹¬çš„æ–‡ä»¶")

    args = parser.parse_args()

    from datetime import datetime
    start_time = datetime.now()

    print("="*80)
    print("ğŸš€ æ‰¹é‡æ¨ç†è„šæœ¬")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ•°æ®æ–‡ä»¶: {args.data_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output_file}")
    print(f"æ‰¹å¤§å°: {args.batch_size}")
    print(f"æ¨ç†å‚æ•°: add_eos_length={args.add_eos_length}, refine_iter={args.refine_iter}, max_new_tokens={args.max_new_tokens}")
    print("="*80)
    print()

    # 1. åŠ è½½æ•°æ®
    data = load_data(
        args.data_file,
        args.prompt_key,
        args.target_key,
        args.num_samples
    )
    print()

    # 2. åŠ è½½æ¨¡å‹
    print("ğŸ”§ åŠ è½½æ¨¡å‹...")
    import os

    # æ£€æŸ¥GPUæ•°é‡
    num_gpus = torch.cuda.device_count()
    print(f"   å¯ç”¨GPUæ•°é‡: {num_gpus}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   ä¸»è®¾å¤‡: {device}")
    print(f"   æ¨¡å‹è·¯å¾„: {args.model_path}")

    if not os.path.exists(args.model_path):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
    print(f"   âœ… æ¨¡å‹è·¯å¾„ç¡®è®¤å­˜åœ¨")

    print(f"   æ­£åœ¨åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
    print(f"   âœ… Tokenizer åŠ è½½å®Œæˆ")

    print(f"   æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡...")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map="auto" if num_gpus > 1 else None,  # ä½¿ç”¨ device_map="auto" å¯ç”¨å¤šGPU
    )

    if num_gpus <= 1:
        model = model.to(device)

    model.eval()

    print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")

    if num_gpus > 1:
        print(f"   ğŸ’¡ ä½¿ç”¨å¤šGPUåŠ è½½ (device_map='auto')ï¼Œæ¨¡å‹å·²è‡ªåŠ¨åˆ†å¸ƒåˆ° {num_gpus} å¼ å¡")
        # æ‰“å°æ¯å¼ å¡çš„æ˜¾å­˜ä½¿ç”¨
        for i in range(num_gpus):
            allocated = torch.cuda.memory_allocated(i) / 1e9
            print(f"      GPU {i}: {allocated:.2f} GB")
    print()

    # 3. æ‰¹é‡æ¨ç†
    print("ğŸ”® å¼€å§‹æ¨ç†...")
    prompt_messages_list = [item["prompt_messages"] for item in data]
    predictions = batch_inference(
        model=model,
        tokenizer=tokenizer,
        prompt_messages_list=prompt_messages_list,
        batch_size=args.batch_size,
        add_eos_length=args.add_eos_length,
        refine_iter=args.refine_iter,
        max_new_tokens=args.max_new_tokens,
    )
    print()

    # 4. æ•´åˆç»“æœ
    results = []
    for i, item in enumerate(data):
        # å°† prompt_messages è½¬æˆå¯è¯»çš„æ–‡æœ¬ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        prompt_text = '\n'.join([
            f"{msg['role']}: {msg['content']}"
            for msg in item["prompt_messages"]
        ])

        results.append({
            "index": i,
            "prompt": prompt_text,
            "target": item["target_content"],
            "prediction": predictions[i],
        })

    # 5. ä¿å­˜ç»“æœ
    save_results(results, args.output_file)
    print()

    # 6. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    metrics = compute_metrics(results)

    if args.save_metrics:
        metrics_file = args.output_file.replace(".jsonl", "_metrics.json")
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}")

    # 7. æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹
    print("\n" + "="*80)
    print("ğŸ“‹ ç¤ºä¾‹ç»“æœï¼ˆå‰3æ¡ï¼‰")
    print("="*80)
    for i, result in enumerate(results[:3]):
        print(f"\nã€æ ·æœ¬ {i+1}ã€‘")
        print(f"Prompt: {result['prompt'][:100]}...")
        print(f"Target: {result['target'][:100]}...")
        print(f"Prediction: {result['prediction'][:100]}...")
        print("-"*80)

    end_time = datetime.now()
    duration = end_time - start_time

    print("\n" + "="*80)
    print("âœ… æ‰¹é‡æ¨ç†å®Œæˆï¼")
    print("="*80)
    print(f"ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ€»è€—æ—¶: {duration}")
    print(f"ç»“æœæ–‡ä»¶: {args.output_file}")
    print("="*80)


if __name__ == "__main__":
    main()
