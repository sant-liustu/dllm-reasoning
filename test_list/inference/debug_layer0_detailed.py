#!/usr/bin/env python3
"""
è¯¦ç»†æ‹†è§£Layer 0çš„è®¡ç®—è¿‡ç¨‹ï¼Œæ‰¾å‡º[P][Mâ‚]å’Œ[P][Mâ‚][Râ‚]å·®å¼‚çš„æ ¹æº

é€æ­¥æ£€æŸ¥ï¼š
1. Embedding
2. Position embeddings (RoPE)
3. Attention (Q, K, V, attention weights, attention output)
4. MLP
5. LayerNorm
"""

import os
os.environ["TORCH_COMPILE_DISABLE"] = "1"

import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from dllm_reasoning.trainer.interleaved_sft_dataset import InterleavedSFTDataset


def detailed_layer0_analysis(model, input_ids, position_ids, block_info, prompt_len, device):
    """
    è¯¦ç»†åˆ†æLayer 0çš„æ¯ä¸€æ­¥è®¡ç®—

    Returns:
        å„ä¸ªä¸­é—´æ­¥éª¤çš„Mâ‚ä½ç½®çš„tensor
    """
    m1_pos = prompt_len
    results = {}

    # ========== 1. Embedding ==========
    print("\n" + "="*80)
    print("1ï¸âƒ£  Embeddingå±‚")
    print("="*80)

    inputs_embeds = model.model.embed_tokens(input_ids)  # [seq_len, hidden_size]
    m1_embedding = inputs_embeds[m1_pos]
    results['embedding'] = m1_embedding

    print(f"Input IDs shape: {input_ids.shape}")
    print(f"Embeddings shape: {inputs_embeds.shape}")
    print(f"Mâ‚ embedding (å‰5ç»´): {m1_embedding[:5]}")
    print(f"Mâ‚ embedding norm: {torch.norm(m1_embedding).item():.6f}")

    # ========== 2. Position IDs ==========
    print("\n" + "="*80)
    print("2ï¸âƒ£  Position IDs")
    print("="*80)

    m1_position_id = position_ids[m1_pos]
    results['position_id'] = m1_position_id

    print(f"Position IDs: {position_ids.tolist()}")
    print(f"Mâ‚ position ID: {m1_position_id.item()}")

    # ========== 3. Layer 0 è¯¦ç»†åˆ†æ ==========
    print("\n" + "="*80)
    print("3ï¸âƒ£  Layer 0 è¯¦ç»†åˆ†æ")
    print("="*80)

    layer0 = model.model.layers[0]

    # 3.1 Input LayerNorm
    print("\n--- 3.1 Input LayerNorm ---")
    normed_inputs = layer0.input_layernorm(inputs_embeds.unsqueeze(0))[0]  # [seq_len, hidden_size]
    m1_normed = normed_inputs[m1_pos]
    results['layer0_input_normed'] = m1_normed

    print(f"Normed inputs shape: {normed_inputs.shape}")
    print(f"Mâ‚ normed (å‰5ç»´): {m1_normed[:5]}")
    print(f"Mâ‚ normed norm: {torch.norm(m1_normed).item():.6f}")

    # 3.2 Attention - Q, K, V projections
    print("\n--- 3.2 Attention Q/K/V projections ---")
    attn = layer0.self_attn

    # Get attention dimensions
    head_dim = attn.head_dim
    num_heads = attn.num_attention_heads
    num_kv_heads = attn.num_key_value_heads

    # Project to Q, K, V
    query_states = attn.q_proj(normed_inputs).view(-1, num_heads, head_dim).transpose(0, 1)  # [num_heads, seq_len, head_dim]
    key_states = attn.k_proj(normed_inputs).view(-1, num_kv_heads, head_dim).transpose(0, 1)
    value_states = attn.v_proj(normed_inputs).view(-1, num_kv_heads, head_dim).transpose(0, 1)

    m1_q = query_states[:, m1_pos, :]  # [num_heads, head_dim]
    m1_k = key_states[:, m1_pos, :]    # [num_kv_heads, head_dim]
    m1_v = value_states[:, m1_pos, :]

    results['layer0_q'] = m1_q
    results['layer0_k'] = m1_k
    results['layer0_v'] = m1_v

    print(f"Q shape: {query_states.shape}")
    print(f"K shape: {key_states.shape}")
    print(f"V shape: {value_states.shape}")
    print(f"Mâ‚ Q[0] (å‰5ç»´): {m1_q[0, :5]}")
    print(f"Mâ‚ K[0] (å‰5ç»´): {m1_k[0, :5]}")
    print(f"Mâ‚ V[0] (å‰5ç»´): {m1_v[0, :5]}")

    # 3.3 Forward through layer 0 to get output
    print("\n--- 3.3 Layer 0 Full Forward ---")

    # Run through the full layer
    with torch.no_grad():
        layer_output = layer0(
            inputs_embeds.unsqueeze(0),
            position_ids=position_ids.unsqueeze(0)
        )[0][0]  # [seq_len, hidden_size]

    m1_layer_output = layer_output[m1_pos]
    results['layer0_output'] = m1_layer_output

    print(f"Layer 0 output shape: {layer_output.shape}")
    print(f"Mâ‚ layer 0 output (å‰5ç»´): {m1_layer_output[:5]}")
    print(f"Mâ‚ layer 0 output norm: {torch.norm(m1_layer_output).item():.6f}")

    return results


def compare_two_configs(model, tokenizer, sample, device):
    """
    å¯¹æ¯”[P][Mâ‚]å’Œ[P][Mâ‚][Râ‚]ä¸¤ç§é…ç½®çš„è¯¦ç»†è®¡ç®—è¿‡ç¨‹
    """
    print("="*80)
    print("è¯¦ç»†å¯¹æ¯”ï¼š[P][Mâ‚] vs [P][Mâ‚][Râ‚]")
    print("="*80)

    prompt_len = sample['prompt_len']
    input_ids_full = sample['input_ids'].to(device)
    position_ids_full = sample['position_ids'].to(device)

    # é…ç½®1: [P][Mâ‚]
    print("\n" + "#"*80)
    print("é…ç½®1: [P][Mâ‚]")
    print("#"*80)

    truncate_pos_1 = prompt_len + 3
    input_ids_1 = input_ids_full[:truncate_pos_1]
    position_ids_1 = position_ids_full[:truncate_pos_1]
    block_info_1 = [('mask', 3)]

    print(f"\nåºåˆ—é•¿åº¦: {input_ids_1.size(0)}")
    print(f"Block info: {block_info_1}")
    print(f"Input IDs: {input_ids_1.tolist()}")

    results_1 = detailed_layer0_analysis(
        model, input_ids_1, position_ids_1, block_info_1, prompt_len, device
    )

    # é…ç½®2: [P][Mâ‚][Râ‚]
    print("\n" + "#"*80)
    print("é…ç½®2: [P][Mâ‚][Râ‚]")
    print("#"*80)

    truncate_pos_2 = prompt_len + 7
    input_ids_2 = input_ids_full[:truncate_pos_2]
    position_ids_2 = position_ids_full[:truncate_pos_2]
    block_info_2 = [('mask', 3), ('real', 4)]

    print(f"\nåºåˆ—é•¿åº¦: {input_ids_2.size(0)}")
    print(f"Block info: {block_info_2}")
    print(f"Input IDs: {input_ids_2.tolist()}")

    results_2 = detailed_layer0_analysis(
        model, input_ids_2, position_ids_2, block_info_2, prompt_len, device
    )

    # ========== å¯¹æ¯”åˆ†æ ==========
    print("\n" + "="*80)
    print("ğŸ“Š å¯¹æ¯”åˆ†æ")
    print("="*80)

    comparisons = [
        ('embedding', 'Embedding'),
        ('rope_cos', 'RoPE cos'),
        ('rope_sin', 'RoPE sin'),
        ('layer0_input_normed', 'Input LayerNorm'),
        ('layer0_q', 'Query (before RoPE)'),
        ('layer0_k', 'Key (before RoPE)'),
        ('layer0_v', 'Value'),
        ('layer0_q_rope', 'Query (after RoPE)'),
        ('layer0_k_rope', 'Key (after RoPE)'),
    ]

    print(f"\n{'æ­¥éª¤':^30} | {'L2è·ç¦»':^15} | {'ä½™å¼¦ç›¸ä¼¼åº¦':^15} | {'æ˜¯å¦ç›¸åŒ':^10}")
    print("-"*80)

    for key, name in comparisons:
        if key not in results_1 or key not in results_2:
            continue

        t1 = results_1[key]
        t2 = results_2[key]

        # Flatten if multi-dimensional
        if t1.dim() > 1:
            t1 = t1.flatten()
            t2 = t2.flatten()

        l2_dist = torch.norm(t1 - t2).item()
        cos_sim = torch.nn.functional.cosine_similarity(
            t1.unsqueeze(0), t2.unsqueeze(0)
        ).item()

        is_same = "âœ…" if l2_dist < 1e-5 else "âŒ"

        print(f"{name:^30} | {l2_dist:>12.6f}  | {cos_sim:>12.6f}  | {is_same:^10}")

    # ========== å…³é”®å‘ç° ==========
    print("\n" + "="*80)
    print("ğŸ” å…³é”®å‘ç°")
    print("="*80)

    # æ£€æŸ¥embeddingæ˜¯å¦ç›¸åŒ
    emb_diff = torch.norm(results_1['embedding'] - results_2['embedding']).item()
    if emb_diff < 1e-5:
        print("âœ… Embeddingå±‚ï¼šMâ‚çš„embeddingå®Œå…¨ç›¸åŒ")
    else:
        print(f"âŒ Embeddingå±‚ï¼šMâ‚çš„embeddingä¸åŒï¼å·®å¼‚: {emb_diff:.6f}")
        print("   è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºMâ‚çš„input_idåº”è¯¥ç›¸åŒ")
        return

    # æ£€æŸ¥position embeddings
    cos_diff = torch.norm(results_1['rope_cos'] - results_2['rope_cos']).item()
    sin_diff = torch.norm(results_1['rope_sin'] - results_2['rope_sin']).item()

    if cos_diff < 1e-5 and sin_diff < 1e-5:
        print("âœ… RoPEï¼šMâ‚çš„position embeddingså®Œå…¨ç›¸åŒ")
    else:
        print(f"âŒ RoPEï¼šMâ‚çš„position embeddingsä¸åŒï¼")
        print(f"   coså·®å¼‚: {cos_diff:.6f}")
        print(f"   sinå·®å¼‚: {sin_diff:.6f}")
        print(f"   position_idé…ç½®1: {position_ids_1[prompt_len].item()}")
        print(f"   position_idé…ç½®2: {position_ids_2[prompt_len].item()}")
        print("\n   âš ï¸ è¿™è¯´æ˜position_idsä¸åŒå¯¼è‡´äº†å·®å¼‚ï¼")
        print("   è™½ç„¶Mâ‚çœ‹ä¸åˆ°Râ‚ï¼Œä½†position_idæœ¬èº«çš„å·®å¼‚å½±å“äº†RoPE encoding")


def main():
    MODEL_PATH = "dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
    DATA_PATH = "data/openr1.parquet"

    print("åŠ è½½æ¨¡å‹...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    ).to(device)

    model.train()

    print("åŠ è½½æ•°æ®é›†...")
    dataset = InterleavedSFTDataset(
        parquet_files=DATA_PATH,
        tokenizer=tokenizer,
        prompt_key="prompt",
        response_key="target",
        block_size=4,
        max_length=6000,
        truncation="right",
    )

    sample = dataset[0]

    print(f"\næ ·æœ¬ä¿¡æ¯:")
    print(f"  Prompté•¿åº¦: {sample['prompt_len']}")
    print(f"  æ€»åºåˆ—é•¿åº¦: {sample['input_ids'].shape[0]}")

    # è¿è¡Œè¯¦ç»†å¯¹æ¯”
    compare_two_configs(model, tokenizer, sample, device)


if __name__ == "__main__":
    main()
