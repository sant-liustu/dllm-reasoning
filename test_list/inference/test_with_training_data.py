#!/usr/bin/env python3
"""
ä½¿ç”¨è®­ç»ƒæ•°æ®éªŒè¯ Interleaved æ¨ç†çš„æ­£ç¡®æ€§

ç›®æ ‡:
1. è¯»å–ä¸€æ¡è®­ç»ƒæ•°æ® (prompt + target)
2. ä½¿ç”¨ prompt + mask é¢„æµ‹ç¬¬ä¸€ä¸ª block
3. å¯¹æ¯”é¢„æµ‹ç»“æœå’ŒçœŸå® target
4. é€ä¸ª block éªŒè¯ï¼Œçœ‹æ¨¡å‹æ˜¯å¦çœŸçš„å­¦ä¼šäº† Next Block Prediction
"""

import sys
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import pandas as pd

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# é…ç½®
MODEL_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
DATA_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/data/openr1.parquet"
BLOCK_SIZE = 4  # è®­ç»ƒæ—¶çš„ block_size
NUM_MASKS = 3   # block_size - 1

print("="*100)
print("ğŸ”¬ ä½¿ç”¨è®­ç»ƒæ•°æ®éªŒè¯ Interleaved æ¨ç†")
print("="*100)

# ========== åŠ è½½æ¨¡å‹ ==========
print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    local_files_only=True,
).to(device).train()  # ä½¿ç”¨ train() æ¨¡å¼ä»¥å¯ç”¨ FlexAttention

print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ (training mode for FlexAttention)")

# è·å–ç‰¹æ®Š token
eos_token_id = tokenizer.eos_token_id
mask_token_id = eos_token_id  # è®­ç»ƒæ—¶ä½¿ç”¨ eos ä½œä¸º mask

print(f"   Mask token ID: {mask_token_id}")
print(f"   EOS token ID: {eos_token_id}")

# ========== åŠ è½½è®­ç»ƒæ•°æ® ==========
print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {DATA_PATH}")
df = pd.read_parquet(DATA_PATH)
print(f"   æ€»æ ·æœ¬æ•°: {len(df)}")

# å–ç¬¬ä¸€æ¡æ•°æ®
sample = df.iloc[0]
prompt = sample['prompt']
target = sample['target']

print(f"\nğŸ“ æµ‹è¯•æ ·æœ¬ 0:")
print(f"   Prompt: {prompt}")
print(f"   Target: {target}")

# ========== Tokenize ==========
print(f"\nğŸ”¤ Tokenization...")

# Tokenize prompt (ä½¿ç”¨ apply_chat_template)
prompt_messages = list(prompt) if isinstance(prompt, (list, tuple)) else prompt
if hasattr(prompt, 'tolist'):
    prompt_messages = prompt.tolist()

input_ids = tokenizer.apply_chat_template(
    prompt_messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt"
).to(device)

prompt_len = input_ids.size(1)
print(f"   Prompt é•¿åº¦: {prompt_len} tokens")
print(f"   Prompt tokens (å‰20ä¸ª): {input_ids[0][:20].tolist()}")

# Tokenize target (å®Œæ•´çš„ response)
# æå– assistant çš„å›å¤å†…å®¹
target_list = target.tolist() if hasattr(target, 'tolist') else target
if isinstance(target_list, list) and len(target_list) > 0:
    target_content = target_list[0].get('content', '') if isinstance(target_list[0], dict) else str(target_list[0])
else:
    target_content = str(target_list)

print(f"   Target content (å‰100å­—ç¬¦): {target_content[:100]}...")

# ========== é‡è¦: å’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼Œå»æ‰ <think> æ ‡ç­¾ ==========
# è®­ç»ƒä»£ç : interleaved_sft_dataset.py line 507-509
if target_content.strip().startswith('<think>'):
    think_start = target_content.find('<think>')
    target_content = target_content[think_start + 7:].lstrip()  # å»æ‰ '<think>' è¿™7ä¸ªå­—ç¬¦
    print(f"   âš ï¸  æ£€æµ‹åˆ° <think> æ ‡ç­¾ï¼Œå·²å»é™¤")
    print(f"   å¤„ç†å target (å‰100å­—ç¬¦): {target_content[:100]}...")

# Tokenize target content (ä¸ä½¿ç”¨ chat templateï¼Œç›´æ¥ tokenize)
target_tokens = tokenizer(target_content, add_special_tokens=False, return_tensors="pt")["input_ids"][0]
target_len = target_tokens.size(0)

print(f"   Target é•¿åº¦: {target_len} tokens")
print(f"   Target tokens (å‰20ä¸ª): {target_tokens[:20].tolist()}")

# ========== é€ Block éªŒè¯ ==========
print(f"\n{'='*100}")
print(f"ğŸ§ª å¼€å§‹é€ Block éªŒè¯ (block_size={BLOCK_SIZE}, num_masks={NUM_MASKS})")
print(f"{'='*100}")

num_blocks = min((target_len + BLOCK_SIZE - 1) // BLOCK_SIZE, 10)  # æœ€å¤šæµ‹è¯•10ä¸ªblock
print(f"   å°†æµ‹è¯•å‰ {num_blocks} ä¸ª blocks")

generated_ids = input_ids.clone()  # ä» prompt å¼€å§‹
all_correct = True

for block_idx in range(num_blocks):
    print(f"\n{'-'*100}")
    print(f"ğŸ“¦ Block {block_idx + 1}/{num_blocks}")
    print(f"{'-'*100}")

    # å½“å‰å·²ç”Ÿæˆé•¿åº¦
    pre_length = generated_ids.size(1)
    print(f"   å½“å‰åºåˆ—é•¿åº¦: {pre_length} tokens")

    # çœŸå®çš„è¿™ä¸ª block çš„ token
    block_start = block_idx * BLOCK_SIZE
    block_end = min((block_idx + 1) * BLOCK_SIZE, target_len)
    true_block = target_tokens[block_start:block_end]
    actual_block_size = true_block.size(0)

    print(f"   çœŸå® block èŒƒå›´: [{block_start}:{block_end}]")
    print(f"   çœŸå® block å¤§å°: {actual_block_size} tokens")
    print(f"   çœŸå® block tokens: {true_block.tolist()}")
    print(f"   çœŸå® block æ–‡æœ¬: {repr(tokenizer.decode(true_block))}")

    # æ·»åŠ  mask tokens
    mask_block = torch.full(
        (1, NUM_MASKS),
        mask_token_id,
        device=device,
        dtype=torch.long,
    )
    current_ids = torch.cat([generated_ids, mask_block], dim=1)

    # æ„é€  position_ids (è¿ç»­çš„)
    position_ids = torch.arange(
        0, pre_length + NUM_MASKS,
        dtype=torch.long,
        device=device
    ).unsqueeze(0)

    print(f"\n   æ·»åŠ  {NUM_MASKS} ä¸ª mask å:")
    print(f"   - åºåˆ—é•¿åº¦: {current_ids.size(1)}")
    print(f"   - Position IDs (æœ€å10ä¸ª): {position_ids[0, -10:].tolist()}")

    # ========== åˆ›å»º FlexAttention BlockMask (å’Œè®­ç»ƒæ—¶ä¸€è‡´) ==========
    # è®­ç»ƒæ—¶ä½¿ç”¨ FlexAttentionï¼Œæ¨ç†æ—¶ä¹Ÿå¿…é¡»ä½¿ç”¨ï¼Œå¦åˆ™ attention pattern ä¸ä¸€è‡´
    from torch.nn.attention.flex_attention import create_block_mask

    # æ„é€  mask (å’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´):
    # - Prompt tokens: çœ‹ä¹‹å‰çš„æ‰€æœ‰ Prompt (causal)
    # - Mask tokens: çœ‹æ‰€æœ‰ Prompt + åŒä¸€ group å†…ä¹‹å‰çš„ Mask (causal)
    # å¿…é¡»ä½¿ç”¨çº¯ Tensor æ“ä½œï¼Œä¸èƒ½ç”¨ Python if (vmap é™åˆ¶)
    def mask_fn(b, h, q_idx, kv_idx):
        # q_idx, kv_idx æ˜¯ä½ç½®ç´¢å¼•
        # Prompt éƒ¨åˆ†: [0, pre_length)
        # Mask éƒ¨åˆ†: [pre_length, pre_length + NUM_MASKS)

        q_is_prompt = q_idx < pre_length
        kv_is_prompt = kv_idx < pre_length

        # 1. Prompt çœ‹ Prompt (causal)
        prompt_see_prompt = q_is_prompt & kv_is_prompt & (kv_idx <= q_idx)

        # 2. Mask çœ‹ Prompt (å…¨è¿æ¥)
        mask_see_prompt = (~q_is_prompt) & kv_is_prompt

        # 3. Mask çœ‹ Mask (causal within same group)
        # åŒä¸€ä¸ª mask group å†…ï¼Œcausal attention
        q_is_mask = ~q_is_prompt
        kv_is_mask = ~kv_is_prompt
        mask_see_mask = q_is_mask & kv_is_mask & (kv_idx <= q_idx)

        # 4. Prompt ä¸èƒ½çœ‹ Mask: q_is_prompt & (~kv_is_prompt) â†’ False (ä¸åŒ…å«)

        return prompt_see_prompt | mask_see_prompt | mask_see_mask

    block_mask = create_block_mask(
        mask_fn,
        B=1,
        H=None,
        Q_LEN=current_ids.size(1),
        KV_LEN=current_ids.size(1),
        device=device
    )

    # å‰å‘ä¼ æ’­ (ä½¿ç”¨ FlexAttention)
    with torch.no_grad():
        outputs = model(
            current_ids,
            position_ids=position_ids,
            attention_mask=block_mask,  # ä½¿ç”¨ FlexAttention BlockMask
            use_cache=False
        )
        logits = outputs.logits

    # æå–é¢„æµ‹ä½ç½®çš„ logits
    # æˆ‘ä»¬è¦é¢„æµ‹ä½ç½® [pre_length, pre_length+1, ..., pre_length+NUM_MASKS]
    # éœ€è¦ logits[pre_length-1 : pre_length+NUM_MASKS]
    pred_logits = logits[0, pre_length-1 : pre_length+NUM_MASKS, :]

    print(f"\n   æå– logits:")
    print(f"   - Logits ç´¢å¼•èŒƒå›´: [{pre_length-1}:{pre_length+NUM_MASKS}]")
    print(f"   - é¢„æµ‹ä½ç½®èŒƒå›´: [{pre_length}:{pre_length+NUM_MASKS+1}]")
    print(f"   - é¢„æµ‹ token æ•°é‡: {pred_logits.size(0)}")

    # === DEBUG: è¯¦ç»†åˆ†ælogits ===
    print(f"\n   === DEBUG: Logitsè¯¦ç»†åˆ†æ ===")
    for i in range(min(pred_logits.size(0), 4)):  # åªçœ‹å‰4ä¸ª
        pos = pre_length - 1 + i
        print(f"\n   Position {pos} é¢„æµ‹ position {pos+1}:")

        # Top 5é¢„æµ‹
        top5_logits, top5_indices = pred_logits[i].topk(5)
        print(f"     Top 5 predictions:")
        for j, (logit_val, token_id) in enumerate(zip(top5_logits, top5_indices)):
            token_text = tokenizer.decode([token_id.item()])
            print(f"       #{j+1}: ID={token_id.item():6d}, logit={logit_val.item():8.4f}, text={repr(token_text)}")

        # çœŸå®tokençš„logit
        if i < actual_block_size:
            true_token_id = true_block[i].item()
            true_logit = pred_logits[i, true_token_id].item()
            true_token_text = tokenizer.decode([true_token_id])
            print(f"     çœŸå® token: ID={true_token_id:6d}, logit={true_logit:8.4f}, text={repr(true_token_text)}")

            # è®¡ç®—çœŸå®tokenåœ¨æ‰€æœ‰tokenä¸­çš„æ’å
            all_logits_sorted = pred_logits[i].sort(descending=True)
            true_rank = (all_logits_sorted.values > true_logit).sum().item() + 1
            print(f"     çœŸå® token æ’å: {true_rank}/{pred_logits.size(1)}")

    print(f"   === END DEBUG ===\n")

    # è§£ç  (è´ªå©ª)
    predicted_tokens = pred_logits.argmax(dim=-1)

    # åªå–å‰ actual_block_size ä¸ª (å› ä¸ºæœ€åä¸€ä¸ª block å¯èƒ½ä¸æ»¡)
    predicted_block = predicted_tokens[:actual_block_size]

    print(f"\n   é¢„æµ‹ç»“æœ:")
    print(f"   - é¢„æµ‹ block tokens: {predicted_block.tolist()}")
    print(f"   - é¢„æµ‹ block æ–‡æœ¬: {repr(tokenizer.decode(predicted_block))}")

    # å¯¹æ¯”
    is_correct = torch.equal(predicted_block.cpu(), true_block.cpu())

    if is_correct:
        print(f"\n   âœ… Block {block_idx + 1} é¢„æµ‹å®Œå…¨æ­£ç¡®ï¼")
    else:
        print(f"\n   âŒ Block {block_idx + 1} é¢„æµ‹é”™è¯¯")

        # é€ä¸ª token å¯¹æ¯”
        print(f"\n   é€ token å¯¹æ¯”:")
        for i in range(actual_block_size):
            pred_tok = predicted_block[i].item()
            true_tok = true_block[i].item()
            pred_text = tokenizer.decode([pred_tok])
            true_text = tokenizer.decode([true_tok])

            if pred_tok == true_tok:
                print(f"      ä½ç½® {block_start + i}: âœ… {repr(pred_text)} (ID: {pred_tok})")
            else:
                print(f"      ä½ç½® {block_start + i}: âŒ é¢„æµ‹ {repr(pred_text)} (ID: {pred_tok}), çœŸå® {repr(true_text)} (ID: {true_tok})")

        all_correct = False
        print(f"\n   âš ï¸  åœæ­¢éªŒè¯ï¼Œå› ä¸ºç¬¬ {block_idx + 1} ä¸ª block é¢„æµ‹é”™è¯¯")
        break

    # æ›´æ–°åºåˆ— (ä½¿ç”¨çœŸå®çš„ tokenï¼Œç»§ç»­ä¸‹ä¸€ä¸ª block)
    generated_ids = torch.cat([generated_ids, true_block.unsqueeze(0).to(device)], dim=1)

print(f"\n{'='*100}")
if all_correct:
    print(f"ğŸ‰ æ‰€æœ‰ {num_blocks} ä¸ª blocks éƒ½é¢„æµ‹æ­£ç¡®ï¼")
    print(f"âœ… æ¨¡å‹ç¡®å®å­¦ä¼šäº† Next Block Predictionï¼")
else:
    print(f"âš ï¸  éªŒè¯å¤±è´¥ï¼Œæ¨¡å‹åœ¨æŸäº› block ä¸Šé¢„æµ‹é”™è¯¯")
    print(f"ğŸ’¡ å¯èƒ½çš„åŸå› :")
    print(f"   1. Position IDs è®¾ç½®ä¸å¯¹")
    print(f"   2. Logits æå–ä½ç½®ä¸å¯¹")
    print(f"   3. è®­ç»ƒæ•°æ®å¤„ç†æ–¹å¼å’Œæ¨ç†ä¸ä¸€è‡´")
    print(f"   4. å…¶ä»–æœªçŸ¥é—®é¢˜")
print(f"{'='*100}")
