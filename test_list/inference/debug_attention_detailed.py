#!/usr/bin/env python3
"""
æ·±å…¥debug attentionè®¡ç®—çš„æ¯ä¸€æ­¥

Hookæ•è·ï¼š
1. Q/K/V projectionä¹‹åçš„å€¼
2. RoPEåº”ç”¨ä¹‹åçš„Q/K
3. Attention scores (Q @ K^T)
4. Attention weights (after softmax)
5. Attention output (weights @ V)
"""

import os
os.environ["TORCH_COMPILE_DISABLE"] = "1"

import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from dllm_reasoning.trainer.interleaved_sft_dataset import InterleavedSFTDataset


def patch_attention_forward(model, prompt_len, captured_dict):
    """
    Patch Layer 0çš„attention forwardå‡½æ•°æ¥æ•è·ä¸­é—´è®¡ç®—ç»“æœ
    """
    layer0_attn = model.model.layers[0].self_attn
    original_forward = layer0_attn.forward
    m1_pos = prompt_len

    def patched_forward(
        hidden_states,
        position_embeddings,
        attention_mask,
        past_key_value=None,
        cache_position=None,
        **kwargs
    ):
        """Patched forwardå‡½æ•°"""
        # è·å–åŸºæœ¬ä¿¡æ¯
        bsz, q_len = hidden_states.shape[:-1]
        hidden_shape = (bsz, q_len, -1, layer0_attn.head_dim)

        # ===== Step 1: Q/K/V Projection =====
        query_states = layer0_attn.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states = layer0_attn.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = layer0_attn.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        captured_dict['q_before_rope'] = query_states[0, :, m1_pos, :].detach().clone()
        captured_dict['k_before_rope'] = key_states[0, :, m1_pos, :].detach().clone()
        captured_dict['v'] = value_states[0, :, m1_pos, :].detach().clone()

        # ===== Step 2: Apply RoPE =====
        # Import from the cached transformers module
        import importlib
        modeling_dllm = importlib.import_module('transformers_modules.huggingface.modeling_dllm')
        apply_rotary_pos_emb = modeling_dllm.apply_rotary_pos_emb

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, cos, sin
        )

        captured_dict['q_after_rope'] = query_states[0, :, m1_pos, :].detach().clone()
        captured_dict['k_after_rope'] = key_states[0, :, m1_pos, :].detach().clone()

        # ===== Step 3: Attention Computation =====
        if layer0_attn.training:
            # Use FlexAttention
            from dllm_reasoning.checkpoints.interleaved_sft.global_step_17172.huggingface.modeling_dllm import fused_flex_attention

            # ä¿å­˜K/Vçš„å®Œæ•´çŸ©é˜µï¼ˆç”¨äºåˆ†æï¼‰
            captured_dict['k_full_shape'] = key_states.shape
            captured_dict['v_full_shape'] = value_states.shape

            attn_output, attn_weights = fused_flex_attention(
                query=query_states,
                key=key_states,
                value=value_states,
                attention_mask=attention_mask,
                enable_gqa=True,
                scale=layer0_attn.scaling,
                return_lse=True
            )

            from einops import rearrange
            attn_output = rearrange(attn_output, 'b h l d -> b l (h d)')

            captured_dict['attn_output_before_o_proj'] = attn_output[0, m1_pos, :].detach().clone()
            if attn_weights is not None:
                captured_dict['attn_weights'] = attn_weights[0, :, m1_pos, :].detach().clone()
        else:
            raise NotImplementedError("Only training mode supported")

        # ===== Step 4: Output Projection =====
        attn_output = layer0_attn.o_proj(attn_output)
        captured_dict['attn_output_after_o_proj'] = attn_output[0, m1_pos, :].detach().clone()

        return attn_output, attn_weights

    # Patch the forward method
    layer0_attn.forward = patched_forward
    return original_forward


def analyze_two_configs(model, tokenizer, sample, device):
    """
    å¯¹æ¯”[P][Mâ‚]å’Œ[P][Mâ‚][Râ‚]ä¸¤ç§é…ç½®çš„attentionè®¡ç®—è¿‡ç¨‹
    """
    print("="*80)
    print("è¯¦ç»†Attentionè®¡ç®—å¯¹æ¯”")
    print("="*80)

    prompt_len = sample['prompt_len']
    input_ids_full = sample['input_ids'].to(device)
    position_ids_full = sample['position_ids'].to(device)

    # é…ç½®1: [P][Mâ‚]
    print("\n" + "#"*80)
    print("é…ç½®1: [P][Mâ‚]")
    print("#"*80)

    truncate_pos_1 = prompt_len + 3
    input_ids_1 = input_ids_full[:truncate_pos_1]
    position_ids_1 = position_ids_full[:truncate_pos_1]
    block_info_1 = [('mask', 3)]

    captured_1 = {}
    original_forward = patch_attention_forward(model, prompt_len, captured_1)

    with torch.no_grad():
        outputs_1 = model(
            input_ids_1.unsqueeze(0),
            position_ids=position_ids_1.unsqueeze(0),
            block_info=[block_info_1],
            prompt_len=[prompt_len],
            seq_lens=[input_ids_1.size(0)],
            use_cache=False,
        )

    # Restore original forward
    model.model.layers[0].self_attn.forward = original_forward

    print(f"åºåˆ—é•¿åº¦: {input_ids_1.size(0)}")
    print(f"K shape: {captured_1['k_full_shape']}")
    print(f"V shape: {captured_1['v_full_shape']}")

    # é…ç½®2: [P][Mâ‚][Râ‚]
    print("\n" + "#"*80)
    print("é…ç½®2: [P][Mâ‚][Râ‚]")
    print("#"*80)

    truncate_pos_2 = prompt_len + 7
    input_ids_2 = input_ids_full[:truncate_pos_2]
    position_ids_2 = position_ids_full[:truncate_pos_2]
    block_info_2 = [('mask', 3), ('real', 4)]

    captured_2 = {}
    original_forward = patch_attention_forward(model, prompt_len, captured_2)

    with torch.no_grad():
        outputs_2 = model(
            input_ids_2.unsqueeze(0),
            position_ids=position_ids_2.unsqueeze(0),
            block_info=[block_info_2],
            prompt_len=[prompt_len],
            seq_lens=[input_ids_2.size(0)],
            use_cache=False,
        )

    # Restore original forward
    model.model.layers[0].self_attn.forward = original_forward

    print(f"åºåˆ—é•¿åº¦: {input_ids_2.size(0)}")
    print(f"K shape: {captured_2['k_full_shape']}")
    print(f"V shape: {captured_2['v_full_shape']}")

    # ========== å¯¹æ¯”åˆ†æ ==========
    print("\n" + "="*80)
    print("ğŸ“Š é€æ­¥å¯¹æ¯”åˆ†æï¼ˆMâ‚ä½ç½®ï¼‰")
    print("="*80)

    comparisons = [
        ('q_before_rope', 'Q (before RoPE)'),
        ('k_before_rope', 'K (before RoPE)'),
        ('v', 'V'),
        ('q_after_rope', 'Q (after RoPE)'),
        ('k_after_rope', 'K (after RoPE)'),
        ('attn_output_before_o_proj', 'Attention output (before o_proj)'),
        ('attn_output_after_o_proj', 'Attention output (after o_proj)'),
    ]

    print(f"\n{'æ­¥éª¤':^40} | {'L2è·ç¦»':^15} | {'ä½™å¼¦ç›¸ä¼¼åº¦':^15} | {'æ˜¯å¦ç›¸åŒ':^10}")
    print("-"*90)

    for key, name in comparisons:
        if key not in captured_1 or key not in captured_2:
            continue

        t1 = captured_1[key]
        t2 = captured_2[key]

        # Flatten
        if t1.dim() > 1:
            t1 = t1.flatten()
            t2 = t2.flatten()

        l2_dist = torch.norm(t1 - t2).item()
        cos_sim = torch.nn.functional.cosine_similarity(
            t1.unsqueeze(0), t2.unsqueeze(0)
        ).item()

        is_same = "âœ…" if l2_dist < 1e-5 else "âŒ"

        print(f"{name:^40} | {l2_dist:>12.6f}  | {cos_sim:>12.6f}  | {is_same:^10}")

    # ========== å…³é”®åˆ†æ ==========
    print("\n" + "="*80)
    print("ğŸ” å…³é”®å‘ç°")
    print("="*80)

    # æ£€æŸ¥Q/K/V before RoPE
    q_before_diff = torch.norm(captured_1['q_before_rope'].flatten() - captured_2['q_before_rope'].flatten()).item()
    k_before_diff = torch.norm(captured_1['k_before_rope'].flatten() - captured_2['k_before_rope'].flatten()).item()
    v_diff = torch.norm(captured_1['v'].flatten() - captured_2['v'].flatten()).item()

    if q_before_diff < 1e-5 and k_before_diff < 1e-5 and v_diff < 1e-5:
        print("âœ… Q/K/V projectionå®Œå…¨ç›¸åŒï¼ˆRoPEä¹‹å‰ï¼‰")
    else:
        print(f"âŒ Q/K/V projectionä¸åŒï¼è¿™ä¸åº”è¯¥å‘ç”Ÿ...")
        return

    # æ£€æŸ¥RoPEä¹‹å
    q_after_diff = torch.norm(captured_1['q_after_rope'].flatten() - captured_2['q_after_rope'].flatten()).item()
    k_after_diff = torch.norm(captured_1['k_after_rope'].flatten() - captured_2['k_after_rope'].flatten()).item()

    if q_after_diff < 1e-5 and k_after_diff < 1e-5:
        print("âœ… RoPEåº”ç”¨åQ/Kå®Œå…¨ç›¸åŒ")
        print("   è¿™è¯´æ˜Mâ‚ä½ç½®çš„position_idç¡®å®ç›¸åŒ")
    else:
        print(f"âŒ RoPEåº”ç”¨åQ/Kä¸åŒï¼")
        print(f"   Qå·®å¼‚: {q_after_diff:.6f}")
        print(f"   Kå·®å¼‚: {k_after_diff:.6f}")
        print("   è¿™è¯´æ˜position_idæœ‰é—®é¢˜")
        return

    # æ£€æŸ¥attention output
    attn_before_diff = torch.norm(
        captured_1['attn_output_before_o_proj'].flatten() -
        captured_2['attn_output_before_o_proj'].flatten()
    ).item()

    if attn_before_diff > 1e-5:
        print(f"\nâŒ Attention outputä¸åŒï¼å·®å¼‚: {attn_before_diff:.6f}")
        print("\nâš ï¸ å…³é”®çŸ›ç›¾ï¼š")
        print(f"   1. Mâ‚çš„Q/K/Vå®Œå…¨ç›¸åŒï¼ˆåŒ…æ‹¬RoPEåï¼‰")
        print(f"   2. ä½†æ˜¯Attention outputå´ä¸åŒï¼")
        print(f"\nå¯èƒ½çš„åŸå› ï¼š")
        print(f"   A. K/VçŸ©é˜µçš„**å…¶ä»–ä½ç½®**ä¸åŒ")
        print(f"      - é…ç½®1: K/V shape = {captured_1['k_full_shape']}")
        print(f"      - é…ç½®2: K/V shape = {captured_2['k_full_shape']}")
        print(f"      - è™½ç„¶Mâ‚ä½ç½®çš„K/Vç›¸åŒï¼Œä½†å…¶ä»–ä½ç½®å¯èƒ½ä¸åŒ")
        print(f"\n   B. Attention maskçš„å½±å“")
        print(f"      - è™½ç„¶ç†è®ºä¸Šmaskåœ¨softmaxå‰åº”ç”¨ï¼ˆæˆ‘ä»¬éªŒè¯è¿‡ï¼‰")
        print(f"      - ä½†å®é™…çš„K/VçŸ©é˜µå¤§å°ä¸åŒï¼Œå¯èƒ½å½±å“FlexAttentionçš„åº•å±‚å®ç°")
        print(f"\n   C. éœ€è¦æ£€æŸ¥Mâ‚**å¯è§**çš„K/Vä½ç½®")
        print(f"      - Mâ‚åº”è¯¥åªèƒ½çœ‹åˆ°Prompt + Mâ‚è‡ªå·±")
        print(f"      - æˆ‘ä»¬éœ€è¦éªŒè¯è¿™äº›ä½ç½®çš„K/Væ˜¯å¦çœŸçš„ç›¸åŒ")

        # é¢å¤–æ£€æŸ¥ï¼šæ£€æŸ¥å‰278ä¸ªK/Væ˜¯å¦ç›¸åŒï¼ˆPrompt + Mâ‚ï¼‰
        print(f"\nğŸ” é¢å¤–æ£€æŸ¥ï¼šå‰278ä¸ªä½ç½®çš„K/Væ˜¯å¦ç›¸åŒï¼Ÿ")
        # è¿™éœ€è¦åœ¨forwardä¸­ä¿å­˜å®Œæ•´çš„K/VçŸ©é˜µ


def main():
    MODEL_PATH = "dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
    DATA_PATH = "data/openr1.parquet"

    print("åŠ è½½æ¨¡å‹...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    ).to(device)

    model.train()

    print("åŠ è½½æ•°æ®é›†...")
    dataset = InterleavedSFTDataset(
        parquet_files=DATA_PATH,
        tokenizer=tokenizer,
        prompt_key="prompt",
        response_key="target",
        block_size=4,
        max_length=6000,
        truncation="right",
    )

    sample = dataset[0]

    print(f"\næ ·æœ¬ä¿¡æ¯:")
    print(f"  Prompté•¿åº¦: {sample['prompt_len']}")
    print(f"  æ€»åºåˆ—é•¿åº¦: {sample['input_ids'].shape[0]}")

    # è¿è¡Œè¯¦ç»†åˆ†æ
    analyze_two_configs(model, tokenizer, sample, device)


if __name__ == "__main__":
    main()
