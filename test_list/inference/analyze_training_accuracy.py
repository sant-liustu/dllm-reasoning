#!/usr/bin/env python3
"""
åˆ†æè®­ç»ƒæ—¶Mask positions vs Real positionsçš„å‡†ç¡®ç‡
"""

import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd

# ç¦ç”¨torch.compile
import torch._dynamo
torch._dynamo.config.disable = True

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dllm_reasoning.trainer.interleaved_sft_dataset import create_interleaved_training_data, create_interleaved_labels
from dllm_reasoning.trainer.flex_attention_utils import create_block_mask_from_batch

MODEL_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/dllm_reasoning/checkpoints/interleaved_sft/global_step_17172/huggingface"
DATA_PATH = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/data/openr1.parquet"

print("="*100)
print("åˆ†æè®­ç»ƒå‡†ç¡®ç‡ç»„æˆ: Mask vs Real positions")
print("="*100)

# ========== åŠ è½½æ¨¡å‹ ==========
print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    local_files_only=True,
).to(device).train()

print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# ========== åŠ è½½è®­ç»ƒæ•°æ® ==========
print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {DATA_PATH}")
df = pd.read_parquet(DATA_PATH)
sample = df.iloc[0]

# ========== å¤„ç†æ•°æ® - å¯¹é½è®­ç»ƒé€»è¾‘ ==========
# å®Œå…¨å¤ç° interleaved_sft_dataset.py:479-516 çš„å¤„ç†é€»è¾‘
prompt = sample['prompt']
target = sample['target']

messages = list(prompt) if isinstance(prompt, (list, tuple)) else prompt
messages = messages.tolist() if hasattr(messages, 'tolist') else messages

target_list = target.tolist() if hasattr(target, 'tolist') else target
if isinstance(target_list, list) and len(target_list) > 0:
    target_content = target_list[0].get('content', '') if isinstance(target_list[0], dict) else str(target_list[0])
else:
    target_content = str(target_list)

# å»æ‰<think> - å¯¹é½è®­ç»ƒæ•°æ®å¤„ç†
if target_content.strip().startswith('<think>'):
    think_start = target_content.find('<think>')
    target_content = target_content[think_start + 7:].lstrip()

# ========== å…³é”®ï¼šä½¿ç”¨å’Œè®­ç»ƒå®Œå…¨ä¸€è‡´çš„tokenizationæ–¹å¼ ==========
# è§ interleaved_sft_dataset.py:502-516
prompt_only_str = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=False
)

full_conversation_str = prompt_only_str + target_content + tokenizer.eos_token

prompt_ids = tokenizer(prompt_only_str, return_tensors="pt", add_special_tokens=False)["input_ids"][0].to(device)
input_ids = tokenizer(full_conversation_str, return_tensors="pt", add_special_tokens=False)["input_ids"][0].to(device)

prompt_len = prompt_ids.size(0)
loss_mask = torch.ones(input_ids.size(0), dtype=torch.long, device=device)
loss_mask[:prompt_len] = 0

# åˆ›å»ºinterleavedæ ¼å¼
interleaved_ids, position_ids, interleaved_loss_mask, block_info = create_interleaved_training_data(
    input_ids=input_ids,
    loss_mask=loss_mask,
    block_size=4,
    mask_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)

# åˆ›å»ºlabels
labels = create_interleaved_labels(
    original_input_ids=input_ids,
    interleaved_input_ids=interleaved_ids,
    interleaved_loss_mask=interleaved_loss_mask,
    block_size=4,
    prompt_len=prompt_len,
    pad_token_id=tokenizer.pad_token_id,
)

print(f"\nğŸ“ æ•°æ®ä¿¡æ¯:")
print(f"   Prompté•¿åº¦: {prompt_len}")
print(f"   Interleavedåºåˆ—é•¿åº¦: {interleaved_ids.size(0)}")
print(f"   Blockæ•°é‡: {len(block_info)}")

# è½¬æ¢block_infoæ ¼å¼
block_info_converted = [(seg_type, seg_len) for seg_type, _, seg_len in block_info]

# ========== Forward pass è®¡ç®—å®Œæ•´åºåˆ—çš„logits ==========
print(f"\nğŸš€ Forward pass...")

batch = {
    "input_ids": interleaved_ids.unsqueeze(0),
    "position_ids": position_ids.unsqueeze(0),
    "block_info": [block_info_converted],
    "prompt_len": [prompt_len],
    "seq_lens": [interleaved_ids.size(0)],
}

block_mask = create_block_mask_from_batch(batch, device)

with torch.no_grad():
    outputs = model(
        interleaved_ids.unsqueeze(0),
        position_ids=position_ids.unsqueeze(0),
        attention_mask=block_mask,
        use_cache=False
    )
    logits = outputs.logits[0]  # [seq_len, vocab]

print(f"   Logits shape: {logits.shape}")

# ========== åˆ†æå„ä¸ªä½ç½®çš„å‡†ç¡®ç‡ ==========
print(f"\nğŸ“Š åˆ†æå„ä½ç½®ç±»å‹çš„å‡†ç¡®ç‡:")

predictions = logits.argmax(dim=-1)
valid_mask = (labels != -100)

# ç»Ÿè®¡
prompt_last_positions = []
mask_positions = []
real_positions = []

# ========== é‡è¦ä¿®æ­£ï¼šblock_infoçš„ä½ç½®æ˜¯ä»prompt_lenå¼€å§‹çš„ï¼ ==========
# æ ¹æ®block_infoç¡®å®šæ¯ä¸ªä½ç½®çš„ç±»å‹
current_pos = prompt_len  # ä»promptç»“æŸä½ç½®å¼€å§‹ï¼
for seg_type, _, seg_len in block_info:
    if seg_type == 'mask':
        mask_positions.extend(range(current_pos, current_pos + seg_len))
    elif seg_type == 'real':
        real_positions.extend(range(current_pos, current_pos + seg_len))
    current_pos += seg_len

# Promptæœ€åä¸€ä¸ªä½ç½®
if labels[prompt_len - 1].item() != -100:
    prompt_last_positions.append(prompt_len - 1)

print(f"\nä½ç½®ç»Ÿè®¡:")
print(f"   Prompt[-1] æœ‰labelçš„ä½ç½®æ•°: {len(prompt_last_positions)}")
print(f"   Mask positions: {len(mask_positions)}")
print(f"   Real positions: {len(real_positions)}")
print(f"   æ€»è®¡æœ‰labelçš„ä½ç½®: {valid_mask.sum().item()}")

# è®¡ç®—å„éƒ¨åˆ†å‡†ç¡®ç‡
def calc_accuracy(positions, name):
    if len(positions) == 0:
        print(f"\n{name}: æ— æ•°æ®")
        return

    positions_tensor = torch.tensor(positions, device=device)
    preds = predictions[positions_tensor]
    labels_subset = labels[positions_tensor]
    valid = labels_subset != -100

    if valid.sum() == 0:
        print(f"\n{name}: æ— æœ‰æ•ˆlabel")
        return

    correct = (preds == labels_subset) & valid
    accuracy = correct.sum().float() / valid.sum().float()

    print(f"\n{name}:")
    print(f"   æœ‰æ•ˆä½ç½®æ•°: {valid.sum().item()}")
    print(f"   å‡†ç¡®æ•°: {correct.sum().item()}")
    print(f"   å‡†ç¡®ç‡: {accuracy.item():.4f}")

    # æ˜¾ç¤ºå‰5ä¸ªä½ç½®çš„é¢„æµ‹è¯¦æƒ…
    print(f"   å‰5ä¸ªä½ç½®è¯¦æƒ…:")
    for i in range(min(5, len(positions))):
        pos = positions[i]
        pred_id = predictions[pos].item()
        true_id = labels[pos].item()
        if true_id != -100:
            is_correct = "âœ…" if pred_id == true_id else "âŒ"
            print(f"     [{i}] pos={pos}: {is_correct} pred={pred_id} '{tokenizer.decode([pred_id])}', true={true_id} '{tokenizer.decode([true_id])}'")

calc_accuracy(prompt_last_positions, "Prompt[-1]")
calc_accuracy(mask_positions, "Mask positions")
calc_accuracy(real_positions, "Real positions")

# æ€»ä½“å‡†ç¡®ç‡
total_correct = (predictions == labels) & valid_mask
total_accuracy = total_correct.sum().float() / valid_mask.sum().float()
print(f"\næ€»ä½“å‡†ç¡®ç‡: {total_accuracy.item():.4f}")

# ========== åˆ†æä¸åŒblockçš„Maskå‡†ç¡®ç‡ ==========
print(f"\n" + "="*100)
print("åˆ†æå„ä¸ªBlockçš„Maskå‡†ç¡®ç‡")
print("="*100)

block_idx = 0
current_pos = prompt_len

for seg_type, _, seg_len in block_info:
    if seg_type == 'mask':
        block_mask_positions = list(range(current_pos, current_pos + seg_len))

        preds = predictions[current_pos:current_pos + seg_len]
        labels_subset = labels[current_pos:current_pos + seg_len]
        valid = labels_subset != -100

        if valid.sum() > 0:
            correct = (preds == labels_subset) & valid
            accuracy = correct.sum().float() / valid.sum().float()

            print(f"\nBlock {block_idx} Mask positions [{current_pos}:{current_pos+seg_len}]:")
            print(f"   å‡†ç¡®ç‡: {accuracy.item():.4f} ({correct.sum().item()}/{valid.sum().item()})")

            # æ˜¾ç¤ºæ¯ä¸ªmaskçš„é¢„æµ‹
            for i in range(seg_len):
                pos = current_pos + i
                pred_id = preds[i].item()
                true_id = labels_subset[i].item()
                if true_id != -100:
                    is_correct = "âœ…" if pred_id == true_id else "âŒ"
                    print(f"     Mask[{i}]: {is_correct} pred={pred_id} '{tokenizer.decode([pred_id])}', true={true_id} '{tokenizer.decode([true_id])}'")

        block_idx += 1

    current_pos += seg_len
