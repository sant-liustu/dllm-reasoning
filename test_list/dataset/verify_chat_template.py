#!/usr/bin/env python3
"""
éªŒè¯ chat_template å¯¹ assistant response çš„å®Œæ•´å¤„ç†æµç¨‹
ç¡®ä¿æ‰‹åŠ¨æ‹¼æ¥ä¸ä¼šé—æ¼ä»»ä½•è§„èŒƒæ“ä½œ
"""

import sys
sys.path.insert(0, '/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/dllm_reasoning')

from transformers import AutoTokenizer
import json

def main():
    # åŠ è½½ tokenizer
    model_path = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/checkpoints/iterative_refine/global_step_17172/huggingface"
    print(f"ğŸ”§ åŠ è½½ tokenizer: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)

    # æ„é€ æµ‹è¯•æ•°æ®
    prompt = [
        {"role": "system", "content": "Please reason step by step..."},
        {"role": "user", "content": "Solve x^2 = 0"}
    ]

    # æµ‹è¯•ç”¨çš„ response (åŒ…å« <think> æ ‡ç­¾)
    response_with_think = """<think>
Okay, so I need to solve the equation xÂ² = 0. Let me think about this step by step.
</think>

Solution: x = 0"""

    # æµ‹è¯•ç”¨çš„ response (ä¸åŒ…å« <think> æ ‡ç­¾)
    response_no_think = "Solution: x = 0"

    print("\n" + "="*80)
    print("ğŸ“‹ æµ‹è¯• 1: Prompt only (add_generation_prompt=True)")
    print("="*80)
    prompt_only_str = tokenizer.apply_chat_template(
        prompt, add_generation_prompt=True, tokenize=False
    )
    print(f"ç»“æœé•¿åº¦: {len(prompt_only_str)} å­—ç¬¦")
    print(f"æœ€å 200 å­—ç¬¦:\n{repr(prompt_only_str[-200:])}")

    print("\n" + "="*80)
    print("ğŸ“‹ æµ‹è¯• 2: Full conversation with <think> response")
    print("="*80)
    full_messages_with_think = prompt + [{"role": "assistant", "content": response_with_think}]
    full_str_with_think = tokenizer.apply_chat_template(
        full_messages_with_think, add_generation_prompt=False, tokenize=False
    )
    print(f"ç»“æœé•¿åº¦: {len(full_str_with_think)} å­—ç¬¦")
    print(f"æœ€å 200 å­—ç¬¦:\n{repr(full_str_with_think[-200:])}")

    print("\n" + "="*80)
    print("ğŸ“‹ æµ‹è¯• 3: Full conversation without <think> response")
    print("="*80)
    full_messages_no_think = prompt + [{"role": "assistant", "content": response_no_think}]
    full_str_no_think = tokenizer.apply_chat_template(
        full_messages_no_think, add_generation_prompt=False, tokenize=False
    )
    print(f"ç»“æœé•¿åº¦: {len(full_str_no_think)} å­—ç¬¦")
    print(f"æœ€å 200 å­—ç¬¦:\n{repr(full_str_no_think[-200:])}")

    print("\n" + "="*80)
    print("ğŸ” å…³é”®åˆ†æ: Response éƒ¨åˆ†çš„å¤„ç†")
    print("="*80)

    # æå– response éƒ¨åˆ† (full - prompt_only)
    response_part_with_think = full_str_with_think[len(prompt_only_str):]
    response_part_no_think = full_str_no_think[len(prompt_only_str):]

    print(f"\nåŸå§‹ response (with think): {len(response_with_think)} å­—ç¬¦")
    print(f"å¤„ç†å response éƒ¨åˆ†: {len(response_part_with_think)} å­—ç¬¦")
    print(f"å®Œæ•´å†…å®¹:\n{repr(response_part_with_think)}")

    print(f"\nåŸå§‹ response (no think): {len(response_no_think)} å­—ç¬¦")
    print(f"å¤„ç†å response éƒ¨åˆ†: {len(response_part_no_think)} å­—ç¬¦")
    print(f"å®Œæ•´å†…å®¹:\n{repr(response_part_no_think)}")

    print("\n" + "="*80)
    print("ğŸ” æ£€æŸ¥ EOS token")
    print("="*80)
    print(f"tokenizer.eos_token = {repr(tokenizer.eos_token)}")
    print(f"tokenizer.eos_token_id = {tokenizer.eos_token_id}")

    # æ£€æŸ¥æ˜¯å¦åŒ…å« EOS token
    has_eos_with_think = tokenizer.eos_token in response_part_with_think if tokenizer.eos_token else False
    has_eos_no_think = tokenizer.eos_token in response_part_no_think if tokenizer.eos_token else False

    print(f"\nResponse (with think) åŒ…å« EOS token: {has_eos_with_think}")
    print(f"Response (no think) åŒ…å« EOS token: {has_eos_no_think}")

    print("\n" + "="*80)
    print("ğŸ” æ£€æŸ¥ç‰¹æ®Šæ ‡è®°")
    print("="*80)

    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ç‰¹æ®Šæ ‡è®°
    special_tokens = ["<ï½œendâ–ofâ–sentenceï½œ>", "<|im_end|>", "<|endoftext|>", "</s>"]

    for token in special_tokens:
        in_with_think = token in response_part_with_think
        in_no_think = token in response_part_no_think
        print(f"  {token}:")
        print(f"    - Response (with think): {in_with_think}")
        print(f"    - Response (no think): {in_no_think}")

    print("\n" + "="*80)
    print("ğŸ’¡ è§£å†³æ–¹æ¡ˆéªŒè¯: æ‰‹åŠ¨æ‹¼æ¥æ–¹å¼")
    print("="*80)

    # æ–¹æ¡ˆ 2: æ‰‹åŠ¨æ‹¼æ¥
    response_content = response_with_think
    if response_content.strip().startswith('<think>'):
        # å»æ‰å¼€å¤´çš„ <think>\n,å› ä¸º prompt å·²ç»æœ‰äº†
        response_content = response_content[response_content.find('<think>') + 7:].lstrip()

    # æ‰‹åŠ¨æ„é€ å®Œæ•´å¯¹è¯
    manual_full_str = prompt_only_str + response_content + tokenizer.eos_token

    print(f"åŸå§‹æ–¹å¼ (apply_chat_template) é•¿åº¦: {len(full_str_with_think)}")
    print(f"æ‰‹åŠ¨æ‹¼æ¥æ–¹å¼é•¿åº¦: {len(manual_full_str)}")
    print(f"\nåŸå§‹æ–¹å¼æœ€å 150 å­—ç¬¦:\n{repr(full_str_with_think[-150:])}")
    print(f"\næ‰‹åŠ¨æ‹¼æ¥æœ€å 150 å­—ç¬¦:\n{repr(manual_full_str[-150:])}")

    # å¯¹æ¯”å·®å¼‚
    print("\n" + "="*80)
    print("ğŸ” å·®å¼‚å¯¹æ¯”")
    print("="*80)

    if full_str_with_think == manual_full_str:
        print("âœ… å®Œå…¨ä¸€è‡´!")
    else:
        print("âŒ å­˜åœ¨å·®å¼‚")

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸åŒçš„ä½ç½®
        min_len = min(len(full_str_with_think), len(manual_full_str))
        first_diff = None
        for i in range(min_len):
            if full_str_with_think[i] != manual_full_str[i]:
                first_diff = i
                break

        if first_diff is not None:
            print(f"\nç¬¬ä¸€ä¸ªå·®å¼‚ä½ç½®: {first_diff}")
            print(f"åŸå§‹æ–¹å¼: {repr(full_str_with_think[first_diff:first_diff+100])}")
            print(f"æ‰‹åŠ¨æ–¹å¼: {repr(manual_full_str[first_diff:first_diff+100])}")
        elif len(full_str_with_think) != len(manual_full_str):
            print(f"\né•¿åº¦ä¸åŒ:")
            print(f"  åŸå§‹æ–¹å¼: {len(full_str_with_think)}")
            print(f"  æ‰‹åŠ¨æ–¹å¼: {len(manual_full_str)}")

    print("\n" + "="*80)
    print("ğŸ“Š Tokenization éªŒè¯")
    print("="*80)

    # Tokenize ä¸¤ç§æ–¹å¼çš„ç»“æœ
    tokens_original = tokenizer(full_str_with_think, return_tensors="pt", add_special_tokens=False)
    tokens_manual = tokenizer(manual_full_str, return_tensors="pt", add_special_tokens=False)

    print(f"åŸå§‹æ–¹å¼ token æ•°é‡: {tokens_original['input_ids'].shape[1]}")
    print(f"æ‰‹åŠ¨æ–¹å¼ token æ•°é‡: {tokens_manual['input_ids'].shape[1]}")
    print(f"åŸå§‹æ–¹å¼æœ€å 10 ä¸ª token IDs: {tokens_original['input_ids'][0, -10:].tolist()}")
    print(f"æ‰‹åŠ¨æ–¹å¼æœ€å 10 ä¸ª token IDs: {tokens_manual['input_ids'][0, -10:].tolist()}")

    # è§£ç æœ€åå‡ ä¸ª token
    print(f"\nåŸå§‹æ–¹å¼æœ€å 10 ä¸ª token è§£ç :")
    for i, tid in enumerate(tokens_original['input_ids'][0, -10:].tolist()):
        decoded = tokenizer.decode([tid])
        print(f"  {i}: {tid} -> {repr(decoded)}")

    print(f"\næ‰‹åŠ¨æ–¹å¼æœ€å 10 ä¸ª token è§£ç :")
    for i, tid in enumerate(tokens_manual['input_ids'][0, -10:].tolist()):
        decoded = tokenizer.decode([tid])
        print(f"  {i}: {tid} -> {repr(decoded)}")

    print("\n" + "="*80)
    print("âœ… éªŒè¯å®Œæˆ")
    print("="*80)

if __name__ == "__main__":
    main()
