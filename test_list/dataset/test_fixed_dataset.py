#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„æ•°æ®å¤„ç†é€»è¾‘
éªŒè¯ <think> å†…å®¹æ˜¯å¦è¢«æ­£ç¡®ä¿ç•™
"""

import sys
sys.path.insert(0, '/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/dllm_reasoning')

from transformers import AutoTokenizer
import pandas as pd

def main():
    # åŠ è½½ tokenizer
    model_path = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/checkpoints/iterative_refine/global_step_17172/huggingface"
    print(f"ğŸ”§ åŠ è½½ tokenizer: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)

    # åŠ è½½ä¸€æ¡çœŸå®è®­ç»ƒæ•°æ®
    data_path = "/data/v-zihaliu/amlt-RLF-ExpConfig/Dream/data/openr1.parquet"
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
    df = pd.read_parquet(data_path)
    sample = df.iloc[0]

    prompt = sample['prompt']
    target = sample['target']
    response_content = target[0]['content']

    print(f"\nåŸå§‹ response é•¿åº¦: {len(response_content)} å­—ç¬¦")
    print(f"æ˜¯å¦åŒ…å« <think>: {'<think>' in response_content}")
    print(f"æ˜¯å¦åŒ…å« </think>: {'</think>' in response_content}")

    # åˆ†æ response ç»“æ„
    if '<think>' in response_content and '</think>' in response_content:
        think_start = response_content.find('<think>')
        think_end = response_content.find('</think>') + 8
        think_content = response_content[think_start:think_end]
        solution_content = response_content[think_end:].strip()

        print(f"\n<think> æ¨ç†éƒ¨åˆ†: {len(think_content)} å­—ç¬¦")
        print(f"Solution éƒ¨åˆ†: {len(solution_content)} å­—ç¬¦")

    print("\n" + "="*80)
    print("ğŸ”§ æ–¹æ³• 1: åŸå§‹æ–¹å¼ (apply_chat_template - ä¼šè¿‡æ»¤ <think>)")
    print("="*80)

    prompt_only_str_old = tokenizer.apply_chat_template(
        list(prompt), add_generation_prompt=True, tokenize=False
    )
    full_messages_old = list(prompt) + [{"role": "assistant", "content": response_content}]
    full_str_old = tokenizer.apply_chat_template(
        full_messages_old, add_generation_prompt=False, tokenize=False
    )

    print(f"Prompt é•¿åº¦: {len(prompt_only_str_old)} å­—ç¬¦")
    print(f"å®Œæ•´å¯¹è¯é•¿åº¦: {len(full_str_old)} å­—ç¬¦")
    print(f"Response éƒ¨åˆ†é•¿åº¦: {len(full_str_old) - len(prompt_only_str_old)} å­—ç¬¦")

    tokens_old = tokenizer(full_str_old, return_tensors="pt", add_special_tokens=False)
    print(f"æ€» token æ•°: {tokens_old['input_ids'].shape[1]}")

    print("\n" + "="*80)
    print("âœ… æ–¹æ³• 2: ä¿®å¤åçš„æ–¹å¼ (æ‰‹åŠ¨æ‹¼æ¥ - ä¿ç•™ <think>)")
    print("="*80)

    # ä½¿ç”¨æ–°çš„é€»è¾‘
    prompt_only_str_new = tokenizer.apply_chat_template(
        list(prompt), add_generation_prompt=True, tokenize=False
    )

    response_content_new = response_content
    if response_content_new.strip().startswith('<think>'):
        think_start = response_content_new.find('<think>')
        response_content_new = response_content_new[think_start + 7:].lstrip()

    full_str_new = prompt_only_str_new + response_content_new + tokenizer.eos_token

    print(f"Prompt é•¿åº¦: {len(prompt_only_str_new)} å­—ç¬¦")
    print(f"å®Œæ•´å¯¹è¯é•¿åº¦: {len(full_str_new)} å­—ç¬¦")
    print(f"Response éƒ¨åˆ†é•¿åº¦: {len(full_str_new) - len(prompt_only_str_new)} å­—ç¬¦")

    tokens_new = tokenizer(full_str_new, return_tensors="pt", add_special_tokens=False)
    print(f"æ€» token æ•°: {tokens_new['input_ids'].shape[1]}")

    print("\n" + "="*80)
    print("ğŸ“Š å¯¹æ¯”ç»“æœ")
    print("="*80)
    print(f"Token æ•°é‡å¢åŠ : {tokens_new['input_ids'].shape[1]} - {tokens_old['input_ids'].shape[1]} = {tokens_new['input_ids'].shape[1] - tokens_old['input_ids'].shape[1]}")
    print(f"Response é•¿åº¦å¢åŠ : {len(full_str_new) - len(prompt_only_str_new)} - {len(full_str_old) - len(prompt_only_str_old)} = {(len(full_str_new) - len(prompt_only_str_new)) - (len(full_str_old) - len(prompt_only_str_old))} å­—ç¬¦")

    # æ£€æŸ¥æ˜¯å¦åŒ…å« <think> å†…å®¹
    response_part_old = full_str_old[len(prompt_only_str_old):]
    response_part_new = full_str_new[len(prompt_only_str_new):]

    print(f"\nåŸå§‹æ–¹å¼ response åŒ…å« <think>: {'<think>' in response_part_old}")
    print(f"åŸå§‹æ–¹å¼ response åŒ…å« </think>: {'</think>' in response_part_old}")
    print(f"æ–°æ–¹å¼ response åŒ…å« <think>: {'<think>' in response_part_new}")
    print(f"æ–°æ–¹å¼ response åŒ…å« </think>: {'</think>' in response_part_new}")

    print("\n" + "="*80)
    print("ğŸ” éªŒè¯ EOS token")
    print("="*80)
    print(f"åŸå§‹æ–¹å¼æœ€å 50 å­—ç¬¦: {repr(full_str_old[-50:])}")
    print(f"æ–°æ–¹å¼æœ€å 50 å­—ç¬¦: {repr(full_str_new[-50:])}")
    print(f"\nåŸå§‹æ–¹å¼åŒ…å« EOS: {tokenizer.eos_token in full_str_old}")
    print(f"æ–°æ–¹å¼åŒ…å« EOS: {tokenizer.eos_token in full_str_new}")

    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("="*80)

    if '</think>' in response_part_new and tokenizer.eos_token in full_str_new:
        print("âœ… æˆåŠŸ! æ–°æ–¹å¼ä¿ç•™äº†å®Œæ•´çš„ <think> æ¨ç†è¿‡ç¨‹å¹¶æ­£ç¡®æ·»åŠ äº† EOS token")
    else:
        print("âŒ å¤±è´¥! è¯·æ£€æŸ¥é€»è¾‘")

if __name__ == "__main__":
    main()
