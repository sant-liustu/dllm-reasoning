# dLLM Reasoning: Interleaved SFT è®­ç»ƒæ¡†æ¶

åŸºäº **Interleaved SFTï¼ˆäº¤é”™å¼ç›‘ç£å¾®è°ƒï¼‰**çš„è®­ç»ƒæ¡†æ¶ï¼Œä½¿ç”¨ FlexAttention BlockMask è¿›è¡Œ Next Block Prediction è®­ç»ƒã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

#### åˆ›å»º Conda ç¯å¢ƒ

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n dllm_env python=3.10
conda activate dllm_env

# å®‰è£…é¡¹ç›®ä¾èµ–
cd dllm_reasoning
pip install -r requirements.txt

# å®‰è£… VERLï¼ˆå¿…éœ€ï¼‰
cd ..
git clone https://github.com/volcengine/verl
cd verl
pip install -e .

# å®‰è£…æœ¬é¡¹ç›®
cd ../dllm_reasoning
pip install -e .
```

---

### 2. æ•°æ®å‡†å¤‡

#### æ•°æ®æ ¼å¼è¦æ±‚

è®­ç»ƒæ•°æ®éœ€è¦æ˜¯ **Parquet æ–‡ä»¶**ï¼ŒåŒ…å«ä»¥ä¸‹åˆ—ï¼š

| åˆ—å | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `prompt` | str æˆ– list | ç”¨æˆ·é—®é¢˜ï¼ˆæ”¯æŒå­—ç¬¦ä¸²æˆ– chat æ¶ˆæ¯åˆ—è¡¨ï¼‰ |
| `target` | str æˆ– list | æ¨¡å‹å›ç­”ï¼ˆæ”¯æŒå­—ç¬¦ä¸²æˆ– chat æ¶ˆæ¯åˆ—è¡¨ï¼‰ |

#### é‡è¦ï¼š`<think>` æ ‡ç­¾å¤„ç†

**å¦‚æœä½ çš„ `target` åˆ—ä¸­çš„å›ç­”ä»¥ `<think>` æ ‡ç­¾å¼€å¤´**ï¼Œæ•°æ®é›†ä¼šè‡ªåŠ¨æå– `<think>` åé¢çš„å†…å®¹ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼š

```python
# åŸå§‹æ•°æ®ç¤ºä¾‹
{
    "prompt": [{"role": "user", "content": "What is 2+2?"}],
    "target": [{"role": "assistant", "content": "<think>\n Let me calculate...</think>The answer is 4."}]
}

# å®é™…è®­ç»ƒæ—¶ä½¿ç”¨çš„å†…å®¹ï¼š
# "Let me calculate...</think>The answer is 4."
# ï¼ˆè‡ªåŠ¨å»æ‰äº†å¼€å¤´çš„ "<think>",å› ä¸ºåœ¨promptçš„templateä¸­ä¼šåœ¨å¤„ç†å®Œçš„promptåæ¥ä¸€ä¸ªthinkå’Œæ¢è¡Œå·ï¼Œæ‰€ä»¥å¤„ç†targetçš„æ—¶å€™å°±æ‰‹åŠ¨å»æ‰äº†å‰é¢çš„thinkå’Œæ¢è¡Œå·ï¼Œå¦‚æœä½ çš„tagetå¼€å¤´æ²¡æœ‰æ¢è¡Œå·æˆ–thinkï¼Œéœ€æ³¨æ„è¿™é‡Œçš„æ•°æ®å¤„ç†ï¼Œå¾—è¿›è¡Œç›¸åº”çš„é€‚é…ä¿®æ”¹
```

å¤„ç†é€»è¾‘è¯¦è§ [interleaved_sft_dataset.py:506-509](dllm_reasoning/trainer/interleaved_sft_dataset.py#L506-L509)

#### åˆ›å»ºæ•°æ®æ–‡ä»¶ç¤ºä¾‹

```python
import pandas as pd

# ç¤ºä¾‹ : Chat æ¶ˆæ¯æ ¼å¼ï¼ˆæ¨èï¼‰
data = [
    {
        "prompt": [{"role": "user", "content": "What is 2+2?"}],
        "target": [{"role": "assistant", "content": "<think>Let me think...</think>The answer is 4."}]
    }
]

# ä¿å­˜ä¸º parquet
df = pd.DataFrame(data)
df.to_parquet("train.parquet", index=False)
```

#### æ”¾ç½®æ•°æ®æ–‡ä»¶

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º data æ–‡ä»¶å¤¹
mkdir -p data

# å°†æ•°æ®æ–‡ä»¶æ”¾å…¥ data ç›®å½•
cp /path/to/your/train.parquet data/
```

---

### 3. ä¿®æ”¹é…ç½®

ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼š[dllm_reasoning/dllm_reasoning/config/interleaved_sft.yaml](dllm_reasoning/dllm_reasoning/config/interleaved_sft.yaml)

**å…³é”®é…ç½®é¡¹ï¼š**

```yaml
data:
  train_files: data/train.parquet      # è®­ç»ƒæ•°æ®è·¯å¾„
  val_files: null                       # éªŒè¯æ•°æ®è·¯å¾„ï¼ˆnull=ä¸ä½¿ç”¨ï¼‰
  prompt_key: prompt                    # Parquet ä¸­çš„ prompt åˆ—å
  response_key: target                  # Parquet ä¸­çš„ response åˆ—å
  block_size: 4                         # Block å¤§å°ï¼ˆNext Block Predictionï¼‰
  max_length: 2048                      # æœ€å¤§åºåˆ—é•¿åº¦

model:
  partial_pretrain: path/to/your/model  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
  enable_gradient_checkpointing: true   # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

optim:
  lr: 1e-5                              # å­¦ä¹ ç‡
  gradient_accumulation_steps: 64       # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°

trainer:
  total_epochs: 3                       # è®­ç»ƒè½®æ•°
  default_local_dir: ./checkpoints/my_exp  # æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„
  save_checkpoint_steps: 1000           # æ¯ N æ­¥ä¿å­˜æ£€æŸ¥ç‚¹
  logger: ['console', 'wandb']          # æ—¥å¿—è®°å½•å™¨
```

---

### 4. å¯åŠ¨è®­ç»ƒ

è®­ç»ƒè„šæœ¬ï¼š[dllm_reasoning/scripts/train_interleaved.sh](dllm_reasoning/scripts/train_interleaved.sh)

```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd /path/to/Dream

# 4 GPU è®­ç»ƒï¼ˆæ¨èï¼‰
bash dllm_reasoning/scripts/train_interleaved.sh 4 ./checkpoints/my_exp

# å• GPU è®­ç»ƒ
bash dllm_reasoning/scripts/train_interleaved.sh 1 ./checkpoints/my_exp

# åå°è®­ç»ƒï¼ˆè¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶ï¼‰
nohup bash dllm_reasoning/scripts/train_interleaved.sh 4 ./checkpoints/my_exp training.log &

# è‡ªå®šä¹‰å‚æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰
bash dllm_reasoning/scripts/train_interleaved.sh 4 ./checkpoints/my_exp \
    data.train_files=data/my_data.parquet \
    model.partial_pretrain=meta-llama/Llama-3-8B \
    trainer.total_epochs=5
```

**è®­ç»ƒå¯åŠ¨åä¼šçœ‹åˆ°ï¼š**

```
é¡¹ç›®æ ¹ç›®å½•: /path/to/Dream
GPU æ•°é‡: 4
ä¿å­˜ç›®å½•: ./checkpoints/my_exp
[2025-01-15 10:00:00] [INFO] åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ...
[2025-01-15 10:00:01] [INFO] åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ: rank=0, world_size=4
[2025-01-15 10:00:05] [INFO] åˆ›å»ºäº¤é”™è®­ç»ƒæ•°æ®é›†...
[InterleavedSFTDataset] Loaded 10000 samples
[InterleavedSFTDataset] Block size: 4
...
[Epoch 1/3] Step 100/1000: loss=1.234, lr=0.00001
```

---

### 5. æ¨ç†æµ‹è¯•

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ä»¥ä¸‹è„šæœ¬æµ‹è¯•ä½ çš„æ¨¡å‹ã€‚

#### æµ‹è¯•è„šæœ¬ï¼š[dllm_reasoning/test_list/inference/ckpt_inference_by_hand.py](dllm_reasoning/test_list/inference/ckpt_inference_by_hand.py)

**ä¿®æ”¹è„šæœ¬é…ç½®ï¼š**

```python
# ä¿®æ”¹ç¬¬ 303-304 è¡Œ
MODEL_PATH = "./checkpoints/my_exp/global_step_1000/huggingface"
DATA_PATH = "data/train.parquet"  # ç”¨äºæµ‹è¯•çš„æ•°æ®æ–‡ä»¶
```

**è¿è¡Œæ¨ç†ï¼š**

```bash
python dllm_reasoning/test_list/inference/ckpt_inference_by_hand.py
```

#### æ¨ç†ç‰¹æ€§

è„šæœ¬æ”¯æŒä»¥ä¸‹æµ‹è¯•åœºæ™¯ï¼ˆå¯åœ¨è„šæœ¬ä¸­æ³¨é‡Š/å–æ¶ˆæ³¨é‡Šï¼‰ï¼š

1. **åœºæ™¯1ï¼šè‡ªç„¶çš„è‡ªå›å½’ç”Ÿæˆ** (ç¬¬ 22-34 è¡Œ)
   - æ ‡å‡†çš„è‡ªå›å½’ç”Ÿæˆï¼Œä¸ä½¿ç”¨ block-wise æ¨ç†

2. **åœºæ™¯1.1ï¼šTeacher Forcing è¿‡æ‹Ÿåˆæµ‹è¯•** (ç¬¬ 36-68 è¡Œ)
   - ä½¿ç”¨ ground truth æµ‹è¯•æ¨¡å‹çš„è¿‡æ‹Ÿåˆå‡†ç¡®ç‡

3. **åœºæ™¯2ï¼šBlockwise ç”Ÿæˆ** (ç¬¬ 71-134 è¡Œ)
   - ä½¿ç”¨ block-wise æ¨ç†ç”Ÿæˆæ–‡æœ¬
   - æ¯æ¬¡ç”Ÿæˆä¸€ä¸ª block çš„ token

4. **åœºæ™¯2.1ï¼šBlockwise Teacher Forcing æµ‹è¯•** (ç¬¬ 136-214 è¡Œ)
   - Block-wise æ¨¡å¼ä¸‹çš„è¿‡æ‹Ÿåˆå‡†ç¡®ç‡æµ‹è¯•

5. **åœºæ™¯3ï¼šé…å¤‡æ¨¡å‹è‡ªæˆ‘æ„ŸçŸ¥çš„ Blockwise ç”Ÿæˆ** (ç¬¬ 217-299 è¡Œï¼Œ**é»˜è®¤å¯ç”¨**)
   - Block-wise ç”Ÿæˆ + è‡ªé€‚åº”åœæ­¢ç­–ç•¥
   - å¦‚æœ token é¢„æµ‹æ¦‚ç‡ < 0.7ï¼Œåœæ­¢å½“å‰ block
   - å¦‚æœæ£€æµ‹åˆ°é‡å¤ tokenï¼Œåœæ­¢å½“å‰ block

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
==========================================
æµ‹è¯•Checkpoint 1000 - ä¸‰ç§Teacher Forcingåœºæ™¯
==========================================

åœºæ™¯3ï¼šé…å¤‡æ¨¡å‹è‡ªæˆ‘æ„ŸçŸ¥çš„Blockwiseç”Ÿæˆ
Block 0, Token 0: pred_token ('The')
top-5 predictions:
  Tokens: 'The', 'A', 'To', 'In', 'For'; Probabilities: 0.8234, 0.0512, 0.0234, 0.0123, 0.0098
Block 0, Token 1: pred_token ('answer')
...
âœ… Blockwiseç”Ÿæˆå®Œæˆ
ä¸€å…±ç”Ÿæˆäº†156ä¸ªtoken
å¹³å‡æ¯ä¸ªblockç”Ÿæˆäº†3.12ä¸ªtoken
ç”Ÿæˆçš„tokensï¼šThe answer is 4 because 2+2 equals 4.
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
Dream/                                   # é¡¹ç›®æ ¹ç›®å½•
â”œâ”€â”€ dllm_reasoning/                      # è®­ç»ƒåŒ…
â”‚   â”œâ”€â”€ dllm_reasoning/                  # æ ¸å¿ƒä»£ç 
â”‚   â”‚   â”œâ”€â”€ config/                      # é…ç½®æ–‡ä»¶
â”‚   â”‚   â”‚   â””â”€â”€ interleaved_sft.yaml    # äº¤é”™è®­ç»ƒé…ç½®
â”‚   â”‚   â”œâ”€â”€ trainer/                     # è®­ç»ƒå™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ interleaved_sft_dataset.py     # æ•°æ®é›†ï¼ˆé‡è¦ï¼ï¼‰
â”‚   â”‚   â”‚   â””â”€â”€ interleaved_sft_trainer.py     # è®­ç»ƒå™¨
â”‚   â”‚   â”œâ”€â”€ train_interleaved_sft.py    # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ scripts/                         # å¯åŠ¨è„šæœ¬
â”‚   â”‚   â””â”€â”€ train_interleaved.sh        # è®­ç»ƒå¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ test_list/inference/             # æ¨ç†æµ‹è¯•
â”‚   â”‚   â””â”€â”€ ckpt_inference_by_hand.py   # æ¨ç†æµ‹è¯•è„šæœ¬ï¼ˆé‡è¦ï¼ï¼‰
â”‚   â”œâ”€â”€ setup.py                         # åŒ…é…ç½®
â”‚   â””â”€â”€ requirements.txt                 # ä¾èµ–åˆ—è¡¨
â”‚
â”œâ”€â”€ data/                                # æ•°æ®ç›®å½•ï¼ˆéœ€åˆ›å»ºï¼‰
â”‚   â””â”€â”€ train.parquet                    # è®­ç»ƒæ•°æ®
â”‚
â”œâ”€â”€ checkpoints/                         # æ£€æŸ¥ç‚¹ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
â”‚   â””â”€â”€ my_exp/
â”‚       â”œâ”€â”€ global_step_1000/
â”‚       â”‚   â””â”€â”€ huggingface/             # HuggingFace æ ¼å¼æ¨¡å‹
â”‚       â””â”€â”€ global_step_2000/
â”‚
â””â”€â”€ log/                                 # æ—¥å¿—ç›®å½•ï¼ˆå¯é€‰ï¼‰
```

---

## âš™ï¸ æ ¸å¿ƒæ¦‚å¿µ

### Interleaved SFT è®­ç»ƒæµç¨‹

ä¸ä¼ ç»Ÿ SFT ä¸åŒï¼ŒInterleaved SFT å°† response åºåˆ—åˆ†æˆå¤šä¸ª blockï¼Œå¹¶é€šè¿‡ mask token è¿›è¡Œå¹¶è¡Œé¢„æµ‹ï¼š

```
åŸå§‹åºåˆ—:  [P0, P1, P2] [R0, R1, R2, R3, R4, R5, R6, R7]
           â†‘ Prompt      â†‘ Response (block_size=4)

äº¤é”™æ ¼å¼:  [P0, P1, P2] [M, M, M] [R0, R1, R2, R3] [M, M, M] [R4, R5, R6, R7]
                         â†‘ Masks    â†‘ Block 0       â†‘ Masks    â†‘ Block 1

é¢„æµ‹ç›®æ ‡:
  - P2 é¢„æµ‹ â†’ R0
  - M0 é¢„æµ‹ â†’ R1 (å¹¶è¡Œ)
  - M1 é¢„æµ‹ â†’ R2 (å¹¶è¡Œ)
  - M2 é¢„æµ‹ â†’ R3 (å¹¶è¡Œ)
  - R0-R3 è‡ªå›å½’é¢„æµ‹åç»­ token
```

### ä¸æ ‡å‡† SFT çš„åŒºåˆ«

| ç‰¹æ€§ | æ ‡å‡† SFT | Interleaved SFT |
|------|----------|-----------------|
| è®­ç»ƒæ–¹å¼ | å•è½®è‡ªå›å½’ | Block-wise å¹¶è¡Œé¢„æµ‹ |
| æ¨ç†é€Ÿåº¦ | é¡ºåºç”Ÿæˆ | å¯å¹¶è¡Œç”Ÿæˆå¤šä¸ª token |
| è®­ç»ƒå¤æ‚åº¦ | ç®€å• | éœ€è¦ FlexAttention æ”¯æŒ |

---

## ğŸ”§ å¸¸ç”¨é…ç½®è°ƒæ•´

### æ˜¾å­˜ä¼˜åŒ–

```yaml
# æ–¹æ³• 1: å‡å° batch size
data:
  micro_batch_size_per_gpu: 1  # é»˜è®¤ 2

# æ–¹æ³• 2: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model:
  enable_gradient_checkpointing: true

# æ–¹æ³• 3: å‡å° block size
data:
  block_size: 2  # é»˜è®¤ 4ï¼ˆä¼šå½±å“å¹¶è¡Œæ•ˆç‡ï¼‰
```

### è®­ç»ƒç­–ç•¥è°ƒæ•´

```yaml
data:
  block_size: 4                         # Block å¤§å°ï¼ˆå½±å“å¹¶è¡Œæ•ˆç‡ï¼‰
  max_length: 2048                      # åºåˆ—æœ€å¤§é•¿åº¦

optim:
  lr: 1e-5                              # å­¦ä¹ ç‡
  warmup_steps_ratio: 0.05              # Warmup æ¯”ä¾‹
  clip_grad: 1.0                        # æ¢¯åº¦è£å‰ª
  gradient_accumulation_steps: 64       # æ¢¯åº¦ç´¯ç§¯

trainer:
  save_checkpoint_steps: 1000           # ä¿å­˜é¢‘ç‡
  max_ckpt_to_keep: 3                   # ä¿ç•™æ£€æŸ¥ç‚¹æ•°é‡
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æŠ¥é”™ `No module named dllm_reasoning`

**åŸå› **: æ²¡æœ‰å®‰è£…åŒ…

**è§£å†³**:
```bash
cd dllm_reasoning
pip install -e .
```

### Q2: è®­ç»ƒæ—¶æŠ¥é”™ `No module named verl`

**åŸå› **: VERL æœªå®‰è£…

**è§£å†³**:
```bash
git clone https://github.com/volcengine/verl
cd verl
pip install -e .
```

### Q3: æ•°æ®ä¸­çš„ `<think>` æ ‡ç­¾å¦‚ä½•å¤„ç†ï¼Ÿ

**ç­”æ¡ˆ**: æ•°æ®é›†ä¼šè‡ªåŠ¨å¤„ç†ï¼š

- å¦‚æœ `target` åˆ—çš„å†…å®¹ä»¥ `<think>` å¼€å¤´ï¼Œä¼šè‡ªåŠ¨å»æ‰å¼€å¤´çš„ `<think>`
- ä¿ç•™åç»­çš„æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬ `</think>` å’Œå…¶ä»–å†…å®¹ï¼‰
- è¯¦è§ä»£ç ï¼š[interleaved_sft_dataset.py:506-509](dllm_reasoning/trainer/interleaved_sft_dataset.py#L506-L509)

### Q4: å¦‚ä½•ç›‘æ§è®­ç»ƒï¼Ÿ

è®­ç»ƒæ—¥å¿—è¾“å‡ºä½ç½®ï¼š
- **ç»ˆç«¯è¾“å‡º**: å®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
- **WandB**: åœ¨é…ç½®ä¸­å¯ç”¨ `trainer.logger: ['console', 'wandb']`

### Q5: æ–­ç‚¹ç»­è®­

è®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤ï¼š

```bash
# ç›´æ¥è¿è¡Œï¼Œä¼šè‡ªåŠ¨æ¢å¤
bash dllm_reasoning/scripts/train_interleaved.sh 4 ./checkpoints/my_exp
```

é…ç½®æ–‡ä»¶ä¸­çš„æ¢å¤è®¾ç½®ï¼š
```yaml
trainer:
  resume_mode: auto  # auto: è‡ªåŠ¨æ¢å¤æœ€æ–°æ£€æŸ¥ç‚¹ | disable: ä»å¤´è®­ç»ƒ
```

---

## ğŸ“Š è®­ç»ƒç›‘æ§æŒ‡æ ‡

è®­ç»ƒæ—¶è¾“å‡ºçš„å…³é”®æŒ‡æ ‡ï¼š

- `loss`: æ€»æŸå¤±
- `grad_norm`: æ¢¯åº¦èŒƒæ•°
- `lr`: å½“å‰å­¦ä¹ ç‡
- `tokens_per_sec`: è®­ç»ƒååé‡

---

## ğŸ¯ è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰æ•°æ®åˆ—å

å¦‚æœä½ çš„æ•°æ®åˆ—åä¸æ˜¯ `prompt` å’Œ `target`ï¼š

```yaml
data:
  prompt_key: instruction  # ä½ çš„ prompt åˆ—å
  response_key: output     # ä½ çš„ response åˆ—å
```

### ä½¿ç”¨ Tensor Parallel

å¯¹äºå¤§æ¨¡å‹ï¼ˆå¦‚ 70Bï¼‰ï¼Œå¯ä»¥å¯ç”¨ Tensor Parallelï¼š

```yaml
model:
  tensor_parallel_size: 4  # ä½¿ç”¨ 4-way TP
```

### å¤šèŠ‚ç‚¹è®­ç»ƒ

ä¿®æ”¹è®­ç»ƒè„šæœ¬çš„ `torchrun` å‚æ•°ï¼š

```bash
# åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œ
torchrun --nnodes=2 --nproc_per_node=8 \
    --node_rank=$NODE_RANK \
    --master_addr=$MASTER_ADDR \
    --master_port=29500 \
    -m dllm_reasoning.train_interleaved_sft \
    ...
```

---

## ğŸ“ å¼•ç”¨

## ğŸ“„ è®¸å¯è¯

Apache License 2.0
