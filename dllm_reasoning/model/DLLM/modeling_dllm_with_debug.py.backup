# This file is modified based on https://github.com/huggingface/transformers/blob/v4.52.4/src/transformers/models/qwen3/modeling_qwen3.py.
#
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/qwen3/modular_qwen3.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Callable, Optional, Tuple, Union, List

import torch
from torch import nn
from einops import rearrange

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.modeling_attn_mask_utils import AttentionMaskConverter
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    QuestionAnsweringModelOutput,
    SequenceClassifierOutputWithPast,
    TokenClassifierOutput,
)
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging

# LossKwargs may not exist in older transformers versions, provide fallback
try:
    from transformers.utils import LossKwargs
except ImportError:
    from typing import TypedDict
    class LossKwargs(TypedDict, total=False):
        pass

# Use relative imports for local modules (required for HuggingFace dynamic loading)
from .configuration_dllm import DLLMConfig
from .fused_linear_diffusion_cross_entropy import FusedLinearDiffusionCrossEntropyLoss

import torch.nn.functional as F

# Optional flash_attn imports - model works without flash_attn installed
try:
    from flash_attn.ops.triton.layer_norm import rms_norm_fn as flash_rms_norm
    from flash_attn import flash_attn_func, flash_attn_varlen_func
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input
    FLASH_ATTN_AVAILABLE = True
except ImportError:
    flash_rms_norm = None
    flash_attn_func = None
    flash_attn_varlen_func = None
    FLASH_ATTN_AVAILABLE = False

try:
    from liger_kernel.ops.swiglu import LigerSiLUMulFunction  # noqa: F401
    liger_kernel_is_available = True
except ImportError:
    liger_kernel_is_available = False


if is_torch_flex_attn_available():
    from torch.nn.attention.flex_attention import BlockMask, create_block_mask, flex_attention
    from transformers.integrations.flex_attention import make_flex_block_causal_mask


logger = logging.get_logger(__name__)


def modify_padded_position_ids_2d(position_ids: torch.LongTensor) -> torch.LongTensor:
    """
    ä½¿ç”¨å®Œå…¨å‘é‡åŒ–çš„ PyTorch æ“ä½œä¿®æ”¹ä¸€ä¸ª batch çš„ packed position_idsã€‚
    è¿™ä¸ªå‡½æ•°å‡è®¾è¾“å…¥æ˜¯ä¸€ä¸ª 2D Tensorï¼Œå½¢çŠ¶ä¸º (batch_size, sequence_length)ã€‚
    å®ƒä¼šç‹¬ç«‹åœ°å¤„ç† batch ä¸­çš„æ¯ä¸€è¡Œã€‚

    Args:
        position_ids: äºŒç»´ PyTorch Tensor, shape (batch_size, sequence_length).

    Returns:
        ä¿®æ”¹åçš„ position_ids Tensor, shape (batch_size, sequence_length).
    """
    if position_ids.dim() != 2:
        raise ValueError(f"Input tensor must be 2D, but got {position_ids.dim()} dimensions.")
        
    batch_size, seq_len = position_ids.shape
    device = position_ids.device

    col_indices = torch.arange(seq_len, device=device, dtype=position_ids.dtype).expand(batch_size, -1)
    mask = (position_ids != 0)

    masked_indices = col_indices * mask
    last_nonzero_idx = torch.max(masked_indices, dim=1).values
    has_nonzero = torch.any(mask, dim=1)
    pad_start_idx = torch.where(has_nonzero, last_nonzero_idx + 1, torch.tensor(0, device=device, dtype=position_ids.dtype))

    padding_mask = col_indices >= pad_start_idx.unsqueeze(1)
    new_pad_values = col_indices - pad_start_idx.unsqueeze(1)
    position_ids = torch.where(padding_mask, new_pad_values, position_ids)

    return position_ids


def calculate_token_nums(position_ids: torch.Tensor):
    """
    ä½¿ç”¨ PyTorch é«˜æ•ˆè®¡ç®—ä¸€ä¸ªæ‰¹æ¬¡ä¸­æ¯ä¸ªæ‰“åŒ…åºåˆ—çš„é•¿åº¦ã€‚

    Args:
        position_ids (torch.Tensor): ä¸€ä¸ª 2D Tensorï¼Œå½¢çŠ¶ä¸º (batch_size, sequence_length)ã€‚
                                     ä¾‹å¦‚ï¼štensor([[0,1,2,3,4,0,1,2,3,4,5,0,1,2,3,0,0,0]])
    Returns:
        list[list[int]]: ä¸€ä¸ªåµŒå¥—åˆ—è¡¨ï¼ŒåŒ…å«æ¯ä¸ªæ‰¹æ¬¡é¡¹ä¸­å„ä¸ªåºåˆ—çš„é•¿åº¦ã€‚
                         ä¾‹å¦‚ï¼š[[5, 6, 4, 1, 1, 1]]
    """
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸º 2D Tensor
    if position_ids.dim() != 2:
        raise ValueError(f"è¾“å…¥å¿…é¡»æ˜¯ 2D Tensorï¼Œä½†å¾—åˆ°äº† {position_ids.dim()}D")

    all_lengths = []
    
    # æˆ‘ä»¬æŒ‰æ‰¹æ¬¡é€è¡Œå¤„ç†ã€‚å› ä¸ºæ¯è¡Œçš„åºåˆ—é•¿åº¦æ•°é‡ä¸åŒï¼ˆraggedï¼‰ï¼Œ
    # æ‰€ä»¥ Python å¾ªç¯åœ¨æ‰¹æ¬¡ç»´åº¦ä¸Šæ˜¯æœ€é«˜æ•ˆä¸”æœ€æ¸…æ™°çš„å†™æ³•ã€‚
    # å¾ªç¯å†…éƒ¨çš„æ“ä½œæ˜¯å®Œå…¨å‘é‡åŒ–çš„ã€‚
    for pids_row in position_ids:
        # è·å–å½“å‰è¡Œçš„æ€»é•¿åº¦
        seq_len = pids_row.shape[0]
        
        # 1. æ‰¾åˆ°æ‰€æœ‰å€¼ä¸º 0 çš„å…ƒç´ çš„ç´¢å¼•
        # pids_row == 0 ä¼šè¿”å›ä¸€ä¸ªå¸ƒå°” Tensor: [True, False, ..., True, ...]
        # torch.nonzero ä¼šè¿”å›è¿™äº› True å€¼çš„ç´¢å¼•
        # .flatten() å°†å…¶ä» (N, 1) å½¢çŠ¶çš„ Tensor å˜ä¸º (N,) å½¢çŠ¶
        zero_indices = torch.nonzero(pids_row == 0).flatten()
        
        # 2. å°†åºåˆ—çš„æ€»é•¿åº¦ä½œä¸ºä¸€ä¸ªé¢å¤–çš„åˆ‡åˆ†ç‚¹æ·»åŠ åˆ°æœ«å°¾
        # è¿™å¯¹äºè®¡ç®—æœ€åä¸€ä¸ªåºåˆ—çš„é•¿åº¦è‡³å…³é‡è¦
        # æ³¨æ„ï¼šè¦ç¡®ä¿æ–°åˆ›å»ºçš„ tensor å’ŒåŸå§‹ tensor åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Š (cpu/cuda)
        split_points = torch.cat([
            zero_indices,
            torch.tensor([seq_len], device=pids_row.device, dtype=zero_indices.dtype)
        ])
        
        # 3. è®¡ç®—ç›¸é‚»åˆ‡åˆ†ç‚¹ä¹‹é—´çš„å·®å€¼ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„é•¿åº¦
        # torch.diff([a, b, c, d]) ä¼šè¿”å› [b-a, c-b, d-c]
        lengths = torch.diff(split_points)

        all_lengths.append(lengths)

    return all_lengths


def forward_add_noise_packed(
    inputs_ids: torch.Tensor,
    num_tokens_list: List[torch.Tensor],
    prompt_mask: torch.Tensor,
    mask_id: int,
    eps: float = 1e-3,
    max_tries: int = 10,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    ä¸ºä¸€æ‰¹æ‰“åŒ…ï¼ˆpackedï¼‰åºåˆ—çš„ token ID æ·»åŠ å™ªå£°ã€‚

    æ­¤å‡½æ•°ä¿ç•™äº†ä¸ºæ¯ä¸ªé€»è¾‘æ ·æœ¬ï¼ˆåœ¨æ¯ä¸ªæ‰¹æ¬¡é¡¹å†…æ‹¼æ¥ï¼‰ç”Ÿæˆç‹¬ç«‹éšæœºå™ªå£°ç‡çš„é€»è¾‘ã€‚
    å®ƒä¼šéšæœºå°†ä¸€éƒ¨åˆ† token çš„ ID æ›¿æ¢ä¸º mask_idã€‚
    è¿™ä¸ªè¿‡ç¨‹ä¼šé¿å¼€è¢« prompt_mask æ ‡è®°çš„ä½ç½®ã€‚

    Args:
        inputs_ids (torch.Tensor): 
            è¾“å…¥çš„ token ID å¼ é‡ï¼Œå½¢çŠ¶ä¸º (bsz, total_tokens)ã€‚
        num_tokens_list (List[torch.Tensor]): 
            ä¸€ä¸ªå¼ é‡åˆ—è¡¨ï¼Œé•¿åº¦ä¸º bszã€‚åˆ—è¡¨ä¸­çš„æ¯ä¸ªå¼ é‡è®°å½•äº†å¯¹åº”æ‰¹æ¬¡é¡¹ä¸­
            æ¯ä¸ªé€»è¾‘æ ·æœ¬çš„é•¿åº¦ã€‚ä¾‹å¦‚: [tensor([len1, len2]), tensor([len3, len4, len5])].
        prompt_mask (torch.Tensor): 
            å¸ƒå°”å‹å¼ é‡ï¼Œå½¢çŠ¶ä¸º (bsz, total_tokens)ï¼Œå€¼ä¸º True çš„ä½ç½®è¡¨ç¤ºæ˜¯ promptï¼Œ
            ä¸åº”æ·»åŠ å™ªå£°ã€‚
        mask_id (int): 
            ç”¨äºæ›¿æ¢çš„ mask token çš„ IDã€‚
        eps (float): 
            å¾®å°å€¼ï¼Œç”¨äºé˜²æ­¢å™ªå£°ç‡ t æ°å¥½ä¸º 0ï¼Œç¡®ä¿ p_mask > 0ã€‚
        max_tries (int): 
            ä¸ºç¡®ä¿è‡³å°‘ä¸€ä¸ªé prompt token è¢« maskï¼Œå¯¹æ¯ä¸ªæ‰¹æ¬¡é¡¹å°è¯•çš„æœ€å¤§æ¬¡æ•°ã€‚

    Returns:
        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        - noisy_input_ids (torch.Tensor): 
            æ·»åŠ å™ªå£°åçš„ token ID å¼ é‡ï¼Œå½¢çŠ¶ä¸º (bsz, total_tokens)ã€‚
        - final_masked_indices (torch.Tensor): 
            å¸ƒå°”å‹å¼ é‡ï¼Œæ ‡è®°äº†å“ªäº›ä½ç½®è¢«å®é™… mask äº†ï¼Œå½¢çŠ¶ä¸º (bsz, total_tokens)ã€‚
        - p_masks (torch.Tensor): 
            ä¸€ä¸ªä¸€ç»´å¼ é‡ï¼ŒåŒ…å«äº†è¢« mask çš„ token å¯¹åº”çš„å®é™…å™ªå£°ç‡ã€‚
    """
    # 1. éªŒè¯å’Œè·å–å½¢çŠ¶
    bsz, total_tokens = inputs_ids.shape
    device = inputs_ids.device

    # æ£€æŸ¥è¾“å…¥çš„ä¸€è‡´æ€§
    assert len(num_tokens_list) == bsz, f"num_tokens_list çš„é•¿åº¦ ({len(num_tokens_list)}) å¿…é¡»ç­‰äº bsz ({bsz})"
    assert prompt_mask.shape == (bsz, total_tokens), f"prompt_mask å½¢çŠ¶ä¸åŒ¹é…, æœŸæœ› {(bsz, total_tokens)}, å¾—åˆ° {prompt_mask.shape}"

    # å‡†å¤‡ç»“æœå®¹å™¨
    noisy_ids_list = []
    final_masked_indices_list = []
    p_masks_per_token_list = []

    # 2. åœ¨æ‰¹æ¬¡ç»´åº¦ä¸Šè¿­ä»£
    # è¿™æ˜¯å¤„ç†ä¸åŒæ‰“åŒ…ç»“æ„æœ€ç›´æ¥æœ‰æ•ˆçš„æ–¹æ³•
    for i in range(bsz):
        # æå–å½“å‰æ‰¹æ¬¡é¡¹çš„æ•°æ®
        current_ids = inputs_ids[i:i+1] # shape: (1, total_tokens)
        current_num_tokens = num_tokens_list[i]
        current_prompt_mask = prompt_mask[i:i+1] # shape: (1, total_tokens)
        
        num_samples_in_item = len(current_num_tokens)
        # éªŒè¯å½“å‰æ‰¹æ¬¡é¡¹çš„ token æ€»æ•°æ˜¯å¦åŒ¹é…
        assert total_tokens == torch.sum(current_num_tokens), \
            f"æ‰¹æ¬¡é¡¹ {i} çš„ num_tokens ä¹‹å’Œ ({torch.sum(current_num_tokens)}) ä¸ total_tokens ({total_tokens}) ä¸åŒ¹é…"

        eligible_for_masking = ~current_prompt_mask

        # å¦‚æœæ²¡æœ‰ä»»ä½• token å¯ä»¥è¢« maskï¼Œç›´æ¥ä½¿ç”¨åŸå§‹è¾“å…¥ï¼Œå¹¶è®¾ç½® p_mask ä¸º eps
        if not eligible_for_masking.any():
            noisy_ids_list.append(current_ids)
            final_masked_indices_list.append(torch.zeros_like(current_prompt_mask, dtype=torch.bool))
            # p_mask_per_token çš„å½¢çŠ¶åº”ä¸º (1, total_tokens) ä»¥ä¾¿åç»­æ‹¼æ¥
            p_masks_per_token_list.append(torch.full((1, total_tokens), eps, device=device, dtype=torch.float))
            continue

        # --- å°è¯•ç”Ÿæˆ maskï¼Œç¡®ä¿è‡³å°‘ mask ä¸€ä¸ª token ---
        final_masked_indices_item = torch.zeros_like(current_prompt_mask, dtype=torch.bool)
        p_mask_per_token = None
        
        for _ in range(max_tries):
            # ä¸ºæ¯ä¸ªé€»è¾‘æ ·æœ¬ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„å™ªå£°ç‡ t
            t = torch.rand(num_samples_in_item, device=device)
            p_mask_per_sample = (1 - eps) * t + eps

            # å°†æ¯ä¸ªæ ·æœ¬çš„å™ªå£°ç‡æ‰©å±•åˆ°å…¶æ‰€æœ‰ token ä¸Š
            p_mask_per_token_1d = torch.repeat_interleave(p_mask_per_sample, current_num_tokens)
            p_mask_per_token = p_mask_per_token_1d.unsqueeze(0) # shape: (1, total_tokens)

            # æ ¹æ®å™ªå£°ç‡ç”Ÿæˆéšæœº mask
            masked_indices = torch.rand_like(p_mask_per_token) < p_mask_per_token
            # åº”ç”¨ prompt maskï¼Œç¡®ä¿ prompt ä¸è¢« mask
            final_masked_indices_item = masked_indices & eligible_for_masking

            # å¦‚æœæˆåŠŸ mask äº†è‡³å°‘ä¸€ä¸ª tokenï¼Œåˆ™è·³å‡ºå°è¯•å¾ªç¯
            if final_masked_indices_item.any():
                break
        
        # å¦‚æœ max_tries ä¹‹åä»ç„¶æ²¡æœ‰ mask ä»»ä½• token (æå°æ¦‚ç‡)ï¼Œå°±å¼ºåˆ¶ mask ä¸€ä¸ªå¯ mask çš„ token
        if not final_masked_indices_item.any():
            eligible_indices = torch.nonzero(eligible_for_masking.squeeze(0), as_tuple=True)[0]
            if len(eligible_indices) > 0:
                # éšæœºé€‰æ‹©ä¸€ä¸ªå¯ mask çš„ä½ç½®
                random_choice = torch.randint(0, len(eligible_indices), (1,)).item()
                force_mask_idx = eligible_indices[random_choice]
                final_masked_indices_item[0, force_mask_idx] = True


        # --- æ ¹æ®æœ€ç»ˆçš„ mask ç”Ÿæˆå¸¦å™ªå£°çš„ IDs ---
        noisy_ids_item = torch.where(
            final_masked_indices_item,
            mask_id,
            current_ids
        )
        
        # ä¿å­˜è¿™ä¸ªæ‰¹æ¬¡é¡¹çš„ç»“æœ
        noisy_ids_list.append(noisy_ids_item)
        final_masked_indices_list.append(final_masked_indices_item)
        p_masks_per_token_list.append(p_mask_per_token)

    # 3. å°†åˆ—è¡¨ä¸­çš„ç»“æœå †å æˆæœ€ç»ˆçš„æ‰¹å¤„ç†å¼ é‡
    noisy_input_ids = torch.cat(noisy_ids_list, dim=0)
    final_masked_indices = torch.cat(final_masked_indices_list, dim=0)
    p_mask_full = torch.cat(p_masks_per_token_list, dim=0)
    
    # 4. æå–è¢« mask ä½ç½®å¯¹åº”çš„å™ªå£°ç‡
    p_masks = p_mask_full[final_masked_indices]
    
    return noisy_input_ids, final_masked_indices, p_masks


def block_diff_mask(b, h, q_idx, kv_idx, block_size=None, n=None):
    """
    Constructs the specialized block diffusion attention mask for training
    composed of three masks:
    - **Block Diagonal Mask (M_BD)**: Self-attention within noised blocks
    - **Offset Block Causal Mask (M_OBC)**: Cross-attention for conditional context
    - **Block Causal Mask (M_BC)**: Attention to update x0

    Args:
        b, h: Batch and head indices (ignored for mask logic).
        q_idx, kv_idx: Query and Key indices.
        seq_len: Total sequence length.
        block_size: Defines the block structure.

    Returns:
        A boolean attention mask.
    """

    # Indicate whether token belongs to xt or x0
    x0_flag_q = q_idx >= n
    x0_flag_kv = kv_idx >= n

    # Compute block indices
    block_q = torch.where(
        x0_flag_q == 1, (q_idx - n) // block_size, q_idx // block_size
    )
    block_kv = torch.where(
        x0_flag_kv == 1, (kv_idx - n) // block_size, kv_idx // block_size
    )

    # **1. Block Diagonal Mask (M_BD) **
    block_diagonal = (block_q == block_kv) & (x0_flag_q == x0_flag_kv)

    # **2. Offset Block-Causal Mask (M_OBC) **
    offset_block_causal = (block_q > block_kv) & (
        x0_flag_kv == 1) & (x0_flag_q == 0)

    # **3. Block-Causal Mask (M_BC) **
    block_causal = (block_q >= block_kv) & (x0_flag_kv == 1) & (x0_flag_q == 1)

    # **4. Combine Masks **
    return block_diagonal | offset_block_causal | block_causal


def block_attn_mask(num_tokens, block_size, device):
    masks = []
    for i in range(len(num_tokens)):
        cur_masks = []
        for num in num_tokens[i]:
            # å…¨éƒ¨è¿”å› n*n è€Œé 2n*2n
            single_mask = block_diff_mask(
                b=None,
                h=None,
                q_idx=torch.arange(num * 2, device=device)[:, None],
                kv_idx=torch.arange(num * 2, device=device)[None, :],
                block_size=block_size,
                n=num,
            )
            cur_masks.append(single_mask)
        masks.append(torch.block_diag(*cur_masks))
    masks = torch.stack(masks, dim=0)
    return masks


@torch.compile(fullgraph=True, mode="max-autotune-no-cudagraphs")
def fused_flex_attention(query, key, value, attention_mask, **kwargs):
    return flex_attention(query, key, value, block_mask=attention_mask, **kwargs)


@use_kernel_forward_from_hub("RMSNorm")
class DLLMRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        DLLMRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        if flash_rms_norm is not None:
            return flash_rms_norm(
                hidden_states, weight=self.weight, bias=None, eps=self.variance_epsilon)
        else:
            # Fallback when flash_attn is not available
            input_dtype = hidden_states.dtype
            hidden_states = hidden_states.to(torch.float32)
            variance = hidden_states.pow(2).mean(-1, keepdim=True)
            hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class DLLMMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(
            self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(
            self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(
            self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        if liger_kernel_is_available:
            return self.down_proj(LigerSiLUMulFunction.apply(self.gate_proj(x), self.up_proj(x)))
        else:
            down_proj = self.down_proj(self.act_fn(
                self.gate_proj(x)) * self.up_proj(x))
            return down_proj


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs,
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(
        attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(
        attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class DLLMAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: DLLMConfig, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(
            config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.hidden_size = config.hidden_size
        self.num_attention_heads = config.num_attention_heads
        self.num_key_value_heads = config.num_key_value_heads

        # Qwen2-style attention: q/k/v with bias=True, o_proj with bias=False
        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)
        self.sliding_window = config.sliding_window
        if not (
            self.config.use_sliding_window
            and getattr(self.config, "sliding_window", None) is not None
            and self.layer_idx >= self.config.max_window_layers
        ):
            self.sliding_window = None

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        input_shape = hidden_states.shape[:-1]
        bsz, q_len = input_shape
        hidden_shape = (*input_shape, -1, self.head_dim)

        # DEBUG: åªåœ¨Layer 0æ‰“å°
        if self.layer_idx == 0 and self.training:
            print(f"\n[Layer 0 DEBUG] seq_len={q_len}")
            # å‡è®¾Mâ‚åœ¨ä½ç½®277
            m1_pos = 277 if q_len > 277 else q_len - 1
            print(f"[Layer 0 DEBUG] hidden_states[0, {m1_pos}, :5] = {hidden_states[0, m1_pos, :5]}")

        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        # DEBUG: æ‰“å°K/VæŠ•å½±åçš„ç»“æœ(åœ¨RoPEä¹‹å‰)
        if self.layer_idx == 0 and self.training:
            m1_pos = 277 if q_len > 277 else q_len - 1
            print(f"[Layer 0 DEBUG] Before RoPE:")
            print(f"  key_states[:, :, {m1_pos}, :5] = {key_states[:, :, m1_pos, :5]}")
            print(f"  value_states[:, :, {m1_pos}, :5] = {value_states[:, :, m1_pos, :5]}")

        cos, sin = position_embeddings

        # DEBUG: æ‰“å°position embeddings
        if self.layer_idx == 0 and self.training:
            m1_pos = 277 if q_len > 277 else q_len - 1
            print(f"[Layer 0 DEBUG] Position embeddings at {m1_pos}:")
            print(f"  cos[0, {m1_pos}, :5] = {cos[0, m1_pos, :5]}")
            print(f"  sin[0, {m1_pos}, :5] = {sin[0, m1_pos, :5]}")

        query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, cos, sin)

        # DEBUG: æ‰“å°RoPEä¹‹åçš„K
        if self.layer_idx == 0 and self.training:
            m1_pos = 277 if q_len > 277 else q_len - 1
            print(f"[Layer 0 DEBUG] After RoPE:")
            print(f"  key_states[:, :, {m1_pos}, :5] = {key_states[:, :, m1_pos, :5]}")

        if past_key_value is not None and kwargs.get("store_kv", False):
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            key_states, value_states = past_key_value.update(
                key_states, value_states, self.layer_idx)
        elif past_key_value is not None and not kwargs.get("store_kv", False) and len(past_key_value) > self.layer_idx:
            # only retrive, do not store kv
            past_key_states, past_value_states = past_key_value[self.layer_idx]
            key_states = torch.cat(
                [past_key_states, key_states], dim=-2)
            value_states = torch.cat(
                [past_value_states, value_states], dim=-2)

        if self.training:
            # Training mode: use FlexAttention for block diffusion

            # DEBUG: åœ¨FlexAttentionä¹‹å‰æ‰“å°Q/K/V
            if self.layer_idx == 0:
                m1_pos = 277 if q_len > 277 else q_len - 1
                print(f"\n[Layer 0 DEBUG] Before FlexAttention:")
                print(f"  query_states[:, :, {m1_pos}, :5] = {query_states[:, :, m1_pos, :5]}")
                print(f"  key_states[:, :, {m1_pos}, :5] = {key_states[:, :, m1_pos, :5]}")
                print(f"  value_states[:, :, {m1_pos}, :5] = {value_states[:, :, m1_pos, :5]}")
                print(f"  Q/K/V shapes: {query_states.shape}, {key_states.shape}, {value_states.shape}")
                print(f"  scaling: {self.scaling}")

            attn_output, attn_weights = fused_flex_attention(
                query=query_states,
                key=key_states,
                value=value_states,
                attention_mask=attention_mask,
                enable_gqa=True,
                scale=self.scaling,
                return_lse=True
            )

            # DEBUG: åœ¨FlexAttentionä¹‹åæ‰“å°è¾“å‡º
            if self.layer_idx == 0:
                m1_pos = 277 if q_len > 277 else q_len - 1
                print(f"\n[Layer 0 DEBUG] After FlexAttention:")
                print(f"  attn_output[:, :, {m1_pos}, :5] = {attn_output[:, :, m1_pos, :5]}")
                print(f"  attn_output shape: {attn_output.shape}")

            attn_weights = attn_weights.to(
                value_states.dtype) if attn_weights is not None else None
            attn_output = rearrange(attn_output, 'b h l d -> b l (h d)')

            # DEBUG: åœ¨rearrangeä¹‹åæ‰“å°
            if self.layer_idx == 0:
                m1_pos = 277 if q_len > 277 else q_len - 1
                print(f"\n[Layer 0 DEBUG] After rearrange:")
                print(f"  attn_output[0, {m1_pos}, :5] = {attn_output[0, m1_pos, :5]}")
        else:
            # Inference mode: use standard Qwen2-style attention (eager)
            attn_output, attn_weights = eager_attention_forward(
                self,
                query_states,
                key_states,
                value_states,
                attention_mask,
                dropout=0.0,
                scaling=self.scaling,
            )
            attn_output = attn_output.reshape(*input_shape, -1).contiguous()

        # DEBUG: åœ¨o_projä¹‹å‰
        if self.layer_idx == 0:
            m1_pos = 277 if q_len > 277 else q_len - 1
            print(f"\n[Layer 0 DEBUG] Before o_proj:")
            print(f"  attn_output[0, {m1_pos}, :5] = {attn_output[0, m1_pos, :5]}")

        attn_output = self.o_proj(attn_output)

        # DEBUG: åœ¨o_projä¹‹å
        if self.layer_idx == 0:
            m1_pos = 277 if q_len > 277 else q_len - 1
            print(f"\n[Layer 0 DEBUG] After o_proj (attention final output):")
            print(f"  attn_output[0, {m1_pos}, :5] = {attn_output[0, m1_pos, :5]}")

        return attn_output, attn_weights  # , attn_weights


class DLLMDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: DLLMConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.self_attn = DLLMAttention(config=config, layer_idx=layer_idx)
        self.mlp = DLLMMLP(config)
        self.input_layernorm = DLLMRMSNorm(
            config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = DLLMRMSNorm(
            config.hidden_size, eps=config.rms_norm_eps)
        if (
            config.sliding_window and config._attn_implementation != "flash_attention_2"
        ):  # diff with Llama is this warning
            logger.warning_once(
                f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
                "unexpected results may be encountered."
            )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        store_kv: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        # necessary, but kept here for BC
        position_embeddings: Optional[Tuple[torch.Tensor,
                                            torch.Tensor]] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            store_kv=store_kv,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        # DEBUG: æ‰“å°æ¯ä¸€å±‚åœ¨Mâ‚ä½ç½®çš„è¾“å‡º
        if self.self_attn.training:
            bsz, seq_len, hidden_dim = hidden_states.shape
            m1_pos = 277 if seq_len > 277 else seq_len - 1
            layer_idx = self.self_attn.layer_idx
            print(f"[Layer {layer_idx:2d} Output] hidden_states[0, {m1_pos}, :5] = {hidden_states[0, m1_pos, :5]}")

        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)

        return outputs


@auto_docstring
class DLLMPreTrainedModel(PreTrainedModel):
    config_class = DLLMConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["DLLMDecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, DLLMRMSNorm):
            module.weight.data.fill_(1.0)


class DLLMRotaryEmbedding(nn.Module):
    def __init__(self, config: DLLMConfig, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get(
                "rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(
            self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    # power user: used with advanced RoPE types (e.g. dynamic rope)
    @dynamic_rope_update
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(
            position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(
            x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @
                     position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


@auto_docstring
class DLLMModel(DLLMPreTrainedModel):
    def __init__(self, config: DLLMConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(
            config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [DLLMDecoderLayer(config, layer_idx)
             for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = DLLMRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = DLLMRotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    def _create_block_mask_from_batch(
        self,
        block_info_batch: List[List[Tuple[str, int, int]]],
        prompt_len_batch: List[int],
        seq_lens: List[int],
        max_seq_len: int,
        device: torch.device,
    ) -> 'BlockMask':
        """
        åœ¨æ¨¡å‹å†…éƒ¨åˆ›å»º FlexAttention BlockMaskï¼Œé¿å…ä»å¤–éƒ¨ä¼ å…¥å¤§tensorã€‚

        ä½¿ç”¨ SDAR çš„æ–¹æ³•ï¼šé¢„å…ˆè®¡ç®— 3D bool tensor [B, Q_LEN, KV_LEN]ï¼Œ
        ä½¿ç”¨çº¯ Tensor æ“ä½œï¼ˆæ—  Python æ§åˆ¶æµï¼‰ï¼Œç„¶åç”¨ç®€å•çš„ lambda å‡½æ•°ç´¢å¼•ã€‚

        Args:
            block_info_batch: List[List[Tuple[str, int, int]]] - block_info for each sample
                              æ¯ä¸ªå…ƒç´ æ˜¯ (seg_type, seg_idx, seg_len)
            prompt_len_batch: List[int] - prompt length for each sample
            seq_lens: List[int] - actual sequence length for each sample
            max_seq_len: int - maximum sequence length in the batch
            device: Device for BlockMask

        Returns:
            BlockMask for FlexAttention
        """
        if not is_torch_flex_attn_available():
            raise RuntimeError("FlexAttention is not available in this PyTorch version")

        batch_size = len(block_info_batch)

        # é¢„å…ˆè®¡ç®— 3D mask tensor [B, max_seq_len, max_seq_len]
        attention_masks = []

        for b_idx in range(batch_size):
            # ä¸ºæ¯ä¸ªæ ·æœ¬æ„é€  2D mask [max_seq_len, max_seq_len]
            # åˆ›å»ºç´¢å¼• grid
            q_indices = torch.arange(max_seq_len, device=device)[:, None]  # [max_seq_len, 1]
            kv_indices = torch.arange(max_seq_len, device=device)[None, :]  # [1, max_seq_len]

            # æ„å»º segment type å’Œ segment index çš„ tensor
            # seg_types: [max_seq_len] - 0=prompt, 1=real, 2=mask
            # seg_indices: [max_seq_len] - segment index
            seg_types = torch.full((max_seq_len,), -1, dtype=torch.long, device=device)
            seg_indices = torch.full((max_seq_len,), -1, dtype=torch.long, device=device)

            # å¡«å…… prompt
            prompt_len = prompt_len_batch[b_idx]
            seg_types[:prompt_len] = 0  # prompt type
            seg_indices[:prompt_len] = 0  # prompt segment index

            # å¡«å…… block_info (real/mask segments)
            # block_info æ ¼å¼: List[Tuple[str, int, int]] = [(seg_type, seg_idx, seg_len), ...]
            current_pos = prompt_len
            for seg_type, seg_idx, seg_len in block_info_batch[b_idx]:
                seg_type_id = 1 if seg_type == 'real' else 2  # 1=real, 2=mask
                seg_types[current_pos:current_pos + seg_len] = seg_type_id
                seg_indices[current_pos:current_pos + seg_len] = seg_idx
                current_pos += seg_len

            # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ„é€  mask
            q_type = seg_types[:, None]
            kv_type = seg_types[None, :]
            q_seg_idx = seg_indices[:, None]
            kv_seg_idx = seg_indices[None, :]

            # è§„åˆ™ 1: Prompt tokens
            # prompt can only see earlier prompt tokens (causal)
            prompt_mask = (q_type == 0) & (kv_type == 0) & (kv_indices <= q_indices)

            # è§„åˆ™ 2: Real tokens
            # can see: all prompt, all previous real blocks, causal within same block
            real_see_prompt = (q_type == 1) & (kv_type == 0)
            real_see_prev_real = (q_type == 1) & (kv_type == 1) & (kv_seg_idx < q_seg_idx)
            real_see_same_real = (q_type == 1) & (kv_type == 1) & (kv_seg_idx == q_seg_idx) & (kv_indices <= q_indices)

            # è§„åˆ™ 3: Mask tokens
            # can see: all prompt, all previous real blocks, causal within same mask block
            mask_see_prompt = (q_type == 2) & (kv_type == 0)
            mask_see_prev_real = (q_type == 2) & (kv_type == 1) & (kv_seg_idx < q_seg_idx)
            mask_see_same_mask = (q_type == 2) & (kv_type == 2) & (kv_seg_idx == q_seg_idx) & (kv_indices <= q_indices)

            # ç»„åˆæ‰€æœ‰è§„åˆ™
            mask_2d = (prompt_mask |
                       real_see_prompt | real_see_prev_real | real_see_same_real |
                       mask_see_prompt | mask_see_prev_real | mask_see_same_mask)

            # è¾¹ç•Œæ£€æŸ¥ï¼šè¶…è¿‡å®é™…åºåˆ—é•¿åº¦çš„éƒ¨åˆ†è®¾ä¸º False
            valid_mask = (q_indices < seq_lens[b_idx]) & (kv_indices < seq_lens[b_idx])
            mask_2d = mask_2d & valid_mask

            attention_masks.append(mask_2d)

        # Stack æˆ [B, max_seq_len, max_seq_len]
        attention_mask = torch.stack(attention_masks, dim=0)

        # ä½¿ç”¨ç®€å•çš„ lambda ç´¢å¼•ï¼ˆSDAR æ–¹æ³•ï¼‰
        # æ³¨æ„ï¼šéœ€è¦ä½¿ç”¨ clamp é˜²æ­¢ FlexAttention æŸ¥è¯¢è¶Šç•Œç´¢å¼•
        def safe_mask_lookup(b, h, q_idx, kv_idx):
            # FlexAttention åœ¨ block-wise å¤„ç†æ—¶å¯èƒ½æŸ¥è¯¢è¾¹ç•Œå¤–çš„ç´¢å¼•
            # ä½¿ç”¨ clamp ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            q_safe = torch.clamp(q_idx, 0, max_seq_len - 1)
            kv_safe = torch.clamp(kv_idx, 0, max_seq_len - 1)
            return attention_mask[b, q_safe, kv_safe]

        block_mask = create_block_mask(
            safe_mask_lookup,
            B=batch_size,
            H=None,  # Broadcast across all heads
            Q_LEN=max_seq_len,
            KV_LEN=max_seq_len,
            device=device,
        )

        return block_mask

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        store_kv: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        # Interleaved Training å‚æ•°
        block_info: Optional[List[List[Tuple[str, int, int]]]] = None,
        prompt_len: Optional[List[int]] = None,
        seq_lens: Optional[List[int]] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> BaseModelOutputWithPast:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError(
                "You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache
        if not isinstance(past_key_values, (type(None), Cache)):
            raise ValueError(
                "The `past_key_values` should be either a `Cache` object or `None`.")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache()

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length(
            ) if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # For inference, create proper causal mask; for training, use custom FlexAttention mask
        if not self.training:
            # Use standard Qwen2-style causal mask for inference
            causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(
                attention_mask,
                sequence_length=inputs_embeds.shape[1],
                target_length=inputs_embeds.shape[1],
                dtype=inputs_embeds.dtype,
                cache_position=cache_position,
                batch_size=inputs_embeds.shape[0],
                config=self.config,
                past_key_values=past_key_values,
            )
            attention_mask = causal_mask
        else:
            # Training mode: å¿…é¡»æä¾› block_info å‚æ•°æ¥åˆ›å»º BlockMask
            if block_info is None or prompt_len is None or seq_lens is None:
                raise ValueError(
                    "Training mode requires block_info, prompt_len, and seq_lens parameters "
                    "to create FlexAttention BlockMask. Please ensure these are passed to the model."
                )
            if not is_torch_flex_attn_available():
                raise RuntimeError(
                    "Training with block_info requires FlexAttention, "
                    "but it's not available in this PyTorch version"
                )
            # åœ¨æ¨¡å‹å†…éƒ¨åˆ›å»º BlockMaskï¼Œé¿å…ä»å¤–éƒ¨ä¼ å…¥å¤§ tensor
            max_seq_len = inputs_embeds.shape[1]
            attention_mask = self._create_block_mask_from_batch(
                block_info_batch=block_info,
                prompt_len_batch=prompt_len,
                seq_lens=seq_lens,
                max_seq_len=max_seq_len,
                device=inputs_embeds.device,
            )

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                store_kv=store_kv,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **flash_attn_kwargs,
            )

            hidden_states = layer_outputs[0]

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )

    def _update_causal_mask(
        self,
        attention_mask: Union[torch.Tensor, "BlockMask"],
        input_tensor: torch.Tensor,
        cache_position: torch.Tensor,
        past_key_values: Cache,
        output_attentions: bool = False,
    ):
        if self.config._attn_implementation == "flash_attention_2":
            if attention_mask is not None and past_key_values is not None:
                is_padding_right = attention_mask[:, -
                                                  1].sum().item() != input_tensor.size()[0]
                if is_padding_right:
                    raise ValueError(
                        "You are attempting to perform batched generation with padding_side='right'"
                        " this may lead to unexpected behaviour for Flash Attention version of Qwen3. Make sure to "
                        " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
                    )
            if attention_mask is not None and 0.0 in attention_mask:
                return attention_mask
            return None
        if self.config._attn_implementation == "flex_attention":
            if isinstance(attention_mask, torch.Tensor):
                seq_len_q, seq_len_kv = attention_mask.shape
                assert seq_len_q == seq_len_kv, f"got {attention_mask.shape=}"
                attention_mask = create_block_mask(
                    # 2d bool tensor, shape: [2*seqlen, 2*seqlen]
                    lambda b, h, q_idx, kv_idx: attention_mask[q_idx, kv_idx],
                    B=None, H=None, Q_LEN=seq_len_q, KV_LEN=seq_len_kv,
                )
            else:
                # Here we pass in flex mask computed externally
                assert isinstance(attention_mask, BlockMask)
            return attention_mask

        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in
        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail
        # to infer the attention mask.
        past_seen_tokens = past_key_values.get_seq_length(
        ) if past_key_values is not None else 0
        using_static_cache = isinstance(past_key_values, StaticCache)
        using_sliding_window_cache = isinstance(
            past_key_values, SlidingWindowCache)

        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward
        if (
            self.config._attn_implementation == "sdpa"
            and not (using_static_cache or using_sliding_window_cache)
            and not output_attentions
        ):
            if AttentionMaskConverter._ignore_causal_mask_sdpa(
                attention_mask,
                inputs_embeds=input_tensor,
                past_key_values_length=past_seen_tokens,
                sliding_window=self.config.sliding_window,
                is_training=self.training,
            ):
                return None

        dtype = input_tensor.dtype
        min_dtype = torch.finfo(dtype).min
        sequence_length = input_tensor.shape[1]
        # SlidingWindowCache or StaticCache
        if using_sliding_window_cache or using_static_cache:
            target_length = past_key_values.get_max_cache_shape()
        # DynamicCache or no cache
        else:
            target_length = (
                attention_mask.shape[-1]
                if isinstance(attention_mask, torch.Tensor)
                else past_seen_tokens + sequence_length + 1
            )

        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).
        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(
            attention_mask,
            sequence_length=sequence_length,
            target_length=target_length,
            dtype=dtype,
            cache_position=cache_position,
            batch_size=input_tensor.shape[0],
            config=self.config,
            past_key_values=past_key_values,
        )

        if (
            self.config._attn_implementation == "sdpa"
            and attention_mask is not None
            and attention_mask.device.type in ["cuda", "xpu", "npu"]
            and not output_attentions
        ):
            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when
            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.
            # Details: https://github.com/pytorch/pytorch/issues/110213
            causal_mask = AttentionMaskConverter._unmask_unattended(
                causal_mask, min_dtype)

        return causal_mask

    @staticmethod
    def _prepare_4d_causal_attention_mask_with_cache_position(
        attention_mask: torch.Tensor,
        sequence_length: int,
        target_length: int,
        dtype: torch.dtype,
        cache_position: torch.Tensor,
        batch_size: int,
        config: DLLMConfig,
        past_key_values: Cache,
    ):
        """
        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.

        Args:
            attention_mask (`torch.Tensor`):
                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.
            sequence_length (`int`):
                The sequence length being processed.
            target_length (`int`):
                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.
            dtype (`torch.dtype`):
                The dtype to use for the 4D attention mask.
            cache_position (`torch.Tensor`):
                Indices depicting the position of the input sequence tokens in the sequence.
            batch_size (`torch.Tensor`):
                Batch size.
            config (`DLLMConfig`):
                The model's configuration class
            past_key_values (`Cache`):
                The cache class that is being used currently to generate
        """
        # Handle BlockMask (from FlexAttention) separately
        if attention_mask is not None and isinstance(attention_mask, BlockMask):
            # BlockMask is already in the correct format for FlexAttention
            return attention_mask
        elif attention_mask is not None and hasattr(attention_mask, 'dim') and attention_mask.dim() == 4:
            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.
            causal_mask = attention_mask
        else:
            min_dtype = torch.finfo(dtype).min
            causal_mask = torch.full(
                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device
            )
            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(
                -1, 1
            )
            text_config = config.get_text_config()
            if getattr(text_config, "use_sliding_window", True) and text_config.sliding_window is not None:
                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also
                # the check is needed to verify is current checkpoint was trained with sliding window or not
                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:
                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (
                        cache_position.reshape(-1, 1) -
                        text_config.sliding_window
                    )
                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)
            causal_mask *= diagonal_attend_mask
            causal_mask = causal_mask[None, None,
                                      :, :].expand(batch_size, 1, -1, -1)
            if attention_mask is not None:
                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit
                if attention_mask.shape[-1] > target_length:
                    attention_mask = attention_mask[:, :target_length]
                mask_length = attention_mask.shape[-1]
                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(
                    causal_mask.device
                )
                padding_mask = padding_mask == 0
                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(
                    padding_mask, min_dtype
                )
        return causal_mask


class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs):
    ...


@auto_docstring
class DLLMForCausalLM(DLLMPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        self.model = DLLMModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(
            config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    def prepare_for_bd_training(self, inputs_ids, position_ids, prompt_mask, block_size):
        """
        Prepare inputs for Block Diffusion training.

        Args:
            inputs_ids: Input token IDs (bsz, seq_len)
            position_ids: Position IDs (bsz, seq_len)
            prompt_mask: Mask indicating prompt positions
            block_size: Block size for block diffusion (training hyperparameter)
        """
        bsz, seq_len = inputs_ids.shape
        num_tokens = calculate_token_nums(position_ids) # List[torch.Tensor]
        noisy_inputs_ids, logits_to_keep_half, p_mask = forward_add_noise_packed(
            inputs_ids=inputs_ids,
            num_tokens_list=num_tokens,
            prompt_mask=prompt_mask,
            mask_id=self.config.mask_token_id,
        )
        router_noisy_part_list = []
        for i in range(bsz):
            cur_router_noisy_part = (torch.arange(num_tokens[i].shape[0] *2) % 2 == 0).to(inputs_ids.device)
            cur_router_noisy_part = cur_router_noisy_part.repeat_interleave(num_tokens[i].repeat_interleave(2))
            router_noisy_part_list.append(cur_router_noisy_part)
        router_noisy_part = torch.stack(router_noisy_part_list, dim=0)

        # concated inputs_ids: (bzs, seq_len x 2)
        concat_inputs_ids = inputs_ids.repeat(1, 2)
        # concated logits_to_keep: (bsz, seq_len x 2)
        logits_to_keep = torch.zeros(
                    bsz, 2 * seq_len, dtype=torch.bool, device=inputs_ids.device)
        # concated position_ids: (bsz, seq_len x 2)
        concat_position_ids = torch.zeros(
                    bsz, 2 * seq_len, dtype=position_ids.dtype, device=position_ids.device)
        for i in range(bsz):
            concat_inputs_ids[i][router_noisy_part[i]] = noisy_inputs_ids[i]
            concat_inputs_ids[i][~router_noisy_part[i]] = inputs_ids[i]

            logits_to_keep[i][router_noisy_part[i]] = logits_to_keep_half[i]

            concat_position_ids[i][router_noisy_part[i]] = position_ids[i]
            concat_position_ids[i][~router_noisy_part[i]] = position_ids[i]

        # create flex_attention mask
        attention_mask = block_attn_mask(num_tokens, block_size, inputs_ids.device)
        flex_attention_mask_3d = create_block_mask(
                            lambda b, h, q_idx, kv_idx: attention_mask[b, q_idx, kv_idx],
                            B=attention_mask.size(0), H=None,
                            Q_LEN=attention_mask.size(1), KV_LEN=attention_mask.size(2),
        )

        return concat_inputs_ids, concat_position_ids, flex_attention_mask_3d, logits_to_keep_half, logits_to_keep, p_mask

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        # Interleaved Training å‚æ•°
        block_info: Optional[List[List[Tuple[str, int, int]]]] = None,
        prompt_len: Optional[List[int]] = None,
        seq_lens: Optional[List[int]] = None,
        **kwargs: Unpack[KwargsForCausalLM],
    ) -> CausalLMOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Example:

        ```python
        >>> from transformers import AutoTokenizer, DLLMForCausalLM

        >>> model = DLLMForCausalLM.from_pretrained("DiffuOpen/DLLM-1.7B-Chat")
        >>> tokenizer = AutoTokenizer.from_pretrained("DiffuOpen/DLLM-1.7B-Chat")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        # ========== æ£€æµ‹æ˜¯å¦ä½¿ç”¨ Interleaved Training æ¨¡å¼ ==========
        # Interleaved Training é€šè¿‡ FlexAttention BlockMask æä¾›é¢„å¤„ç†å¥½çš„æ•°æ®
        # Block Diffusion åœ¨æ¨¡å‹å†…éƒ¨åŠ¨æ€æ’å…¥ mask tokens
        # âš ï¸ å·²å¼ƒç”¨ï¼šæˆ‘ä»¬ç°åœ¨åœ¨ Dataset å±‚é¢é¢„å¤„ç†æ•°æ®ï¼Œä¸å†éœ€è¦æ¨¡å‹å†…éƒ¨çš„ Block Diffusion é€»è¾‘
        # try:
        #     from torch.nn.attention.flex_attention import BlockMask
        #     is_interleaved_training = isinstance(attention_mask, BlockMask)
        # except ImportError:
        #     is_interleaved_training = False

        # ========== Block Diffusion è®­ç»ƒè·¯å¾„ï¼ˆå·²æ³¨é‡Šæ‰ï¼‰==========
        # åŸæœ‰é€»è¾‘ï¼šåœ¨æ¨¡å‹å†…éƒ¨åŠ¨æ€æ’å…¥ mask tokens
        # ç°åœ¨æˆ‘ä»¬åœ¨ Dataset å±‚é¢é¢„å¤„ç†æ•°æ®æˆ [P][M][R][M][R]... æ ¼å¼ï¼Œæ›´è§£è€¦ã€æ›´çµæ´»
        # if self.training and not is_interleaved_training:
        #     assert inputs_embeds is None, "only support input_ids during training"
        #     prompt_mask = (labels == -100) if labels is not None else None
        #     position_ids = modify_padded_position_ids_2d(position_ids)
        #     concat_inputs_ids, concat_position_ids, flex_attention_mask_3d, logits_to_keep_half, logits_to_keep, p_mask = self.prepare_for_bd_training(input_ids, position_ids, prompt_mask)
        #     outputs = self.model(
        #         input_ids=concat_inputs_ids,
        #         attention_mask=flex_attention_mask_3d,
        #         position_ids=concat_position_ids,
        #         output_attentions=output_attentions,
        #         output_hidden_states=output_hidden_states,
        #         return_dict=True,
        #         cache_position=cache_position,
        #         **kwargs,
        #     )
        #     hidden_states = outputs.last_hidden_state
        #     hidden_states = hidden_states[logits_to_keep].contiguous()
        #     assert labels is not None, "Labels must be provided for training."
        #     labels = labels[logits_to_keep_half].contiguous()
        #     loss_fct = FusedLinearDiffusionCrossEntropyLoss(reduction='sum')
        #     loss = loss_fct(  # it will return (sum_loss, unreduced_loss)
        #             # conduct `view(-1, V)` inside the function
        #             x=hidden_states,
        #             target=labels,
        #             weight=self.lm_head.weight,
        #             bias=self.lm_head.bias,
        #             p_mask=p_mask,
        #         )
        #     loss = loss / labels.numel()
        #     logits = None

        if self.training:
            # ========== Interleaved Training è®­ç»ƒè·¯å¾„ï¼ˆæ–°é€»è¾‘ï¼‰==========
            # æ•°æ®å·²ç»åœ¨ Dataset å±‚é¢é¢„å¤„ç†æˆ [P][M][R][M][R]... æ ¼å¼
            # æ¨¡å‹å†…éƒ¨ä¼šæ ¹æ® block_info å‚æ•°åˆ›å»º FlexAttention BlockMask
            # ä¸éœ€è¦ prepare_for_bd_training()ï¼Œç›´æ¥å‰å‘ä¼ æ’­
            assert inputs_embeds is None, "only support input_ids during training"

            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,  # ä¼šè¢«å¿½ç•¥ï¼Œæ¨¡å‹å†…éƒ¨åˆ›å»º BlockMask
                position_ids=position_ids,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=True,
                cache_position=cache_position,
                # ä¼ é€’ Interleaved Training å‚æ•°
                block_info=block_info,
                prompt_len=prompt_len,
                seq_lens=seq_lens,
                **kwargs,
            )

            hidden_states = outputs.last_hidden_state

            # âœ… æ–¹æ¡ˆAï¼šå®Œå…¨è§£è€¦ - ä¸åœ¨æ¨¡å‹å†…éƒ¨è®¡ç®— loss
            # å°† hidden_states è¿”å›ç»™ Trainerï¼Œç”± Trainer è´Ÿè´£ï¼š
            # 1. è®¡ç®— logits = lm_head(hidden_states)
            # 2. è®¡ç®— loss = CrossEntropyLoss(logits, labels)
            # è¿™æ ·å¯ä»¥åœ¨æµ‹è¯•ä¸­éªŒè¯ logits å’Œæ ‡ç­¾å¯¹é½

            # æ³¨æ„ï¼šä¸ºäº†å…¼å®¹ HuggingFace Trainer APIï¼Œæˆ‘ä»¬è¿”å› logits è€Œä¸æ˜¯ hidden_states
            # ä½†å®é™…ä¸Šè¿™é‡Œçš„ "logits" æ˜¯ hidden_statesï¼ˆæœªç»è¿‡ lm_headï¼‰
            # Trainer ä¼šè´Ÿè´£åº”ç”¨ lm_head
            logits = self.lm_head(hidden_states)

            # DEBUG: æ‰“å°æœ€ç»ˆlogitsåœ¨Mâ‚ä½ç½®çš„å€¼
            bsz, seq_len, vocab_size = logits.shape
            m1_pos = 277 if seq_len > 277 else seq_len - 1
            print(f"\n[Final Logits] logits[0, {m1_pos}, :5] = {logits[0, m1_pos, :5]}")
            print(f"[Final Logits] Top 5 token IDs: {logits[0, m1_pos].topk(5).indices.tolist()}\n")

            loss = None
        else:
            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
            outputs: BaseModelOutputWithPast = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                cache_position=cache_position,
                **kwargs,
            )

            hidden_states = outputs.last_hidden_state
            # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
            slice_indices = slice(-logits_to_keep,
                                None) if isinstance(logits_to_keep, int) else logits_to_keep
            hidden_states = hidden_states[:, slice_indices, :].contiguous()
            fuse_linear_and_cross_entropy = self.config.fuse_cross_entropy and self.training
            if fuse_linear_and_cross_entropy:
                # When using fused_linear_ce_loss, we do not compute the whole logits on HBM
                logits = None
            else:
                logits = self.lm_head(hidden_states)

            loss = None
            if labels is not None:
                # FusedLinearCrossEntropyLoss will be implemented by monkey patch when training
                # We don't use it when inferencing
                loss_fct = nn.CrossEntropyLoss()  # nn.CE
                loss = loss_fct(
                    logits.view(-1, self.config.vocab_size), labels.view(-1))

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


__all__ = [
    "DLLMForCausalLM",
    "DLLMModel",
    "DLLMPreTrainedModel",
]
