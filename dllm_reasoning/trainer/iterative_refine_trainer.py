"""
è¿­ä»£ç²¾ç‚¼è®­ç»ƒå™¨ (Iterative Refine Trainer)

æ ¸å¿ƒæ€æƒ³ï¼š
1. å¯¹ response åŒºåŸŸåŠ å™ªï¼ˆç”¨ EOS tokenï¼‰
2. æ¨¡å‹å‰å‘ä¼ æ’­å¾—åˆ° logits
3. è®¡ç®— lossï¼ˆå¯¹åŸå§‹ response çš„ next token predictionï¼‰
4. è´ªå©ªè§£ç å¾—åˆ°ç²¾ç‚¼åçš„åºåˆ—
5. é‡å¤ 2-4 æ­¥è‹¥å¹²æ¬¡
6. èšåˆæ‰€æœ‰è½®æ¬¡çš„ loss è¿›è¡Œä¸€æ¬¡æ¢¯åº¦æ›´æ–°
"""

import os
import logging
import re
from typing import Optional

import torch
import torch.distributed
from torch import nn, optim
from torch.distributed.device_mesh import DeviceMesh
from torch.distributed.fsdp import CPUOffload, FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import MixedPrecision, ShardingStrategy, StateDictType
from torch.distributed.fsdp import FullStateDictConfig
from torch.utils.data import DataLoader, DistributedSampler, Dataset
from tqdm import tqdm
from transformers import AutoConfig, AutoModelForCausalLM, PreTrainedTokenizer
from tensordict import TensorDict

# VERL å·¥å…·å‡½æ•°
from verl.utils.fs import copy_local_path_from_hdfs
from verl.utils.fsdp_utils import (
    get_fsdp_wrap_policy,
    get_init_weight_context_manager,
    init_fn,
)
from verl.utils.torch_functional import get_cosine_schedule_with_warmup
from verl.utils.tracking import Tracking
from verl.utils import hf_tokenizer
from verl.utils.logger import log_with_rank
import verl.utils.hdfs_io as hdfs_io

# VERL checkpoint ç›¸å…³
from verl.utils.checkpoint.fsdp_checkpoint_manager import FSDPCheckpointManager
from verl.utils.checkpoint.checkpoint_manager import find_latest_ckpt_path
from omegaconf import DictConfig

# æœ¬åœ°æ¨¡å—
from dllm_reasoning.utils.noise_utils import q_sample, greedy_decode_response
from dllm_reasoning.losses import compute_loss_on_response, compute_iterative_loss

logger = logging.getLogger(__name__)
logger.setLevel(os.getenv("ITERATIVE_SFT_LOGGING_LEVEL", "INFO"))


def extract_step(path):
    """ä»æ£€æŸ¥ç‚¹è·¯å¾„ä¸­æå–æ­¥æ•°"""
    match = re.search(r"global_step_(\d+)", path)
    if match:
        return int(match.group(1))
    return None


class IterativeRefineTrainer:
    """
    è¿­ä»£ç²¾ç‚¼è®­ç»ƒå™¨

    ä¸ Dream çš„ä¸»è¦åŒºåˆ«ï¼š
    1. ä¸éœ€è¦ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼ˆä½¿ç”¨æ ‡å‡†çš„ AR æ¨¡å‹ï¼‰
    2. å¤šè½®å‰å‘ä¼ æ’­ï¼ˆs0 â†’ s1 â†’ ...ï¼‰
    3. æ¯è½®éƒ½å¯¹åŸå§‹ token è®¡ç®— loss
    4. åªåœ¨ response åŒºåŸŸæ“ä½œ
    """

    def __init__(
        self,
        config,
        device_mesh: DeviceMesh,
        tokenizer: PreTrainedTokenizer,
        train_dataset: Dataset,
        val_dataset: Optional[Dataset] = None,
    ):
        self.config = config
        self.device_mesh = device_mesh
        self.tokenizer = tokenizer
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset

        self.rank = device_mesh.get_rank()
        self.world_size = device_mesh.size()

        # ========== åˆå§‹åŒ–è°ƒè¯•æ—¥å¿—æ–‡ä»¶ ==========
        self._setup_debug_logger()

        # æ„å»ºæ•°æ®åŠ è½½å™¨
        self._build_dataloader()

        # æ„å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
        self._build_model_optimizer()

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0

        # è·å– EOS token ID
        self.eos_token_id = self.tokenizer.eos_token_id
        if self.eos_token_id is None:
            raise ValueError("tokenizer æ²¡æœ‰å®šä¹‰ eos_token_id!")

        logger.info(f"ä½¿ç”¨ EOS token ID: {self.eos_token_id} ({self.tokenizer.eos_token})")

        # è¿­ä»£é…ç½®
        self.num_iterations = config.iterative.get("num_iterations", 2)
        self.noise_min = config.iterative.get("noise_min", 0.1)
        self.noise_max = config.iterative.get("noise_max", 0.9)
        self.loss_weights = config.iterative.get("loss_weights", [1.0] * self.num_iterations)

        logger.info(f"è¿­ä»£é…ç½®: num_iterations={self.num_iterations}, "
                   f"noise_range=[{self.noise_min}, {self.noise_max}], "
                   f"loss_weights={self.loss_weights}")

        # åˆå§‹åŒ– checkpoint manager å’Œæ¢å¤è®­ç»ƒçŠ¶æ€
        self.resume_global_step = 0
        self._init_checkpoint_manager()
        self.load_checkpoint()

    def _setup_debug_logger(self):
        """è®¾ç½®è°ƒè¯•æ—¥å¿—æ–‡ä»¶"""
        if self.rank == 0:
            # åªåœ¨ rank 0 ä¸Šåˆ›å»ºè°ƒè¯•æ—¥å¿—
            # ä½¿ç”¨ Path è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼ˆæ›´æ¸…æ™°ï¼‰
            from pathlib import Path
            project_root = Path(__file__).resolve().parents[2]
            log_dir = project_root / "log"
            log_dir.mkdir(parents=True, exist_ok=True)
            debug_log_path = str(log_dir / "debug.log")

            # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler(debug_log_path, mode='w')
            file_handler.setLevel(logging.INFO)
            file_handler.setFormatter(logging.Formatter(
                '[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            ))

            # æ·»åŠ åˆ° logger
            debug_logger = logging.getLogger('DEBUG')
            debug_logger.setLevel(logging.INFO)
            debug_logger.addHandler(file_handler)
            debug_logger.propagate = False  # ä¸ä¼ æ’­åˆ°çˆ¶logger

            self.debug_logger = debug_logger
            self.debug_logger.info("=" * 80)
            self.debug_logger.info("è°ƒè¯•æ—¥å¿—åˆå§‹åŒ–å®Œæˆ")
            self.debug_logger.info(f"æ—¥å¿—æ–‡ä»¶: {debug_log_path}")
            self.debug_logger.info("=" * 80)
        else:
            self.debug_logger = None

    def _build_dataloader(self):
        """æ„å»ºæ•°æ®åŠ è½½å™¨"""
        # è®­ç»ƒé›†
        self.train_sampler = DistributedSampler(
            self.train_dataset,
            num_replicas=self.world_size,
            rank=self.rank,
            shuffle=True,
        )

        self.train_dataloader = DataLoader(
            self.train_dataset,
            batch_size=self.config.data.micro_batch_size_per_gpu,
            sampler=self.train_sampler,
            num_workers=0,
            pin_memory=True,
        )

        # éªŒè¯é›†ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.val_dataset is not None:
            self.val_sampler = DistributedSampler(
                self.val_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=False,
            )

            self.val_dataloader = DataLoader(
                self.val_dataset,
                batch_size=self.config.data.micro_batch_size_per_gpu,
                sampler=self.val_sampler,
                num_workers=0,
                pin_memory=True,
            )
        else:
            self.val_dataloader = None

        logger.info(f"æ•°æ®åŠ è½½å™¨æ„å»ºå®Œæˆ: train_size={len(self.train_dataset)}, "
                   f"val_size={len(self.val_dataset) if self.val_dataset else 0}")

    def _build_model_optimizer(self):
        """æ„å»ºæ¨¡å‹ã€FSDP åŒ…è£…ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""

        # 1. ä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨ VERL çš„æ–‡ä»¶ç³»ç»Ÿå·¥å…·ï¼‰
        logger.info(f"åŠ è½½æ¨¡å‹: {self.config.model.partial_pretrain}")
        local_model_path = copy_local_path_from_hdfs(
            src=self.config.model.partial_pretrain,
            verbose=True
        )

        # 2. åŠ è½½é…ç½®
        config = AutoConfig.from_pretrained(
            local_model_path,
            trust_remote_code=self.config.model.get("trust_remote_code", False),
        )

        # 3. Meta Tensor åˆå§‹åŒ–ï¼ˆä½¿ç”¨ VERL å·¥å…·ï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
        init_context = get_init_weight_context_manager(
            use_meta_tensor=not config.tie_word_embeddings
        )

        with init_context():
            # åŠ è½½æ ‡å‡†çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¸ä¿®æ”¹æ¶æ„ï¼ï¼‰
            self.model = AutoModelForCausalLM.from_pretrained(
                local_model_path,
                config=config,
                torch_dtype=torch.float32,  # FSDP ä¼šå¤„ç†æ··åˆç²¾åº¦
                trust_remote_code=self.config.model.get("trust_remote_code", False),
            )

        logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆ: {self.model.__class__.__name__}")

        # 4. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¯é€‰ï¼‰
        if self.config.model.get("enable_gradient_checkpointing", False):
            self.model.gradient_checkpointing_enable(
                gradient_checkpointing_kwargs={"use_reentrant": False}
            )
            logger.info("å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")

        # 5. FSDP åŒ…è£…ï¼ˆä½¿ç”¨ VERL å·¥å…·ï¼‰
        mixed_precision = MixedPrecision(
            param_dtype=torch.bfloat16,
            reduce_dtype=torch.float32,
            buffer_dtype=torch.float32,
        )

        auto_wrap_policy = get_fsdp_wrap_policy(
            self.model,
            config=self.config.model.fsdp_config.wrap_policy,
        )

        cpu_offload = None
        if self.config.model.fsdp_config.get("cpu_offload", False):
            cpu_offload = CPUOffload(offload_params=True)

        self.fsdp_model = FSDP(
            module=self.model,
            auto_wrap_policy=auto_wrap_policy,
            param_init_fn=init_fn,
            sharding_strategy=ShardingStrategy.FULL_SHARD,
            mixed_precision=mixed_precision,
            device_mesh=self.device_mesh,
            sync_module_states=True,
            device_id=torch.cuda.current_device(),
            cpu_offload=cpu_offload,
            use_orig_params=False,
        )

        logger.info("FSDP åŒ…è£…å®Œæˆ")

        # 6. ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½¿ç”¨ VERL å·¥å…·ï¼‰
        self.optimizer = optim.AdamW(
            self.fsdp_model.parameters(),
            lr=self.config.optim.lr,
            betas=self.config.optim.betas,
            weight_decay=self.config.optim.weight_decay,
        )

        # æ¢¯åº¦ç´¯ç§¯é…ç½®
        self.gradient_accumulation_steps = self.config.optim.get("gradient_accumulation_steps", 1)

        # è®¡ç®—æ€»æ­¥æ•°
        steps_per_epoch = len(self.train_dataloader)
        self.total_steps = steps_per_epoch * self.config.trainer.total_epochs  # æ€»æ•°æ®æ­¥æ•°
        # æ€»çš„ä¼˜åŒ–å™¨æ›´æ–°æ¬¡æ•° = æ•°æ®æ­¥æ•° / æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        self.total_optimizer_steps = self.total_steps // self.gradient_accumulation_steps
        num_warmup_steps = int(self.total_optimizer_steps * self.config.optim.warmup_steps_ratio)

        self.lr_scheduler = get_cosine_schedule_with_warmup(
            optimizer=self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=self.total_optimizer_steps,
        )

        logger.info(f"ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»ºå®Œæˆ: total_data_steps={self.total_steps}, "
                   f"gradient_accumulation_steps={self.gradient_accumulation_steps}, "
                   f"total_optimizer_steps={self.total_optimizer_steps}, "
                   f"warmup_steps={num_warmup_steps}, "
                   f"effective_batch_size={self.config.data.micro_batch_size_per_gpu * self.world_size * self.gradient_accumulation_steps}")

    def _init_checkpoint_manager(self):
        """åˆå§‹åŒ– checkpoint manager (å‚è€ƒ VERL çš„å®ç°)"""
        # è·å– checkpoint é…ç½®ï¼Œè®¾ç½®é»˜è®¤å€¼
        checkpoint_config = getattr(self.config.trainer, "checkpoint", {})

        # é»˜è®¤ä¿å­˜å’ŒåŠ è½½æ‰€æœ‰å†…å®¹: model, optimizer, extra (lr_scheduler + rng)
        save_contents = checkpoint_config.get("save_contents", ["model", "optimizer", "extra"])
        load_contents = checkpoint_config.get("load_contents", save_contents)

        # åˆ›å»º checkpoint é…ç½®å­—å…¸
        checkpoint_config_dict = DictConfig({
            "load_contents": load_contents,
            "save_contents": save_contents,
        })

        # åˆå§‹åŒ– checkpoint manager
        self.checkpoint_manager = FSDPCheckpointManager(
            model=self.fsdp_model,
            optimizer=self.optimizer,
            lr_scheduler=self.lr_scheduler,
            processing_class=self.tokenizer,
            checkpoint_config=checkpoint_config_dict,
        )

        log_with_rank(
            f"Checkpoint manager å·²åˆå§‹åŒ–: save_contents={save_contents}, load_contents={load_contents}",
            logger=logger,
            rank=self.rank,
            log_only_rank_0=True,
        )

    def load_checkpoint(self):
        """åŠ è½½ checkpoint (å‚è€ƒ VERL çš„å®ç°)"""
        # æ ¹æ®é…ç½®ç¡®å®šæ¢å¤è·¯å¾„
        checkpoint_path = self._determine_resume_path()

        if checkpoint_path is None:
            log_with_rank(
                "æ²¡æœ‰æ‰¾åˆ° checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ",
                logger=logger,
                rank=self.rank,
                log_only_rank_0=True,
            )
            return 0

        # ä» checkpoint è·¯å¾„ä¸­æå–æ­¥æ•°
        resume_step = extract_step(checkpoint_path)
        if resume_step is None:
            log_with_rank(
                f"è­¦å‘Š: æ— æ³•ä» {checkpoint_path} ä¸­æå–æ­¥æ•°ï¼Œä» step 0 å¼€å§‹",
                logger=logger,
                rank=self.rank,
                level=logging.WARNING,
                log_only_rank_0=True,
            )
            return 0

        self.resume_global_step = resume_step

        # ä½¿ç”¨ checkpoint manager åŠ è½½æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.checkpoint_manager.load_checkpoint(checkpoint_path)

        log_with_rank(
            f"æˆåŠŸä» {checkpoint_path} åŠ è½½ checkpoint (step {resume_step})",
            logger=logger,
            rank=self.rank,
            log_only_rank_0=True,
        )

        return resume_step

    def _determine_resume_path(self):
        """æ ¹æ®é…ç½®ç¡®å®šæ¢å¤è·¯å¾„ (å‚è€ƒ VERL çš„å®ç°)"""
        resume_mode = getattr(self.config.trainer, "resume_mode", "auto")
        resume_from_path = getattr(self.config.trainer, "resume_from_path", None)

        if resume_mode == "disable":
            return None
        elif resume_mode == "auto":
            if resume_from_path is not None:
                assert os.path.exists(resume_from_path), (
                    "resume_from_path å¿…é¡»æ˜¯ null æˆ–ä¸€ä¸ªå­˜åœ¨çš„è·¯å¾„ (å½“ resume_mode='auto' æ—¶)"
                )
                assert "global_step_" in resume_from_path, "resume_from_path å¿…é¡»åŒ…å« global_step_"
                return resume_from_path
            # å°è¯•åœ¨é»˜è®¤ç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„ checkpoint
            return self._find_latest_checkpoint()
        elif resume_mode == "resume_path":
            assert os.path.exists(resume_from_path), (
                "resume_from_path å¿…é¡»æ˜¯ä¸€ä¸ªå­˜åœ¨çš„è·¯å¾„ (å½“ resume_mode='resume_path' æ—¶)"
            )
            assert "global_step_" in resume_from_path, "resume_from_path å¿…é¡»åŒ…å« global_step_"
            return resume_from_path
        else:
            raise ValueError(f"æ— æ•ˆçš„ resume_mode: {resume_mode}ã€‚å¿…é¡»æ˜¯ 'auto', 'disable', æˆ– 'resume_path'")

    def _find_latest_checkpoint(self):
        """åœ¨é»˜è®¤ç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„ checkpoint (å‚è€ƒ VERL çš„å®ç°)"""
        checkpoint_dir = self.config.trainer.default_local_dir

        if not os.path.exists(checkpoint_dir):
            return None

        latest_checkpoint = find_latest_ckpt_path(checkpoint_dir)

        if latest_checkpoint and self.rank == 0:
            step_num = extract_step(latest_checkpoint)
            print(f"æ‰¾åˆ°æœ€æ–° checkpoint: {latest_checkpoint} (step {step_num})")

        return latest_checkpoint

    def _compute_iterative_loss(self, batch: dict, global_step: int = -1):
        """
        è®¡ç®—å¤šè½®è¿­ä»£çš„ lossï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰

        æµç¨‹ï¼š
        1. ä»åŸå§‹ batch è·å– t0 å’Œ response_mask
        2. åŠ å™ªå¾—åˆ° s0
        3. å‰å‘ä¼ æ’­ s0ï¼Œè®¡ç®— loss_s0
        4. è´ªå©ªè§£ç å¾—åˆ° s1
        5. å‰å‘ä¼ æ’­ s1ï¼Œè®¡ç®— loss_s1
        6. ... (å¯ç»§ç»­æ›´å¤šè½®)
        7. èšåˆæ‰€æœ‰ loss

        Args:
            batch: dict åŒ…å« input_ids, attention_mask, position_ids, loss_mask
            global_step: int - å…¨å±€æ­¥æ•°ï¼Œç”¨äºè°ƒè¯•æ—¥å¿—

        Returns:
            total_loss: scalar tensor
            metrics: dict - ç”¨äºæ—¥å¿—çš„æŒ‡æ ‡
        """
        # å‡†å¤‡æ•°æ®
        t0 = batch["input_ids"].cuda()  # [batch_size, seq_len] - åŸå§‹ token
        attention_mask = batch["attention_mask"].cuda()
        position_ids = batch["position_ids"].cuda()
        response_mask = batch["loss_mask"].cuda()  # [batch_size, seq_len] - 1=response, 0=instruction

        batch_size = t0.shape[0]

        # é‡‡æ ·å™ªå£°æ¯”ä¾‹ t
        t = torch.rand((batch_size,), dtype=torch.float, device=t0.device)
        t = self.noise_min + (self.noise_max - self.noise_min) * t

        # ========== é˜¶æ®µ3ï¼šåŠ å™ªå’Œè¿­ä»£è¿‡ç¨‹è°ƒè¯•ï¼ˆæ¯10æ­¥è¯¦ç»†è®°å½•ï¼‰ ==========
        should_log_detail = (global_step % 10 == 0) and self.debug_logger is not None and global_step >= 0

        if should_log_detail:
            self.debug_logger.info("\n" + "=" * 80)
            self.debug_logger.info(f"[Step {global_step}] è¿­ä»£ç²¾ç‚¼è¿‡ç¨‹è¯¦è§£")
            self.debug_logger.info("=" * 80)

            # è®°å½•åŸå§‹åºåˆ— t0
            t0_sample = t0[0].cpu().tolist()
            response_mask_sample = response_mask[0].cpu().tolist()

            # æ‰¾åˆ° response åŒºåŸŸçš„èµ·å§‹å’Œç»“æŸä½ç½®
            response_indices = [i for i, mask in enumerate(response_mask_sample) if mask == 1]
            if response_indices:
                response_start = response_indices[0]
                response_end = response_indices[-1] + 1
                response_tokens = t0_sample[response_start:response_end]
                response_text = self.tokenizer.decode(response_tokens, skip_special_tokens=False)

                self.debug_logger.info(f"\n[åŸå§‹åºåˆ— t0]")
                self.debug_logger.info(f"  Response åŒºåŸŸ: tokens[{response_start}:{response_end}] (å…± {len(response_tokens)} tokens)")
                self.debug_logger.info(f"  Response æ–‡æœ¬: {response_text[:200]}")
                self.debug_logger.info(f"  å™ªå£°æ¯”ä¾‹ t: {t[0].item():.3f}")

        # ç”¨äºå­˜å‚¨æ¯ä¸€è½®çš„ loss
        losses = []

        # å½“å‰åºåˆ—ï¼ˆåˆå§‹ä¸ºåŸå§‹åºåˆ—ï¼‰
        current_input_ids = t0

        # å¤šè½®è¿­ä»£
        for iter_idx in range(self.num_iterations):
            # ===========================================================
            # Step 1: åŠ å™ªï¼ˆç¬¬ä¸€è½®ï¼‰æˆ–ä½¿ç”¨ä¸Šä¸€è½®çš„è§£ç ç»“æœï¼ˆåç»­è½®ï¼‰
            # ===========================================================
            if iter_idx == 0:
                # ç¬¬ä¸€è½®ï¼šåŠ å™ª
                # ä½¿ç”¨ Dream çš„ q_sample å‡½æ•°ï¼Œä½†è¿™é‡Œ mask_token_id ç”¨ eos_token_id
                s_i, _, _ = q_sample(
                    input_ids=t0,
                    maskable_mask=response_mask.bool(),
                    mask_token_id=self.eos_token_id,
                    min=0.0,  # æˆ‘ä»¬å·²ç»åœ¨å¤–é¢é‡‡æ ·äº† tï¼Œè¿™é‡Œä¸å†é‡å¤é‡‡æ ·
                    max=1.0,
                    eos_token_id=self.eos_token_id,
                    t=t,
                )

                # ========== è®°å½•åŠ å™ªæ•ˆæœ ==========
                if should_log_detail:
                    s_i_sample = s_i[0].cpu().tolist()
                    response_indices = [i for i, mask in enumerate(response_mask_sample) if mask == 1]
                    if response_indices:
                        response_start = response_indices[0]
                        response_end = response_indices[-1] + 1

                        # å¯¹æ¯”åŸå§‹å’ŒåŠ å™ªåçš„ response åŒºåŸŸ
                        original_response = t0_sample[response_start:response_end]
                        noised_response = s_i_sample[response_start:response_end]

                        # ç»Ÿè®¡è¢«æ›¿æ¢çš„ token æ•°é‡
                        num_replaced = sum(1 for orig, noised in zip(original_response, noised_response)
                                          if orig != noised)
                        replace_ratio = num_replaced / len(original_response) if original_response else 0

                        noised_text = self.tokenizer.decode(noised_response, skip_special_tokens=False)

                        self.debug_logger.info(f"\n[è½®æ¬¡ {iter_idx}] åŠ å™ªåçš„åºåˆ— s{iter_idx}")
                        self.debug_logger.info(f"  è¢«æ›¿æ¢çš„ token æ•°: {num_replaced}/{len(original_response)} ({replace_ratio:.1%})")
                        self.debug_logger.info(f"  ç†è®ºå™ªå£°æ¯”ä¾‹: {t[0].item():.1%}")
                        self.debug_logger.info(f"  åŠ å™ªåæ–‡æœ¬: {noised_text[:200]}")

                        # æ˜¾ç¤ºå‰ 10 ä¸ª token çš„å¯¹æ¯”
                        self.debug_logger.info(f"  å‰10ä¸ªtokenå¯¹æ¯” (åŸå§‹ -> åŠ å™ª):")
                        for i in range(min(10, len(original_response))):
                            orig_tok = original_response[i]
                            noise_tok = noised_response[i]
                            if orig_tok != noise_tok:
                                orig_str = self.tokenizer.decode([orig_tok])
                                noise_str = self.tokenizer.decode([noise_tok])
                                self.debug_logger.info(f"    ä½ç½®{i}: {orig_tok}('{orig_str}') -> {noise_tok}('{noise_str}') [EOS={self.eos_token_id}]")
            else:
                # åç»­è½®ï¼šä½¿ç”¨ä¸Šä¸€è½®çš„è§£ç ç»“æœ
                s_i = current_input_ids

                # ========== è®°å½•ç²¾ç‚¼åçš„åºåˆ— ==========
                if should_log_detail:
                    s_i_sample = s_i[0].cpu().tolist()
                    response_indices = [i for i, mask in enumerate(response_mask_sample) if mask == 1]
                    if response_indices:
                        response_start = response_indices[0]
                        response_end = response_indices[-1] + 1
                        refined_response = s_i_sample[response_start:response_end]
                        refined_text = self.tokenizer.decode(refined_response, skip_special_tokens=False)

                        # ä¸åŸå§‹åºåˆ—å¯¹æ¯”
                        original_response = t0_sample[response_start:response_end]
                        num_diff = sum(1 for orig, refined in zip(original_response, refined_response)
                                      if orig != refined)
                        diff_ratio = num_diff / len(original_response) if original_response else 0

                        self.debug_logger.info(f"\n[è½®æ¬¡ {iter_idx}] ç²¾ç‚¼åçš„åºåˆ— s{iter_idx}")
                        self.debug_logger.info(f"  ä¸åŸå§‹åºåˆ—çš„å·®å¼‚: {num_diff}/{len(original_response)} ({diff_ratio:.1%})")
                        self.debug_logger.info(f"  ç²¾ç‚¼åæ–‡æœ¬: {refined_text[:200]}")

                        # ğŸ” é¢å¤–è°ƒè¯•ï¼šæ˜¾ç¤º s1 çš„å‰5ä¸ª response token
                        self.debug_logger.info(f"  ç²¾ç‚¼å response çš„å‰5ä¸ªtoken:")
                        for idx, (orig_tok, refined_tok) in enumerate(zip(original_response[:5], refined_response[:5])):
                            orig_str = self.tokenizer.decode([orig_tok])
                            refined_str = self.tokenizer.decode([refined_tok])
                            match = "âœ“" if orig_tok == refined_tok else "âœ—"
                            self.debug_logger.info(f"    ä½ç½®{response_start+idx}: åŸå§‹={orig_tok}('{orig_str}') vs ç²¾ç‚¼={refined_tok}('{refined_str}') {match}")

            # ===========================================================
            # Step 2: å‰å‘ä¼ æ’­
            # ===========================================================
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                output = self.fsdp_model(
                    input_ids=s_i,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    use_cache=False,
                )
                logits = output.logits  # [batch_size, seq_len, vocab_size]

            # ===========================================================
            # Step 3: è®¡ç®— lossï¼ˆå¯¹åŸå§‹ t0 çš„ next token predictionï¼‰
            # ===========================================================
            loss_i = compute_loss_on_response(
                logits=logits,
                labels=t0,  # æ³¨æ„ï¼šå§‹ç»ˆæ˜¯å¯¹åŸå§‹ t0 è®¡ç®— loss
                response_mask=response_mask,
            )
            losses.append(loss_i)

            # ========== è®°å½• loss å’Œé¢„æµ‹è´¨é‡ ==========
            if should_log_detail:
                self.debug_logger.info(f"  Loss (å¯¹åŸå§‹t0): {loss_i.item():.6f}")

                # åˆ†æé¢„æµ‹çš„ top-1 token
                with torch.no_grad():
                    response_indices = [i for i, mask in enumerate(response_mask_sample) if mask == 1]
                    if response_indices and len(response_indices) > 0:
                        # ğŸ” å…³é”®ï¼šå…ˆæ˜¾ç¤º instruction æœ€åä¸€ä¸ªä½ç½®é¢„æµ‹ response ç¬¬ä¸€ä¸ªä½ç½®
                        response_start = response_indices[0]
                        if response_start > 0:  # ç¡®ä¿æœ‰ instruction åŒºåŸŸ
                            instruction_last_pos = response_start - 1
                            pred_first_response = torch.argmax(logits[0, instruction_last_pos]).item()
                            target_first_response = t0_sample[response_start]
                            pred_str = self.tokenizer.decode([pred_first_response])
                            target_str = self.tokenizer.decode([target_first_response])
                            match = "âœ“" if pred_first_response == target_first_response else "âœ—"

                            self.debug_logger.info(f"  ã€å…³é”®ã€‘Instructionæœ€åä½ç½®é¢„æµ‹Responseç¬¬ä¸€ä¸ªtoken:")
                            self.debug_logger.info(f"    ä½ç½®{instruction_last_pos}->ä½ç½®{response_start}: é¢„æµ‹={pred_first_response}('{pred_str}') vs ç›®æ ‡={target_first_response}('{target_str}') {match}")

                        # ç„¶åçœ‹ response åŒºåŸŸçš„å‰å‡ ä¸ª token çš„é¢„æµ‹ï¼ˆé¢„æµ‹çš„æ˜¯ä¸‹ä¸€ä¸ªï¼‰
                        sample_positions = response_indices[:5]  # å‰5ä¸ª response token
                        logits_sample = logits[0, sample_positions].cpu()  # [5, vocab_size]
                        predicted_tokens = torch.argmax(logits_sample, dim=-1).tolist()
                        target_tokens = [t0_sample[pos + 1] for pos in sample_positions if pos + 1 < len(t0_sample)]

                        self.debug_logger.info(f"  Responseå‰5ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ªtoken:")
                        for idx, (pred, target) in enumerate(zip(predicted_tokens, target_tokens)):
                            pred_str = self.tokenizer.decode([pred])
                            target_str = self.tokenizer.decode([target])
                            match = "âœ“" if pred == target else "âœ—"
                            next_pos = sample_positions[idx] + 1
                            self.debug_logger.info(f"    ä½ç½®{sample_positions[idx]}->ä½ç½®{next_pos}: é¢„æµ‹={pred}('{pred_str}') vs ç›®æ ‡={target}('{target_str}') {match}")

            # ===========================================================
            # Step 4: è´ªå©ªè§£ç ï¼ˆä¸ºä¸‹ä¸€è½®å‡†å¤‡ï¼‰
            # ===========================================================
            if iter_idx < self.num_iterations - 1:  # æœ€åä¸€è½®ä¸éœ€è¦è§£ç 
                # ä½¿ç”¨ with torch.no_grad() å’Œ detach() æ¥é‡Šæ”¾æ˜¾å­˜
                with torch.no_grad():
                    current_input_ids = greedy_decode_response(
                        logits=logits.detach(),  # ç«‹å³detachï¼Œé‡Šæ”¾æ¢¯åº¦å ç”¨çš„æ˜¾å­˜
                        original_input_ids=t0,  # ğŸ”§ ä¿®å¤ï¼šåº”è¯¥ä¼ å…¥ t0ï¼Œä¿ç•™åŸå§‹ instruction
                        response_mask=response_mask,
                    )

                # æ³¨æ„ï¼šä¸æ˜¾å¼åˆ é™¤ logitsï¼Œå› ä¸º loss_i çš„è®¡ç®—å›¾å¯èƒ½è¿˜ä¾èµ–å®ƒ
                # Python çš„åƒåœ¾å›æ”¶ä¼šåœ¨å¾ªç¯ç»“æŸæ—¶è‡ªåŠ¨æ¸…ç†
                # æ˜¾å¼ del å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹ç ´åè®¡ç®—å›¾ï¼Œå¯¼è‡´ backward() å¤±è´¥

        # ===========================================================
        # Step 5: èšåˆæ‰€æœ‰è½®æ¬¡çš„ loss
        # ===========================================================
        total_loss, loss_dict = compute_iterative_loss(
            losses=losses,
            weights=self.loss_weights,
        )

        # æ·»åŠ é¢å¤–çš„æŒ‡æ ‡
        loss_dict['noise_mean'] = t.mean().item()

        # ========== è®°å½•æœ€ç»ˆçš„ loss èšåˆç»“æœ ==========
        if should_log_detail:
            self.debug_logger.info(f"\n[Loss èšåˆ]")
            self.debug_logger.info(f"  å„è½® loss:")
            for idx, (loss_val, weight) in enumerate(zip(losses, self.loss_weights)):
                self.debug_logger.info(f"    è½®æ¬¡{idx}: loss={loss_val.item():.6f}, weight={weight}, åŠ æƒloss={loss_val.item()*weight:.6f}")
            self.debug_logger.info(f"  Total loss: {total_loss.item():.6f}")
            self.debug_logger.info("=" * 80)

        return total_loss, loss_dict

    def training_step(self, batch: dict, global_step: int):
        """
        å•ä¸ªè®­ç»ƒæ­¥éª¤ï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰

        Args:
            batch: dict åŒ…å« input_ids, attention_mask, position_ids, loss_mask
            global_step: int - å…¨å±€æ­¥æ•°

        Returns:
            metrics: dict - è®­ç»ƒæŒ‡æ ‡ï¼ˆä»…åœ¨ä¼˜åŒ–å™¨æ›´æ–°æ—¶è¿”å›å®Œæ•´æŒ‡æ ‡ï¼‰
        """
        self.fsdp_model.train()

        # åˆ¤æ–­æ˜¯å¦éœ€è¦åœ¨è¿™ä¸€æ­¥æ›´æ–°ä¼˜åŒ–å™¨
        is_accumulation_step = (global_step + 1) % self.gradient_accumulation_steps != 0
        should_update_optimizer = not is_accumulation_step

        # ========== é˜¶æ®µ2ï¼šè®­ç»ƒè¿‡ç¨‹ç›‘æ§æ—¥å¿—ï¼ˆæ¯10æ­¥è¯¦ç»†è®°å½•ä¸€æ¬¡ï¼‰ ==========
        should_log_detail = (global_step % 10 == 0) and self.debug_logger is not None

        if should_log_detail:
            input_ids = batch["input_ids"]
            loss_mask = batch["loss_mask"]

            self.debug_logger.info("\n" + "=" * 80)
            self.debug_logger.info(f"[Step {global_step}] è®­ç»ƒæ‰¹æ¬¡è¯¦ç»†ä¿¡æ¯")
            self.debug_logger.info("=" * 80)
            self.debug_logger.info(f"Batch size: {input_ids.shape[0]}")
            self.debug_logger.info(f"Sequence length: {input_ids.shape[1]}")
            self.debug_logger.info(f"Gradient accumulation: {(global_step % self.gradient_accumulation_steps) + 1}/{self.gradient_accumulation_steps}")
            self.debug_logger.info(f"Will update optimizer: {should_update_optimizer}")

            # è§£ç ç¬¬ä¸€ä¸ªæ ·æœ¬æŸ¥çœ‹å†…å®¹
            first_sample_ids = input_ids[0].tolist()
            first_sample_text = self.tokenizer.decode(first_sample_ids, skip_special_tokens=False)
            first_sample_loss_mask = loss_mask[0].tolist()

            # ç»Ÿè®¡ response åŒºåŸŸ
            response_length = sum(first_sample_loss_mask)
            prompt_length = len(first_sample_loss_mask) - response_length

            self.debug_logger.info(f"\næ ·æœ¬0å†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰:")
            self.debug_logger.info(f"  {first_sample_text[:500]}")
            self.debug_logger.info(f"\nPrompt é•¿åº¦: {prompt_length} tokens")
            self.debug_logger.info(f"Response é•¿åº¦: {response_length} tokens")

        # è®¡ç®—å¤šè½®è¿­ä»£çš„ loss
        loss, metrics = self._compute_iterative_loss(batch, global_step)

        # æ¢¯åº¦ç´¯ç§¯ï¼šlosséœ€è¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°
        scaled_loss = loss / self.gradient_accumulation_steps

        # åå‘ä¼ æ’­ï¼ˆç´¯ç§¯é˜¶æ®µç¦æ­¢FSDPæ¢¯åº¦åŒæ­¥ï¼‰
        if is_accumulation_step:
            # ç´¯ç§¯é˜¶æ®µï¼šä½¿ç”¨ no_sync() ç¦æ­¢æ¢¯åº¦åŒæ­¥
            with self.fsdp_model.no_sync():
                scaled_loss.backward()
        else:
            # æœ€åä¸€æ­¥ï¼šæ­£å¸¸ backwardï¼Œä¼šè¿›è¡Œæ¢¯åº¦åŒæ­¥
            scaled_loss.backward()

        # åªåœ¨ç´¯ç§¯å®Œæˆæ—¶æ›´æ–°ä¼˜åŒ–å™¨
        if should_update_optimizer:
            # æ¢¯åº¦è£å‰ª
            grad_norm = self.fsdp_model.clip_grad_norm_(
                max_norm=self.config.optim.clip_grad
            )

            # ä¼˜åŒ–å™¨æ›´æ–°
            self.optimizer.step()
            self.lr_scheduler.step()
            self.optimizer.zero_grad()

            # æ·»åŠ é¢å¤–çš„æŒ‡æ ‡
            metrics['grad_norm'] = grad_norm.item()
            metrics['lr'] = self.lr_scheduler.get_last_lr()[0]
            metrics['is_optimizer_step'] = True
        else:
            # ç´¯ç§¯é˜¶æ®µï¼Œä¸æ›´æ–°ä¼˜åŒ–å™¨ï¼Œä½†æ˜¾ç¤ºå½“å‰å­¦ä¹ ç‡
            metrics['grad_norm'] = 0.0  # ç´¯ç§¯é˜¶æ®µä¸è£å‰ªæ¢¯åº¦
            metrics['lr'] = self.lr_scheduler.get_last_lr()[0]
            metrics['is_optimizer_step'] = False

        # ========== è®°å½•è®­ç»ƒæŒ‡æ ‡ ==========
        if should_log_detail:
            self.debug_logger.info(f"\n[Step {global_step}] è®­ç»ƒæŒ‡æ ‡:")
            for key, value in metrics.items():
                if isinstance(value, float):
                    self.debug_logger.info(f"  {key}: {value:.6f}")
                else:
                    self.debug_logger.info(f"  {key}: {value}")
            self.debug_logger.info("=" * 80)

        return metrics

    def save_checkpoint(self, step: int):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹ (ä½¿ç”¨ VERL çš„ checkpoint_manager)

        Args:
            step: int - å…¨å±€æ­¥æ•°
        """
        path = os.path.join(
            self.config.trainer.default_local_dir,
            f"global_step_{step}"
        )

        # è·å– max_ckpt_to_keep é…ç½®
        max_ckpt_to_keep = getattr(self.config.trainer, "max_ckpt_to_keep", None)

        # ä½¿ç”¨ checkpoint_manager ä¿å­˜
        # å®ƒä¼šè‡ªåŠ¨å¤„ç† FSDP sharded stateã€optimizerã€lr_schedulerã€rng ç­‰
        self.checkpoint_manager.save_checkpoint(
            local_path=path,
            hdfs_path=None,  # HDFS ç”± checkpoint_manager å†…éƒ¨å¤„ç†
            global_step=step,
            max_ckpt_to_keep=max_ckpt_to_keep,
        )

        log_with_rank(
            f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {path}",
            logger=logger,
            rank=self.rank,
            log_only_rank_0=True,
        )

        # å¯é€‰ï¼šå¤åˆ¶åˆ° HDFSï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if self.rank == 0 and self.config.trainer.get("default_hdfs_dir"):
            hdfs_path = os.path.join(
                self.config.trainer.default_hdfs_dir,
                f"global_step_{step}"
            )
            try:
                hdfs_io.makedirs(os.path.dirname(hdfs_path), exist_ok=True)
                hdfs_io.copy(src=path, dst=hdfs_path, dirs_exist_ok=True)
                logger.info(f"æ£€æŸ¥ç‚¹å·²å¤‡ä»½åˆ° HDFS: {hdfs_path}")
            except Exception as e:
                logger.warning(f"HDFS å¤‡ä»½å¤±è´¥: {e}")

        torch.distributed.barrier()

    def fit(self):
        """
        ä¸»è®­ç»ƒå¾ªç¯
        """
        # åˆå§‹åŒ–è¿½è¸ªï¼ˆä½¿ç”¨ VERL çš„ Trackingï¼‰
        if self.rank == 0:
            tracking = Tracking(
                project_name=self.config.trainer.project_name,
                experiment_name=self.config.trainer.experiment_name,
                default_backend=self.config.trainer.get("logger", ["console"]),
            )

        # ä»æ¢å¤çš„æ­¥æ•°å¼€å§‹ï¼ˆå¦‚æœæœ‰ checkpointï¼‰
        global_step = self.resume_global_step

        logger.info("=" * 60)
        logger.info("å¼€å§‹è®­ç»ƒ")
        logger.info(f"Total epochs: {self.config.trainer.total_epochs}")
        logger.info(f"Steps per epoch: {len(self.train_dataloader)}")
        logger.info(f"Total steps: {self.total_steps}")
        if self.resume_global_step > 0:
            logger.info(f"ä» checkpoint æ¢å¤: global_step={self.resume_global_step}")
        logger.info("=" * 60)

        # ========== é˜¶æ®µ4ï¼šè®°å½•å®Œæ•´çš„è¶…å‚æ•°é…ç½® ==========
        if self.debug_logger is not None:
            self.debug_logger.info("\n" + "=" * 80)
            self.debug_logger.info("[é˜¶æ®µ4] è¶…å‚æ•°é…ç½®")
            self.debug_logger.info("=" * 80)
            self.debug_logger.info("\nè®­ç»ƒé…ç½®:")
            self.debug_logger.info(f"  Total epochs: {self.config.trainer.total_epochs}")
            self.debug_logger.info(f"  Steps per epoch: {len(self.train_dataloader)}")
            self.debug_logger.info(f"  Total steps: {self.total_steps}")
            self.debug_logger.info(f"  Batch size per GPU: {self.config.data.micro_batch_size_per_gpu}")
            self.debug_logger.info(f"  World size: {self.world_size}")
            self.debug_logger.info(f"  Global batch size: {self.config.data.micro_batch_size_per_gpu * self.world_size}")

            self.debug_logger.info("\nä¼˜åŒ–å™¨é…ç½®:")
            self.debug_logger.info(f"  Learning rate: {self.config.optim.lr}")
            self.debug_logger.info(f"  Betas: {self.config.optim.betas}")
            self.debug_logger.info(f"  Weight decay: {self.config.optim.weight_decay}")
            self.debug_logger.info(f"  Gradient clipping: {self.config.optim.clip_grad}")
            self.debug_logger.info(f"  Gradient accumulation steps: {self.gradient_accumulation_steps}")
            self.debug_logger.info(f"  Effective batch size: {self.config.data.micro_batch_size_per_gpu * self.world_size * self.gradient_accumulation_steps}")
            self.debug_logger.info(f"  Warmup steps ratio: {self.config.optim.warmup_steps_ratio}")
            self.debug_logger.info(f"  Total optimizer steps: {self.total_optimizer_steps}")
            warmup_steps = int(self.total_optimizer_steps * self.config.optim.warmup_steps_ratio)
            self.debug_logger.info(f"  Warmup steps: {warmup_steps}")

            self.debug_logger.info("\nè¿­ä»£ç²¾ç‚¼é…ç½®:")
            self.debug_logger.info(f"  Num iterations: {self.num_iterations}")
            self.debug_logger.info(f"  Noise range: [{self.noise_min}, {self.noise_max}]")
            self.debug_logger.info(f"  Loss weights: {self.loss_weights}")
            self.debug_logger.info(f"  EOS token ID: {self.eos_token_id}")

            self.debug_logger.info("\næ•°æ®é…ç½®:")
            self.debug_logger.info(f"  Train files: {self.config.data.train_files}")
            self.debug_logger.info(f"  Max length: {self.config.data.max_length}")
            self.debug_logger.info(f"  Truncation: {self.config.data.truncation}")
            self.debug_logger.info(f"  Prompt key: {self.config.data.prompt_key}")
            self.debug_logger.info(f"  Response key: {self.config.data.response_key}")

            self.debug_logger.info("\næ¨¡å‹é…ç½®:")
            self.debug_logger.info(f"  Model: {self.config.model.partial_pretrain}")
            self.debug_logger.info(f"  Gradient checkpointing: {self.config.model.get('enable_gradient_checkpointing', False)}")

            self.debug_logger.info("=" * 80)

        # è®­ç»ƒå¼€å§‹å‰æ¸…é›¶æ¢¯åº¦
        self.optimizer.zero_grad()

        # ä¸»è®­ç»ƒå¾ªç¯
        for epoch in range(self.current_epoch, self.config.trainer.total_epochs):
            self.current_epoch = epoch
            self.train_sampler.set_epoch(epoch)  # é‡è¦ï¼šè®© DDP çš„ shuffle æ­£ç¡®å·¥ä½œ

            # Epoch å†…çš„è®­ç»ƒ
            dataloader_iter = iter(self.train_dataloader)
            pbar = tqdm(
                dataloader_iter,
                desc=f"Epoch {epoch}",
                total=len(self.train_dataloader),
                disable=(self.rank != 0),
            )

            for batch in pbar:
                # è®­ç»ƒæ­¥éª¤
                metrics = self.training_step(batch, global_step)

                # æ›´æ–°è¿›åº¦æ¡
                if self.rank == 0:
                    pbar.set_postfix({
                        'loss': f"{metrics['loss_total']:.4f}",
                        'lr': f"{metrics['lr']:.2e}",
                    })

                    # è®°å½•åˆ°è¿½è¸ªç³»ç»Ÿ
                    tracking.log(
                        data={f"train/{k}": v for k, v in metrics.items()},
                        step=global_step,
                    )

                global_step += 1

                # ä¿å­˜æ£€æŸ¥ç‚¹
                if global_step % self.config.trainer.save_checkpoint_steps == 0:
                    self.save_checkpoint(step=global_step)

                # ========== è°ƒè¯•æ¨¡å¼ï¼šæœ€å¤§æ­¥æ•°é™åˆ¶ ==========
                max_debug_steps = self.config.trainer.get("max_debug_steps", None)
                if max_debug_steps is not None and global_step >= max_debug_steps:
                    if self.rank == 0:
                        logger.info(f"\nè¾¾åˆ°è°ƒè¯•æœ€å¤§æ­¥æ•° {max_debug_steps}ï¼Œåœæ­¢è®­ç»ƒ")
                    return

            # Epoch ç»“æŸåä¿å­˜
            if self.rank == 0:
                logger.info(f"Epoch {epoch} å®Œæˆ")

        # è®­ç»ƒç»“æŸ
        if self.rank == 0:
            logger.info("=" * 60)
            logger.info("è®­ç»ƒå®Œæˆï¼")
            logger.info("=" * 60)

        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        self.save_checkpoint(step=global_step)
