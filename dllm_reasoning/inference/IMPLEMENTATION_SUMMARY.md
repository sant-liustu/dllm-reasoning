# æŽ¨ç†å®žçŽ°æ€»ç»“

## âœ… å·²å®Œæˆçš„åŠŸèƒ½

### 1. æ ¸å¿ƒæŽ¨ç†å‡½æ•° (`generator.py`)

**ä¸»å‡½æ•°:**
- `iterative_generate()`: å—çŠ¶è¿­ä»£ç”Ÿæˆ
  - æ”¯æŒæ‰¹é‡æŽ¨ç†
  - æ”¯æŒ attention_mask
  - EOS æ£€æµ‹å’Œæˆªæ–­
  - æœ€å¤§é•¿åº¦é™åˆ¶
  - è¯¦ç»†çš„æ—¥å¿—è¾“å‡º

**è¾…åŠ©å‡½æ•°:**
- `_refine_simple()`: Refine å¾ªçŽ¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼Œä¸ä½¿ç”¨ KV cacheï¼‰
- `_has_eos()`: EOS token æ£€æµ‹
- `_truncate_at_eos()`: æˆªæ–­åˆ°ç¬¬ä¸€ä¸ª EOS

**æ ¸å¿ƒç‰¹æ€§:**
- âœ… æ­£ç¡®å¤„ç† next token prediction åç§»
- âœ… åªæ›´æ–°æ–°å—ï¼Œä¸æ”¹å˜ prefix
- âœ… æ”¯æŒè´ªå©ªè§£ç 
- âœ… ç¬¬ä¸€è½® refine æ‹¼æŽ¥æœ€åŽä¸€ä¸ª tokenï¼ŒåŽç»­è½®ç›´æŽ¥æ›¿æ¢
- âœ… å®Œæ•´çš„å‚æ•°éªŒè¯å’Œé”™è¯¯å¤„ç†

### 2. æŽ¨ç†æ¼”ç¤ºè„šæœ¬ (`demo.py`)

**åŠŸèƒ½:**
- âœ… å•ä¸ª prompt æŽ¨ç†
- âœ… æ‰¹é‡æŽ¨ç†ï¼ˆä»Žæ–‡ä»¶è¯»å–ï¼‰
- âœ… è‡ªåŠ¨åº”ç”¨ chat template
- âœ… æ‰¹å¤„ç†æ”¯æŒï¼ˆå¯é…ç½® batch_sizeï¼‰
- âœ… ç»“æžœåŒæ—¶æ‰“å°åˆ°ç»ˆç«¯å’Œä¿å­˜åˆ°æ–‡ä»¶
- âœ… æ”¯æŒ .json å’Œ .jsonl æ ¼å¼
- âœ… å®Œæ•´çš„å‘½ä»¤è¡Œå‚æ•°
- âœ… è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤ºï¼ˆtqdmï¼‰
- âœ… é”™è¯¯å¤„ç†å’Œæ—¥å¿—

**å‘½ä»¤è¡Œå‚æ•°:**
```bash
# å¿…éœ€å‚æ•°
--model_path         # æ¨¡åž‹è·¯å¾„
--prompt / --prompts_file  # è¾“å…¥ï¼ˆäºŒé€‰ä¸€ï¼‰

# ç”Ÿæˆå‚æ•°
--add_eos_length     # é»˜è®¤ 127
--refine_iter        # é»˜è®¤ 2
--max_new_tokens     # é»˜è®¤ 1024
--max_length         # é»˜è®¤ 8192
--batch_size         # é»˜è®¤ 1

# è¾“å‡ºå‚æ•°
--output_file        # é»˜è®¤ inference_results.jsonl
--max_display        # é»˜è®¤ 5

# å…¶ä»–
--use_chat_template  # åº”ç”¨ chat template
--device            # cuda/cpu
--trust_remote_code # é»˜è®¤ True
```

### 3. æ–‡æ¡£

**å®Œæˆçš„æ–‡æ¡£:**
- âœ… `inference/README.md` - æŽ¨ç†æ¨¡å—ä¸“é—¨æ–‡æ¡£
- âœ… `README.md` æ›´æ–° - æ·»åŠ æŽ¨ç†ä½¿ç”¨éƒ¨åˆ†
- âœ… ä»£ç å†…æ³¨é‡Šå’Œ docstringï¼ˆéžå¸¸è¯¦ç»†ï¼‰
- âœ… `IMPLEMENTATION_SUMMARY.md` - æœ¬æ–‡æ¡£

---

## ðŸ“‹ å®žçŽ°ç»†èŠ‚

### Next Token Prediction åç§»å¤„ç†

**æ ¸å¿ƒç†è§£:**
```python
# æ·»åŠ  N ä¸ª EOS â†’ ç”Ÿæˆ N+1 ä¸ª token
add_eos_length = 127
# æ‹¼æŽ¥åŽåºåˆ—é•¿åº¦: pre_length + 127
# å¯é¢„æµ‹ä½ç½®: [pre_length, pre_length+1, ..., pre_length+127]
# å…± 128 ä¸ªä½ç½®

# æå– logits
new_block_logits = logits[:, pre_length-1 : pre_length+add_eos_length, :]
# å½¢çŠ¶: [batch, 128, vocab]

# è§£ç 
predicted_tokens = new_block_logits.argmax(dim=-1)  # [batch, 128]

# æ›´æ–°åºåˆ—
# ç¬¬ä¸€è½® refine: å‰ 127 ä¸ªæ›¿æ¢ï¼Œæœ€åŽ 1 ä¸ªæ‹¼æŽ¥
# åŽç»­ refine: å…¨éƒ¨ 128 ä¸ªæ›¿æ¢
```

### Refine å¾ªçŽ¯é€»è¾‘

```python
for refine_step in range(refine_iter):
    # 1. å‰å‘ä¼ æ’­
    logits = model(current_ids).logits
    
    # 2. æå–æ–°å— logits
    new_block_logits = logits[:, pre_length-1 : pre_length+add_eos_length, :]
    
    # 3. è§£ç 
    predicted_tokens = new_block_logits.argmax(dim=-1)
    
    # 4. æ›´æ–°åºåˆ—
    if refine_step == 0:
        # ç¬¬ä¸€è½®: å‰ N ä¸ªæ›¿æ¢ + æœ€åŽ 1 ä¸ªæ‹¼æŽ¥
        current_ids[:, pre_length:] = predicted_tokens[:, :add_eos_length]
        last_token = predicted_tokens[:, add_eos_length:add_eos_length+1]
        current_ids = torch.cat([current_ids, last_token], dim=1)
    else:
        # åŽç»­è½®: å…¨éƒ¨æ›¿æ¢
        current_ids[:, pre_length:pre_length+add_eos_length+1] = predicted_tokens
```

### åœæ­¢æ¡ä»¶æ£€æµ‹

```python
# æ¡ä»¶1: EOS æ£€æµ‹
new_block = current_ids[:, pre_length:]
if (new_block == eos_token_id).any():
    generated_ids = _truncate_at_eos(current_ids, eos_token_id, pad_token_id)
    break

# æ¡ä»¶2: æœ€å¤§é•¿åº¦
if current_ids.size(1) >= max_length:
    generated_ids = current_ids
    break
```

---

## ðŸŽ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```bash
# å•ä¸ª prompt
python -m dllm_reasoning.inference.demo \
    --model_path /path/to/checkpoint \
    --prompt "What is 2+2?"
```

### æ‰¹é‡æŽ¨ç†

```bash
# åˆ›å»º prompts.txt
cat > prompts.txt << EOF
What is 2+2?
Explain quantum physics.
Write a Python function.
